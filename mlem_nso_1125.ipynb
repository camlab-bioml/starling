{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlem_nso.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDReOA8IGpOdx4eviUS11i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camlab-bioml/2021_IMC_Jett/blob/main/mlem_nso_1125.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb6UYGkpoz-u"
      },
      "source": [
        "import argparse\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "import random\n",
        "\n",
        "def compute_p_y_given_z(Y, Theta, dist, reg=1e-3):\n",
        "  \n",
        "  \"\"\" Returns NxC\n",
        "  p(y_n | z_n = c)\n",
        "  \"\"\"\n",
        "  \n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma']) + reg\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_Y = D.Normal(loc=mu, scale=sigma)\n",
        "  elif dist == 'student':\n",
        "    dist_Y = D.StudentT(df=torch.tensor([2.0]), loc=mu, scale=sigma)\n",
        "\n",
        "  return dist_Y.log_prob(Y.reshape(-1, 1, NF)).sum(2) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_z(S, Theta, dist, reg=1e-3):\n",
        "  \n",
        "  \"\"\" Returns NxC\n",
        "  p(s_n | z_n = c)\n",
        "  \"\"\"\n",
        "  \n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega']) + reg\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_S = D.Normal(loc=psi, scale=omega)\n",
        "  elif dist == 'student':\n",
        "    dist_S = D.StudentT(df=torch.tensor([2.0]), loc=psi, scale=omega)\n",
        "\n",
        "  return dist_S.log_prob(S.reshape(-1,1)) \n",
        "\n",
        "def compute_p_y_given_gamma(Y, Theta, dist, reg=1e-3):\n",
        "  \n",
        "  \"\"\" NxCxC\n",
        "  p(y_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma']) + reg\n",
        "\n",
        "  mu2 = mu.reshape(1, NC, NF)\n",
        "  mu2 = (mu2 + mu2.permute(1, 0, 2)) / 2.0 # C x C x G matrix \n",
        "\n",
        "  sigma2 = sigma.reshape(1, NC, NF)\n",
        "  sigma2 = (sigma2 + sigma2.permute(1, 0, 2)) / 2.0\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_Y2 = D.Normal(loc=mu2, scale=sigma2)\n",
        "  elif dist == 'student':\n",
        "    dist_Y2 = D.StudentT(df=torch.tensor([2.0]), loc=mu2, scale=sigma2)\n",
        "\n",
        "  return dist_Y2.log_prob(Y.reshape(-1, 1, 1, NF)).sum(3) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_gamma(S, Theta, dist, reg=1e-3):\n",
        "  \n",
        "  \"\"\" NxCxC\n",
        "  p(s_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "  \n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega']) + reg\n",
        "\n",
        "  psi2 = psi.reshape(-1,1)\n",
        "  psi2 = psi2 + psi2.T\n",
        "\n",
        "  omega2 = omega.reshape(-1,1)\n",
        "  omega2 = omega2 + omega2.T\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_S2 = D.Normal(loc=psi2, scale=omega2)\n",
        "  elif dist == 'student':\n",
        "    dist_S2 = D.StudentT(df=torch.tensor([2.0]), loc=psi2, scale=omega2)\n",
        "\n",
        "  return dist_S2.log_prob(S.reshape(-1, 1, 1))\n",
        "\n",
        "def _ics(logL, n_obs, n_features, n_clusters, incCellSize): #, n, p, c\n",
        "  param_mu = n_clusters * n_features\n",
        "  param_sigma = n_clusters * n_features\n",
        "  \n",
        "  param_delta = 1\n",
        "  param_pi = n_clusters - 1\n",
        "  param_tau = ((n_clusters * n_clusters) - n_clusters)/2 + n_clusters - 1\n",
        "  \n",
        "  params = param_mu + param_sigma + param_delta + param_pi + param_tau\n",
        "\n",
        "  if incCellSize:\n",
        "    param_psi = n_clusters\n",
        "    param_omega = n_clusters\n",
        "    params += (param_psi + param_omega)\n",
        "  \n",
        "  return 2 * (params - logL), -2 * logL + params * np.log(n_obs)\n",
        "\n",
        "def ll(Y, S, Theta, dist, incCellSize):\n",
        "  \n",
        "  \"\"\"compute\n",
        "  p(gamma = [c,c'], d= 1 | Y,S)\n",
        "  p(z = c, d=0 | Y,S)\n",
        "  \"\"\"\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "\n",
        "  ## singlet calculation\n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta, dist)\n",
        "  p_data_given_z_d0 = p_y_given_z + log_pi\n",
        "\n",
        "  if incCellSize: # cellsize case\n",
        "    p_s_given_z = compute_p_s_given_z(S, Theta, dist)\n",
        "    p_data_given_z_d0 += p_s_given_z\n",
        "  \n",
        "  p_data_given_d0 = torch.logsumexp(p_data_given_z_d0, dim=1) # this is p(data|d=0)\n",
        "\n",
        "  ## doublet calculation\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta, dist)\n",
        "\n",
        "  if incCellSize: # cellsize case\n",
        "    p_s_given_gamma = compute_p_s_given_gamma(S, Theta, dist)\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + p_s_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "  else:\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "\n",
        "  p_data = torch.cat([p_data_given_z_d0 + log_delta[0], p_data_given_gamma_d1 + log_delta[1]], dim=1)\n",
        "\n",
        "  return torch.logsumexp(p_data, dim=1).mean()\n",
        "\n",
        "def compute_r_v_2(Y, S, Theta, dist, incCellSize):\n",
        "  \n",
        "  \"\"\"Need to compute\n",
        "  p(gamma = [c,c'], d= 1 | Y,S)\n",
        "  p(z = c, d=0 | Y,S)\n",
        "  \"\"\"\n",
        "  \n",
        "  #lookups = np.triu_indices(nc) # wanted indices\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "\n",
        "  ## singlet calculation\n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta, dist)\n",
        "  p_data_given_z_d0 = p_y_given_z + log_pi\n",
        "\n",
        "  if incCellSize: # singlet case\n",
        "    p_s_given_z = compute_p_s_given_z(S, Theta, dist)\n",
        "    p_data_given_z_d0 += p_s_given_z\n",
        "  \n",
        "  p_data_given_d0 = torch.logsumexp(p_data_given_z_d0, dim=1) # this is p(data|d=0)\n",
        "\n",
        "  ## doublet calculation\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta, dist)\n",
        "  \n",
        "  if incCellSize: # doublet case\n",
        "    p_s_given_gamma = compute_p_s_given_gamma(S, Theta, dist)\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + p_s_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "  else:\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "  \n",
        "  ## LL\n",
        "  p_data = torch.cat([p_data_given_z_d0 + log_delta[0], p_data_given_gamma_d1 + log_delta[1]], dim=1)\n",
        "  p_data = torch.logsumexp(p_data, dim=1)\n",
        "\n",
        "  ## singlet & doublet probability\n",
        "  r = p_data_given_z_d0.T + log_delta[0] - p_data\n",
        "  v = p_data_given_gamma_d1.T + log_delta[1] - p_data\n",
        "\n",
        "  ## normalize\n",
        "  p_singlet = torch.exp(p_data_given_d0 + log_delta[0] - p_data)\n",
        "\n",
        "  return r.T, v.T.reshape(-1, NC, NC), p_data, p_singlet\n",
        "\n",
        "def mlem_ycs(Y, S, Theta, dist):\n",
        "  \n",
        "  wandb.init(project=\"mlem_{}_nc{}\".format(PROJECT_NAME, NC))\n",
        "  \n",
        "  lookups = np.triu_indices(NC) # wanted indices\n",
        "  uwanted = np.tril_indices(NC, -1)\n",
        "  \n",
        "  opt = optim.Adam(Theta.values(), lr=LEARNING_RATE)\n",
        "  \n",
        "  XX = torch.hstack((Y, S.reshape(-1,1))).float()\n",
        "  trainloader = DataLoader(torch.tensor(XX), batch_size=BATCH_SIZE, shuffle=True)\n",
        "  #validloader = DataLoader(valid, batch_size=1280, shuffle=False)\n",
        "  #testloader = DataLoader(test, batch_size=1280, shuffle=False)\n",
        "    \n",
        "  loss = []\n",
        "  for epoch in range(N_ITER):\n",
        "    \n",
        "    nlls = 0\n",
        "    for j, train_batch in enumerate(trainloader):\n",
        "      \n",
        "      bY = train_batch[:,:NF]\n",
        "      bS = train_batch[:,NF]\n",
        "      \n",
        "      opt.zero_grad()  \n",
        "      nll = -ll(bY, bS, Theta, dist, incCellSize=True)\n",
        "      nll.backward()\n",
        "      opt.step()\n",
        "            \n",
        "      nlls += nll.detach()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      aic, bic = _ics(-nlls, Y.shape[0], NF, NC, incCellSize=True) #, n, p, c\n",
        "\n",
        "      wandb.log({\n",
        "        'nll': nlls, \n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "      })\n",
        "\n",
        "      if epoch > 5 and abs((np.mean(loss[-10:]) - np.mean(loss[-11:-1]))/np.mean(loss[-10:])) < TOL:\n",
        "      #if epoch > 5 and abs(np.mean(loss[-10:]) - np.mean(loss[-11:-1])) < TOL:\n",
        "        #print(nlls)\n",
        "        #print(F.softmax(Theta['is_delta'], 0).exp())\n",
        "        #print(Theta['log_psi'].exp())\n",
        "        break\n",
        "            \n",
        "      loss.append(nlls)\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    r, v, L, p_singlet = compute_r_v_2(Y, S, Theta, dist, True)\n",
        "\n",
        "    ugt = torch.tensor(v[:,lookups[0], lookups[1]]).exp()\n",
        "    lt = torch.tensor(v[:,uwanted[0], uwanted[1]]).exp()\n",
        "    ugt[:,lookups[0] != lookups[1]] = ugt[:,lookups[0] != lookups[1]] + lt             \n",
        "    p_cluster = torch.hstack((ugt, torch.tensor(r).exp()))\n",
        "\n",
        "  return p_singlet, p_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ffmS-ruFaAY"
      },
      "source": [
        "!pip install scanpy\n",
        "import scanpy as sc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N79MVxJFxhi",
        "outputId": "4915e88e-bd39-4075-8aca-d4746c572c7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y67MfezzG_K",
        "outputId": "ce4f39d0-b831-478a-d125-581581304276"
      },
      "source": [
        "## mouse data\n",
        "#adata = sc.read_h5ad(\"DAMM/data/mouse_single_cell_expression.h5ad\")\n",
        "#adata = sc.read_h5ad(\"DAMM/data/mouse_1000.h5ad\")\n",
        "adata = sc.read_h5ad(\"DAMM/data/mouse_single_cell_expression_dc.h5ad\")\n",
        "\n",
        "included_names = ['B220', 'CCR7', 'CD11b', 'CD11c', 'CD19', 'CD28', 'CD3', 'CD31', 'CD4',\n",
        " 'CD45', 'CD49b', 'CD68', 'CD73', 'CD8', 'CTLA4', 'FOXP3', 'GATA3', 'GFP', \n",
        " 'GranzymeB', 'HA', 'ICOS', 'IL7Ra', 'Ly6G', 'MHCII', 'PD1', 'PDL1', 'PNAd', \n",
        " 'Perforin', 'RFP', 'S100A8-9', 'TBET', 'TCF1', 'YAP', 'iNOS']\n",
        "SS = adata.obs['size']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc2zXJslMkUA"
      },
      "source": [
        "#cell_sel = np.random.choice(adata.shape[0], size=5000)\n",
        "#adata[cell_sel,:].write_h5ad(filename='DAMM/data/mouse_5k.h5ad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2KZqd8aMXwx"
      },
      "source": [
        "## human data\n",
        "#adata = sc.read_h5ad(\"DAMM/data/basel_zuri_subsample.h5ad\")\n",
        "adata = sc.read_h5ad(\"DAMM/data/basel_zuri.h5ad\")\n",
        "included_names = ['EGFR', 'ECadherin', 'ER', 'GATA3', 'Histone_H3_1', 'Ki67', 'SMA', \n",
        "'Vimentin', 'cleaved_Parp', 'Her2', 'p53', 'panCytokeratin', 'CD19', 'PR', 'Myc', \n",
        "'Fibronectin', 'CK14', 'Slug', 'CD20', 'vWF', 'Histone_H3_2', 'CK5', 'CD44', 'CD45', \n",
        "'CD68', 'CD3', 'CAIX', 'CK8/18', 'CK7', 'phospho Histone', 'phospho S6', 'phospho mTOR']\n",
        "\n",
        "cell_sel = np.random.choice(adata.shape[0], size=10000)\n",
        "adata = adata[cell_sel,:]\n",
        "\n",
        "SS = adata.obs['Area'] ## for human data"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA3UE-PJFie4"
      },
      "source": [
        "adata = adata[:,included_names]\n",
        "#adata = adata[adata.obs['size'] > 20,:]\n",
        "\n",
        "YY = adata.X.copy()\n",
        "#YY = np.array(np.arcsinh(YY / 5.))\n",
        "\n",
        "NO, NF = YY.shape #number obs & features \n",
        "\n",
        "for i in range(NF):\n",
        "  YY[:,i] = winsorize(YY[:,i], limits=[0, 0.01]).data\n",
        "\n",
        "SS = winsorize(SS, limits=[0, 0.01]).data"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhZjzw7gkGG3"
      },
      "source": [
        "#N_training = 5000\n",
        "idx_singlet = np.random.choice(adata.shape[0], size=NO)\n",
        "Y_singlet = YY[idx_singlet,:] ## expression\n",
        "S_singlet = SS[idx_singlet]\n",
        "\n",
        "idx_doublet = [np.random.choice(adata.shape[0], size=NO), np.random.choice(adata.shape[0], size=NO)]\n",
        "Y_doublet = (YY[idx_doublet[0],:] + YY[idx_doublet[1],:])/2.\n",
        "S_doublet = SS[idx_doublet[0]] + SS[idx_doublet[1]]\n",
        "\n",
        "fake_Y = torch.tensor(np.vstack([Y_singlet, Y_doublet]))\n",
        "fake_S = torch.tensor(np.hstack([S_singlet, S_doublet]))\n",
        "fake_label = torch.tensor(np.concatenate([np.ones(NO), np.zeros(NO)]))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EPIKHBBF_6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346ad3a8-61a9-4651-c432-04b7697ffd82"
      },
      "source": [
        "NC = 10 # number of clusters\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 128\n",
        "N_ITER = 100\n",
        "TOL = 1e-3 #converagence criterion\n",
        "lam = .01\n",
        "\n",
        "fake_loss = nn.BCELoss()\n",
        "\n",
        "## w&b api key\n",
        "wandb.login(key='4117bb00bef94e0904c16afed79f1888e0839eb9')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz6agw03blYG"
      },
      "source": [
        "Y = torch.tensor(YY)\n",
        "S = torch.tensor(SS)\n",
        "\n",
        "kms = KMeans(NC).fit(Y)\n",
        "init_labels = kms.labels_\n",
        "init_label_class = np.unique(init_labels)\n",
        "\n",
        "mu_init = np.array([YY[init_labels == c,:].mean(0) for c in init_label_class])\n",
        "sigma_init = np.array([YY[init_labels == c,:].std(0) for c in init_label_class])\n",
        "\n",
        "psi_init = np.array([SS[init_labels == c].mean() for c in init_label_class])\n",
        "omega_init = np.array([SS[init_labels == c].std() for c in init_label_class])\n",
        "\n",
        "pi_init = np.array([np.mean(init_labels == c) for c in init_label_class])\n",
        "tau_init = np.ones((NC,NC))\n",
        "tau_init = tau_init / tau_init.sum()\n",
        "\n",
        "Theta = {\n",
        "    'log_mu': np.log(mu_init + 1e-6),\n",
        "    'log_sigma': np.log(sigma_init + 1e-6), #np.zeros_like(sigma_init),\n",
        "    'log_psi': np.log(psi_init + 1e-6),\n",
        "    'log_omega': np.log(omega_init + 1e-6),\n",
        "    'is_delta': np.log([0.95, 0.05]),\n",
        "    'is_pi': np.log(pi_init),\n",
        "    'is_tau': np.log(tau_init),\n",
        "\n",
        "}\n",
        "\n",
        "Theta = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "Theta['is_delta'].requires_grad = False"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEzWTNlH9IRU"
      },
      "source": [
        "dist = \"student\"\n",
        "opt = optim.Adam(Theta.values(), lr=LEARNING_RATE)\n",
        "\n",
        "#XX = torch.hstack((Y, S.reshape(-1,1))).float()\n",
        "#fake_XX = torch.hstack((fake_Y, fake_S.reshape(-1,1))).float()\n",
        "\n",
        "#trainloader = DataLoader(torch.tensor(XX), batch_size=BATCH_SIZE, shuffle=True)\n",
        "#validloader = DataLoader(valid, batch_size=1280, shuffle=False)\n",
        "#testloader = DataLoader(test, batch_size=1280, shuffle=False)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cqP9YdqfDFD"
      },
      "source": [
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[i] for d in self.datasets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(d) for d in self.datasets)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(ConcatDataset(Y, S, fake_Y, fake_S, fake_label), batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tMBOhzBXZh_"
      },
      "source": [
        "PROJECT_NAME = 'mlem_human_nso_ycs_fakeData_stuT'\n",
        "wandb.init(project=\"mlem_{}_nc{}\".format(PROJECT_NAME, NC))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGKyiQ-3496Y"
      },
      "source": [
        "  loss = []\n",
        "  for epoch in range(N_ITER):\n",
        "    \n",
        "    #print(epoch)\n",
        "\n",
        "    nlls = 0; flos = 0; tlos = 0\n",
        "    for j, (bY, bS, bfY, bfS, bfl) in enumerate(train_loader):\n",
        "\n",
        "      opt.zero_grad()  \n",
        "      nll = -ll(bY, bS, Theta, dist, True)\n",
        "\n",
        "      _, _, _, p_fake_singlet = compute_r_v_2(bfY, bfS, Theta, dist, True)\n",
        "      floss = -fake_loss(p_fake_singlet, bfl) * lam\n",
        "\n",
        "      #if j % 10 == 0:\n",
        "      #  print(\"nll {}; floss {}\".format(nll, floss))\n",
        "      \n",
        "      closs = nll + floss\n",
        "      closs.backward()\n",
        "      opt.step()\n",
        "      \n",
        "      nlls += nll.detach()\n",
        "      flos += floss.detach()\n",
        "      tlos += nll.detach() + floss.detach()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      #aic, bic = _ics(-nlls, Y.shape[0], NF, NC, incCellSize=True) #, n, p, c\n",
        "\n",
        "      wandb.log({\n",
        "        'nll': nlls, \n",
        "        'fake_loss': flos,\n",
        "        'total_loss': tlos,\n",
        "        #'AIC': aic,\n",
        "        #'BIC': bic,\n",
        "      })\n",
        "\n",
        "      if epoch > 10 and abs(np.mean(loss[-5:]) - np.mean(loss[-6:-1])) < TOL:\n",
        "        break\n",
        "      \n",
        "      print(\"epoch {}; total loss {}\".format(epoch+1, tlos))\n",
        "\n",
        "      loss.append(tlos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swuU-UW6Lcxe",
        "outputId": "e77efe19-4a30-4a57-cb1f-f59ab3cb1dd8"
      },
      "source": [
        "with torch.no_grad():\n",
        "  r, v, L, p_singlet = compute_r_v_2(Y, S, Theta, dist, True)\n",
        "  print(sum(p_singlet > .5)/len(p_singlet))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5771)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkzWynntw0p4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Rq0fWuS92UoK",
        "outputId": "b789dcb0-d61f-4a3d-d6f3-18dc72219be6"
      },
      "source": [
        "plt.hist(p_singlet.detach().numpy(), bins=50);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtPRL-ICkoUv"
      },
      "source": [
        "y_pred = np.array([0.1580, 0.4137, 0.2285])\n",
        "y_true = np.array([0.0, 1.0, 0.0]) #2 labels: (0,1)\n",
        "def BCE(y_pred, y_true):\n",
        "    total_bce_loss = np.sum(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
        "    # Getting the mean BCE loss\n",
        "    num_of_samples = y_pred.shape[0]\n",
        "    mean_bce_loss = total_bce_loss / num_of_samples\n",
        "    return mean_bce_loss\n",
        "bce_value = BCE(y_pred, y_true)\n",
        "print (\"BCE error is: \" + str(bce_value))\n",
        "\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "sigmoid = torch.nn.Sigmoid() # Ensuring inputs are between 0 and 1\n",
        "input = torch.tensor(y_pred)\n",
        "target = torch.tensor(y_true)\n",
        "output = bce_loss(input, target)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}