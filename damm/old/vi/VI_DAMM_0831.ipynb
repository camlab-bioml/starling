{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VI_DAMM_0831.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6+MKGS1pL5Pm6g2VslX/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camlab-bioml/2021_IMC_Jett/blob/main/VI_DAMM_0831.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8bx7aq-lq2Q"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as datasets\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBiqVkEyrNjF"
      },
      "source": [
        "#%%capture\n",
        "#!pip install wandb --upgrade"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJxAa9PdpRiY"
      },
      "source": [
        "#import wandb\n",
        "#wandb.login()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxK61unVln3j"
      },
      "source": [
        "## generate data\n",
        "def generateData(n_clusters = 3, n_obs = 10000, n_features = 2):\n",
        "\n",
        "  #n_clusters = 3; n_obs = 100; n_features = 2\n",
        "  \n",
        "  ## set truth expression means/covariances (multivariate) ##\n",
        "  mu = np.random.rand(n_clusters, n_features)\n",
        "  # mu = np.sort(mu, 0) ## sort expressions\n",
        "  sigma = 0.001 * np.identity(n_features) ## variance-covariance matrix\n",
        "\n",
        "  ## set truth cell size means/variances (univariate) ##\n",
        "  psi = [np.random.normal(100, 25) for i in range(n_clusters)]\n",
        "  #psi = np.arange(90, 90 + 5 * n_clusters, 5)\n",
        "  psi = np.sort(psi, 0)\n",
        "  omega = 1 ## standard deviation\n",
        "  ###\n",
        "\n",
        "  ## set latent variables distributions ##\n",
        "  lambda_arr = np.random.binomial(1, .95, n_obs) # p=.95 (a cell belongs to singlet or doublet) \n",
        "\n",
        "  n_singlet = np.sum(lambda_arr == 1) ## number of cells in singlet clusters\n",
        "  n_doublet = np.sum(lambda_arr == 0) ## number of cells in doublet clusters\n",
        "  \n",
        "  lambda0_arr = n_singlet / n_obs ## proportion of cells belong to singlet\n",
        "  lambda1_arr = n_doublet / n_obs ## proportion of cells belong to doublet\n",
        "\n",
        "  #pi_arr = np.sort(np.random.sample(n_clusters))\n",
        "  pi_arr = np.sort(np.random.rand(n_clusters))\n",
        "  pi_arr /= pi_arr.sum()\n",
        "\n",
        "  n_doublet_clusters = int((n_clusters * n_clusters - n_clusters)/2 + n_clusters)\n",
        "  #tau_arr = np.sort(np.random.sample(n_doublet_clusters))\n",
        "  tau_arr = np.sort(np.random.rand(n_doublet_clusters))\n",
        "  tau_arr /= tau_arr.sum()\n",
        "\n",
        "  ## draw cells based on defined parameters theta1 = (mu, sigma, psi, omega) & theta2 = (lambda, pi, tau)\n",
        "  x = np.zeros((n_singlet, n_features+5))\n",
        "  for i in range(n_singlet):\n",
        "    selected_cluster = np.random.choice(n_clusters, size = 1, p = pi_arr)[0] ## select a single cell cluster\n",
        "    x[i] = np.append(np.random.multivariate_normal(mu[selected_cluster], sigma),\n",
        "                     [np.random.normal(psi[selected_cluster], omega), \n",
        "                      0, selected_cluster, 0, selected_cluster + n_doublet_clusters])\n",
        "  \n",
        "  x[x < 0] = 1e-4\n",
        "  lookups = np.triu_indices(n_clusters) # wanted indices\n",
        "  xx = np.zeros((n_doublet, n_features+5))\n",
        "  for i in range(n_doublet):\n",
        "    selected_cluster = np.random.choice(n_doublet_clusters, p = tau_arr)\n",
        "\n",
        "    indx1 = lookups[0][selected_cluster]\n",
        "    indx2 = lookups[1][selected_cluster]\n",
        "\n",
        "    xx[i] = np.append(np.random.multivariate_normal( (mu[indx1] + mu[indx2])/2, (sigma + sigma)/2 ),\n",
        "                     [np.random.normal( (psi[indx1] + psi[indx2]), omega+omega ), \n",
        "                      1, indx1, indx2, selected_cluster])\n",
        "  xx[xx < 0] = 1e-4\n",
        "  xxx = np.append(x, xx).reshape(n_obs, n_features+5)\n",
        "\n",
        "  truth_theta = {\n",
        "    'log_mu': np.log(mu),\n",
        "    'log_sigma': np.log(sigma),\n",
        "    'log_psi': np.log(psi),\n",
        "    'log_omega': np.log(omega),\n",
        "    \"log_lambda0\": np.log(lambda0_arr),\n",
        "    'log_pi': np.log(pi_arr),\n",
        "    'log_tau': np.log(tau_arr)\n",
        "  }\n",
        "\n",
        "  return xxx[:,:n_features], xxx[:,n_features], xxx, truth_theta\n",
        "\n",
        "  #return torch.tensor(xxx[:,:n_features]), torch.tensor(xxx[:,n_features]), torch.tensor(xxx), [mu, sigma, psi, omega], [lambda0_arr, pi_arr, tau_arr]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onvr3wPluMep"
      },
      "source": [
        "def compute_p_y_given_z(Y, Theta):\n",
        "  \"\"\" Returns NxC\n",
        "  p(y_n | z_n = c)\n",
        "  \"\"\"\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma'])\n",
        "\n",
        "  dist_Y = D.Normal(mu, sigma)\n",
        "  return dist_Y.log_prob(Y.reshape(Y.shape[0], 1, nf)).sum(2) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_z(S, Theta):\n",
        "  \"\"\" Returns NxC\n",
        "  p(s_n | z_n = c)\n",
        "  \"\"\"\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega'])\n",
        "\n",
        "  dist_S = D.Normal(psi, omega)\n",
        "  return dist_S.log_prob(S.reshape(-1,1)) \n",
        "\n",
        "def compute_p_y_given_gamma(Y, Theta):\n",
        "  \"\"\" NxCxC\n",
        "  p(y_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma'])\n",
        "\n",
        "  mu2 = mu.reshape(1, nc, nf)\n",
        "  mu2 = (mu2 + mu2.permute(1, 0, 2)) / 2.0 # C x C x G matrix \n",
        "\n",
        "  sigma2 = sigma.reshape(1, nc, nf)\n",
        "  sigma2 = (sigma2 + sigma2.permute(1,0,2)) / 2.0\n",
        "\n",
        "  dist_Y2 = D.Normal(mu2, sigma2)\n",
        "  return  dist_Y2.log_prob(Y.reshape(-1, 1, 1, nf)).sum(3) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_gamma(S, Theta):\n",
        "  \"\"\" NxCxC\n",
        "  p(s_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega'])\n",
        "\n",
        "  psi2 = psi.reshape(-1,1)\n",
        "  psi2 = psi2 + psi2.T\n",
        "\n",
        "  omega2 = omega.reshape(-1,1)\n",
        "  omega2 = omega2 + omega2.T\n",
        "\n",
        "  dist_S2 = D.Normal(psi2, omega2)\n",
        "  return dist_S2.log_prob(S.reshape(-1, 1, 1))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb4OWe1ouixI"
      },
      "source": [
        "class BasicForwardNet(nn.Module):\n",
        "  \"\"\"Encoder for when data is input without any encoding\"\"\"\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim = 5):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input = nn.Linear(input_dim, hidden_dim)\n",
        "    self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.output = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.input(x))\n",
        "    out = F.relu(self.linear1(out))\n",
        "    out = self.output(out)\n",
        "        \n",
        "    return F.softmax(out, dim=1), F.log_softmax(out, dim=1) ## r/v/d log_r/log_v/log_d"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDKzLK7fvUMS"
      },
      "source": [
        "def ELBO(Theta, Y, S, q1, log_q1, q2, log_q2):\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'])\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1)).reshape(nc,nc)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'])\n",
        "  \n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta)\n",
        "  p_s_given_z = compute_p_s_given_z(S, Theta)\n",
        "\n",
        "  log_rzd0 = p_s_given_z + p_y_given_z + log_pi + log_delta[0]\n",
        "\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta)\n",
        "  p_s_given_gamma = compute_p_s_given_gamma(S, Theta)\n",
        "\n",
        "  log_vgd1 = p_y_given_gamma + p_s_given_gamma + log_tau + log_delta[1]\n",
        "\n",
        "  #remove_indices = np.tril_indices(nc, -1) ## remove indices\n",
        "  #log_rd1g[:, remove_indices[0], remove_indices[1]] = float(\"NaN\")\n",
        "\n",
        "  #q1 = r.exp() * log_rd0z #; q1[torch.isnan(q1)] = 0.0\n",
        "  #q2 = v.exp() * log_rd1g #; q2[torch.isnan(q2)] = 0.0\n",
        "\n",
        "  #q1 = r * (log_rd0z - log_r) ## r * log_r / r * log_rd0z\n",
        "  #q2 = v.reshape(Y.shape[0], nc, nc) * (log_rd1g - log_v.reshape(Y.shape[0], nc, nc))\n",
        "\n",
        "  #q1 = (d[:,0].reshape(-1,1) * r) * (log_rd0z - log_r) ## r * log_r / r * log_rd0z\n",
        "  #q2 = (d[:,1].reshape(-1,1) * v).reshape(Y.shape[0], nc, nc) * (log_rd1g - log_v.reshape(Y.shape[0], nc, nc))\n",
        "\n",
        "  elbo1 = q1 * (log_rzd0 - log_q1)\n",
        "  elbo2 = q2 * (log_vgd1 - log_q2)\n",
        "\n",
        "  return elbo1.sum() + elbo2.sum()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyZnekKcJY4E"
      },
      "source": [
        "#F.sigmoid(torch.tensor([.3, .6]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xx9Xqzt2Agp",
        "outputId": "b993a3e2-237f-40a4-9483-88c987bc9866"
      },
      "source": [
        "nc = 5; no = 1000; nf = 2\n",
        "\n",
        "Y, S, XX, theta_true = generateData(n_clusters = nc, n_obs = no, n_features = nf)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dyDPpWuwu02"
      },
      "source": [
        "N_INIT = 20\n",
        "\n",
        "Y = np.array(Y)\n",
        "S = np.array(S)\n",
        "\n",
        "kms = [KMeans(nc).fit(Y) for i in range(N_INIT)]\n",
        "inertias = [k.inertia_ for k in kms]\n",
        "km = kms[np.argmin(np.array(inertias))] ## selected \"best\" kmeans based on inertia score\n",
        "init_labels = km.labels_\n",
        "\n",
        "mu_init = np.array([Y[init_labels == i,:].mean(0) for i in np.unique(init_labels)])\n",
        "sigma_init = np.array([Y[init_labels == i,:].std(0) for i in np.unique(init_labels)])\n",
        "psi_init = np.array([S[init_labels == i].mean() for i in np.unique(init_labels)])\n",
        "omega_init = np.array([S[init_labels == i].std() for i in np.unique(init_labels)])\n",
        "pi_init = np.array([np.mean(init_labels == i) for i in np.unique(init_labels)])\n",
        "tau_init = np.ones((nc,nc))\n",
        "tau_init = tau_init / tau_init.sum()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApeDIuoCvRk5"
      },
      "source": [
        "P = Y.shape[1] + 1\n",
        "r_net = BasicForwardNet(P, nc)\n",
        "v_net = BasicForwardNet(P, nc ** 2) \n",
        "d_net = BasicForwardNet(P, 2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIHkZCfW4cA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb5e32e-1463-43ab-daef-6f56ace33df5"
      },
      "source": [
        "Theta = {\n",
        "    'log_mu': np.log(mu_init) + 0.05 * np.random.randn(mu_init.shape[0], mu_init.shape[1]),\n",
        "    'log_sigma': np.log(sigma_init), #np.zeros_like(sigma_init),\n",
        "    'log_psi': np.log(psi_init),\n",
        "    'log_omega': np.log(omega_init),\n",
        "    \"is_delta\": F.log_softmax(torch.tensor([0.95, 1-0.95])),\n",
        "    'is_pi': F.log_softmax(torch.tensor(pi_init)),\n",
        "    'is_tau': F.log_softmax(torch.tensor(tau_init))\n",
        "}\n",
        "Theta = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "\n",
        "#Theta['is_delta'].requires_grad = False\n",
        "#Theta['is_pi'].requires_grad = False\n",
        "#Theta['is_tau'].requires_grad = False"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JJj8U-O3Chf"
      },
      "source": [
        "Y = torch.tensor(Y)\n",
        "S = torch.tensor(S)\n",
        "YS = torch.hstack((Y,S.reshape(-1,1))).float()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_e_N_--2lWU"
      },
      "source": [
        "N_ITER = 50000\n",
        "lr = 1e-4\n",
        "tol = 1e-3\n",
        "params = list(Theta.values()) + list(r_net.parameters()) + list(v_net.parameters()) + list(d_net.parameters())\n",
        "opt = optim.AdamW(params, lr=lr)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdeRpKNYEVoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba315f05-8b81-4bb1-c503-dd0169cdf255"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4f_Y5KOE9jh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "4930001c-7b59-43ca-cea9-41b41f02b63a"
      },
      "source": [
        "wandb.init(project='jett-vi',\n",
        "           config={\n",
        "    \"N_EPOCHS\": N_ITER,\n",
        "    \"LR\": lr,\n",
        "    \"TOL\": tol,\n",
        "    'MODEL_TYPE': 'vi',\n",
        "    'DATA_TYPE': 'toy_data'\n",
        "    })"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myujulee\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.1<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">exalted-pyramid-15</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/yujulee/jett-vi\" target=\"_blank\">https://wandb.ai/yujulee/jett-vi</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/yujulee/jett-vi/runs/3f8qoqtx\" target=\"_blank\">https://wandb.ai/yujulee/jett-vi/runs/3f8qoqtx</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210831_183936-3f8qoqtx</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f8212b78c50>"
            ],
            "text/html": [
              "<h1>Run(3f8qoqtx)</h1><iframe src=\"https://wandb.ai/yujulee/jett-vi/runs/3f8qoqtx\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm5DFrPwtmeS",
        "outputId": "bf1975d4-6b89-4efe-e61b-171659406ea6"
      },
      "source": [
        "#loss = []\n",
        "for i in range(N_ITER):\n",
        "  \n",
        "  opt.zero_grad()\n",
        "  r, log_r = r_net(YS)\n",
        "  v, log_v = v_net(YS)\n",
        "  d, log_d = d_net(YS)\n",
        "  \n",
        "  q1 = d[:,0].reshape(-1,1) * r\n",
        "  log_q1 = log_d[:,0].reshape(-1,1) + log_r\n",
        "\n",
        "  q2 = (d[:,1].reshape(-1,1) * v).reshape(Y.shape[0], nc, nc)\n",
        "  log_q2 = (log_d[:,1].reshape(-1,1) + log_v).reshape(Y.shape[0], nc, nc)\n",
        "\n",
        "  #assert( ((d_net(YS)[0][:,1].reshape(-1,1) * v_net(YS)[0]).sum(1) + (d_net(YS)[0][:,0].reshape(-1,1) * r_net(YS)[0]).sum(1)).sum() == 1000. )\n",
        "\n",
        "  nelbo = -ELBO(Theta, Y, S, q1, log_q1, q2, log_q2)\n",
        "  nelbo.backward()\n",
        "  opt.step()\n",
        "  \n",
        "  if i % (1000 - 1) == 0:\n",
        "    print(\"NELBO: {}; lambda: {}\".format(nelbo.detach(), F.log_softmax(Theta['is_delta'].detach()).exp()))\n",
        "  \n",
        "  if i > 0 and abs(loss[-1] + nelbo.detach().sum()) < tol:\n",
        "    break\n",
        "           \n",
        "  loss.append(-nelbo.detach().sum())\n",
        "  \n",
        "  '''\n",
        "  wandb.log({\n",
        "    'ITER': i + 1, \n",
        "    'elbo': elbo.detach(),\n",
        "    'log_mu': Theta['log_mu'],\n",
        "    'log_sigma': Theta['log_sigma'], #np.zeros_like(sigma_init),\n",
        "    'log_psi': Theta['log_psi'],\n",
        "    'log_omega': Theta['log_omega'],\n",
        "    \"is_delta\": Theta['is_delta'],\n",
        "    'is_pi': Theta['is_pi'],\n",
        "    'is_tau': Theta['is_tau'],\n",
        "    'r': r,\n",
        "    'v': v,\n",
        "    'd': d,\n",
        "      })\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NELBO: 4711.335402218515; lambda: tensor([0.7555, 0.2445])\n",
            "NELBO: 4675.178250545157; lambda: tensor([0.7590, 0.2410])\n",
            "NELBO: 4639.460127334241; lambda: tensor([0.7625, 0.2375])\n",
            "NELBO: 4604.096164185553; lambda: tensor([0.7659, 0.2341])\n",
            "NELBO: 4568.95755924212; lambda: tensor([0.7693, 0.2307])\n",
            "NELBO: 4533.798675935918; lambda: tensor([0.7726, 0.2274])\n",
            "NELBO: 4498.364920571923; lambda: tensor([0.7759, 0.2241])\n",
            "NELBO: 4462.518500089011; lambda: tensor([0.7792, 0.2208])\n",
            "NELBO: 4426.053426691622; lambda: tensor([0.7824, 0.2176])\n",
            "NELBO: 4388.692741426011; lambda: tensor([0.7856, 0.2144])\n",
            "NELBO: 4350.275187313555; lambda: tensor([0.7886, 0.2114])\n",
            "NELBO: 4311.027033592249; lambda: tensor([0.7916, 0.2084])\n",
            "NELBO: 4271.816261392525; lambda: tensor([0.7945, 0.2055])\n",
            "NELBO: 4233.2705610058465; lambda: tensor([0.7974, 0.2026])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "csC4ugVvgchK",
        "outputId": "fbf4ddfd-4f73-4476-cb71-f33315b10053"
      },
      "source": [
        "ddd"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fb145a0fef0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ddd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDtVIym2y4eS"
      },
      "source": [
        "F.log_softmax(Theta['is_delta']).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXPBFeNGYhjq"
      },
      "source": [
        "np.exp(theta_true['log_lambda0'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZThbK27Z_-sG"
      },
      "source": [
        "F.log_softmax(Theta['is_pi']).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_hzSFsLMBZz"
      },
      "source": [
        "np.exp(theta_true['log_pi'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6gkB7uDPNI-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WS9NxwqNxqJ"
      },
      "source": [
        "#F.log_softmax(v.mean(0).reshape(-1)).reshape(nc, nc).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRpGlo4nAF6F"
      },
      "source": [
        "F.log_softmax(Theta['is_tau'].reshape(-1)).reshape(nc, nc).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdbkCJKWMLud"
      },
      "source": [
        "np.exp(theta_true['log_tau'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjEgccaxPYHj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPWjfsZnAWXx"
      },
      "source": [
        "Theta['log_mu'].exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIidtiH6m4gx"
      },
      "source": [
        "np.exp(theta_true['log_mu'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uFyXRli1kOY"
      },
      "source": [
        "lus = np.triu_indices(nc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-1bmqwf1kBs"
      },
      "source": [
        "#from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "part1 = int(0.7 * XX.shape[0])\n",
        "part2 = int((XX.shape[0] - part1) / 2)\n",
        "part3 = XX.shape[0] - part1 - part2\n",
        "\n",
        "train, valid, test = random_split(torch.Tensor(XX), [part1, part2, part3], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "trainloader = DataLoader(train, batch_size=256, shuffle=True)\n",
        "validloader = DataLoader(valid, batch_size=256, shuffle=False)\n",
        "testloader = DataLoader(test, batch_size=256, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdL2OQLQKi86"
      },
      "source": [
        "'''\n",
        "for epoch in range(N_ITER):\n",
        "  \n",
        "  for j, batch_data in enumerate(trainloader):\n",
        "    \n",
        "    bY = torch.tensor(batch_data[:,:nf])\n",
        "    bS = torch.tensor(batch_data[:,nf])\n",
        "    bYS = torch.hstack((bY,bS.reshape(-1,1))).float()\n",
        "\n",
        "    #print(bY.shape)\n",
        "    #print(bS.shape)\n",
        "    opt.zero_grad()\n",
        "\n",
        "    r, log_r = r_net(bYS)\n",
        "    v, log_v = v_net(bYS)\n",
        "    d, log_d = d_net(bYS)\n",
        "  \n",
        "    loss = -ELBO(Theta, bY, bS, r, log_r, v, log_v, d, log_d)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  \n",
        "  if epoch % (1000 - 1) == 0:\n",
        "    print(loss.detach())\n",
        "    #print(F.log_softmax(d.mean(0)).exp())\n",
        "    #print(F.log_softmax(Theta['is_pi']).exp())\n",
        "    #print(loss.detach())\n",
        "    #print(Theta['is_delta'].detach().exp())\n",
        "    #print(Theta['is_pi'].detach().exp())\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9PuVTIn-nG_"
      },
      "source": [
        "F.log_softmax(Theta['is_delta']).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjMcn4sP-wft"
      },
      "source": [
        "F.log_softmax(Theta['is_pi']).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M824L26-_G4D"
      },
      "source": [
        "F.log_softmax(Theta['is_tau'].reshape(-1)).reshape(nc, nc).exp()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}