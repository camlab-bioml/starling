{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "damm_011122",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbsBHQpdoQk5/394FCD/vH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camlab-bioml/2021_IMC_Jett/blob/main/damm_011122.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from scipy.stats.mstats import winsorize"
      ],
      "metadata": {
        "id": "CTo0LU1QOEaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_p_y_given_z(Y, Theta, dist, spillover_rate=None):\n",
        "  \n",
        "  ''' return p(x_n | z_n = c)_NxC '''\n",
        "\n",
        "  if spillover_rate is None:\n",
        "    mu = torch.exp(Theta['log_mu'])\n",
        "  else:\n",
        "    l = torch.sigmoid(Theta['is_so']) * spillover_rate\n",
        "    first_half = (1 - l).reshape(-1, 1, 1) * torch.exp(Theta['log_mu'])\n",
        "    second_half = (l.reshape(-1, 1) * SPILLOVER_MAT).reshape(-1, 1, Y.shape[1])\n",
        "    mu = (first_half + second_half).mean(0)\n",
        "    Theta['mnu'] = mu\n",
        "\n",
        "  sigma = torch.exp(Theta['log_sigma'])\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_Y = D.Normal(loc = mu, scale = sigma)\n",
        "    #dist_Y = D.MultivariateNormal(loc = mu, covariance_matrix = sigma.reshape(-1,1,Y.shape[1]) * torch.eye(Y.shape[1]))\n",
        "  elif dist == 'student':\n",
        "    dist_Y = D.StudentT(df = 2.0, loc = mu, scale = sigma)\n",
        "    #dist_Y = MultivariateStudentT(df = 2.0, loc = mu, scale_tril = sigma.reshape(-1,1,Y.shape[1]) * torch.eye(Y.shape[1]))\n",
        "  \n",
        "  Y_reshape = Y.reshape(-1, 1, Y.shape[1])\n",
        "  res = dist_Y.log_prob(Y_reshape)\n",
        "  return res.sum(2) # <- sum because IID over G\n",
        "\n",
        "  #return dist_Y.log_prob(Y.reshape(-1, 1, Y.shape[1]))\n",
        "\n",
        "def compute_p_y_given_gamma(Y, Theta, dist):\n",
        "  \n",
        "  \"\"\" p(y_n | gamma_n = [c,c'])_NxCxC \"\"\"\n",
        "\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  mu_reshape = mu.reshape(1, mu.shape[0], mu.shape[1])\n",
        "  mu2 = (mu + mu_reshape.permute(1, 0, 2)) / 2.0 # C x C x G matrix \n",
        "  #loc_input = mu2.reshape(-1, Y.shape[1])\n",
        "\n",
        "  sigma = torch.exp(Theta['log_sigma'])\n",
        "  sigma_reshape = sigma.reshape(1, mu.shape[0], mu.shape[1])\n",
        "  sigma2 = (sigma + sigma_reshape.permute(1, 0, 2)) / 2.0\n",
        "  #scale_input = sigma2.reshape(loc_input.shape[0], 1, Y.shape[1]) * torch.eye(Y.shape[1])\n",
        "  \n",
        "  if dist == 'normal':\n",
        "    dist_Y2 = D.Normal(loc = mu2, scale = sigma2)\n",
        "    #dist_Y2 = D.MultivariateNormal(loc = loc_input, covariance_matrix = scale_input)\n",
        "  elif dist == 'student':\n",
        "    dist_Y2 = D.StudentT(df = 2.0, loc = mu2, scale = sigma2)\n",
        "    #dist_Y2 = MultivariateStudentT(df = 2.0, loc = loc_input, scale_tril = scale_input)\n",
        "\n",
        "  Y_reshape = Y.reshape(-1, 1, 1, Y.shape[1])\n",
        "  res = dist_Y2.log_prob(Y_reshape)\n",
        "  return res.sum(3) # <- sum because IID over G\n",
        "  #return dist_Y2.log_prob(Y.reshape(-1, 1, Y.shape[1])).reshape(-1, mu.shape[0], mu.shape[0])"
      ],
      "metadata": {
        "id": "97G0THileeoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_p_s_given_z(S, Theta, dist):\n",
        "  \n",
        "  ''' return p(s_n | z_n = c)_NxC '''\n",
        "\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega'])\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_S = D.Normal(loc = psi, scale = omega)\n",
        "  elif dist == 'student':\n",
        "    dist_S = D.StudentT(df = 2.0, loc = psi, scale = omega)\n",
        "\n",
        "  S_reshape = S.reshape(-1,1)\n",
        "  return dist_S.log_prob(S_reshape) \n",
        "\n",
        "def compute_p_s_given_gamma(S, Theta, dist):\n",
        "  \n",
        "  \"\"\" p(s_n | gamma_n = [c,c'])_NxCxC \"\"\"\n",
        "\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  psi_reshape = psi.reshape(1,-1)\n",
        "  psi2 = psi + psi_reshape.permute(1,0)\n",
        "\n",
        "  omega = torch.exp(Theta['log_omega'])\n",
        "  omega_reshape = omega.reshape(1,-1)\n",
        "  omega2 = omega + omega_reshape.permute(1,0)\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_S2 = D.Normal(loc = psi2, scale = omega2)\n",
        "  elif dist == 'student':\n",
        "    dist_S2 = D.StudentT(df = 2.0, loc = psi2, scale = omega2)\n",
        "  \n",
        "  S_reshape = S.reshape(-1, 1, 1)\n",
        "  return dist_S2.log_prob(S_reshape)"
      ],
      "metadata": {
        "id": "MKkw6RaiZ4ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_init(Y, S, k):\n",
        "  \n",
        "  kms = KMeans(k).fit(Y)\n",
        "  init_labels = kms.labels_\n",
        "  init_label_class = np.unique(init_labels)\n",
        "\n",
        "  mu_init = np.array([Y[init_labels == c,:].mean(0) for c in init_label_class])\n",
        "  sigma_init = np.array([Y[init_labels == c,:].std(0) for c in init_label_class])\n",
        "  #sigma_init = np.array([X[init_labels == c,:].var(0) for c in init_label_class]) ## D.MultivariateNormal (no covariance)\n",
        "\n",
        "  pi_init = np.array([np.mean(init_labels == c) for c in init_label_class])\n",
        "  tau_init = np.ones((k, k))\n",
        "  tau_init = tau_init / tau_init.sum()\n",
        "\n",
        "  Theta = {\n",
        "    'log_mu': np.log(mu_init),\n",
        "    'log_sigma': np.log(sigma_init), #np.zeros_like(sigma_init),\n",
        "    'is_delta': np.log([0.95, 0.05]),\n",
        "    'is_pi': np.log(pi_init),\n",
        "    'is_tau': np.log(tau_init),\n",
        "    'is_so': torch.randn(Y.shape[0]),\n",
        "    'mnu': torch.zeros(Y.shape[0], k, Y.shape[1])\n",
        "  }\n",
        "\n",
        "  if S is not None:\n",
        "    psi_init = np.array([S[init_labels == c].mean() for c in init_label_class])\n",
        "    omega_init = np.array([S[init_labels == c].std() for c in init_label_class])\n",
        "\n",
        "    Theta['log_psi'] = np.log(psi_init),\n",
        "    Theta['log_omega'] = np.log(omega_init)\n",
        "    \n",
        "  Theta = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "  Theta['is_delta'].requires_grad = False\n",
        "  Theta['mnu'].requires_grad = False\n",
        "\n",
        "  return Theta\n",
        "\n",
        "def simulate_data(Y, S): ## use real data to simulate singlets/doublets\n",
        "  \n",
        "  ''' return same number of cells as in Y/S, half of them are singlets and another half are doublets '''\n",
        "\n",
        "  #N_training = 5000\n",
        "  sample_size = int(Y.shape[0]/2)\n",
        "  idx_singlet = np.random.choice(Y.shape[0], size = sample_size, replace=True)\n",
        "  Y_singlet = Y[idx_singlet,:] ## expression\n",
        "  \n",
        "  idx_doublet = [np.random.choice(Y.shape[0], size = sample_size), np.random.choice(Y.shape[0], size = sample_size)]\n",
        "  Y_doublet = (Y[idx_doublet[0],:] + Y[idx_doublet[1],:])/2.\n",
        "  \n",
        "  fake_Y = torch.tensor(np.vstack([Y_singlet, Y_doublet]))\n",
        "  fake_label = torch.tensor(np.concatenate([np.ones(sample_size), np.zeros(sample_size)]))\n",
        "\n",
        "  if S is None:\n",
        "    return fake_Y, None, fake_label\n",
        "  else:\n",
        "    S_singlet = S[idx_singlet]\n",
        "    S_doublet = S[idx_doublet[0]] + S[idx_doublet[1]]  \n",
        "    fake_S = torch.tensor(np.hstack([S_singlet, S_doublet]))\n",
        "    return fake_Y, fake_S, fake_label ## have cell size and create fake cell size\n",
        "\n",
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, *datasets):\n",
        "    self.datasets = datasets\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return tuple(d[i] for d in self.datasets)\n",
        "\n",
        "  def __len__(self):\n",
        "    return min(len(d) for d in self.datasets)\n",
        "\n",
        "def compute_nll_posteriors_p_singlet(Y, S, Theta, dist, spillover_rate=None):\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'])\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1)).reshape(log_pi.shape[0], log_pi.shape[0])\n",
        "  log_delta = F.log_softmax(Theta['is_delta'])\n",
        "\n",
        "  prob_y_given_z = compute_p_y_given_z(Y, Theta, dist, spillover_rate) ## p(y_n|z=c)\n",
        "  prob_data_given_z_d0 = prob_y_given_z + log_pi\n",
        "  \n",
        "  if S is not None:\n",
        "    prob_s_given_z = compute_p_s_given_z(S, Theta, dist) ## p(data_n|z=c)\n",
        "    prob_data_given_z_d0 += prob_s_given_z ## p(data_n|z=c,d=0) -> NxC\n",
        "  \n",
        "  prob_y_given_gamma = compute_p_y_given_gamma(Y, Theta, dist) ## p(y_n|g=[c,c']) -> NxCxC\n",
        "  prob_data_given_gamma_d1 = prob_y_given_gamma + log_tau\n",
        "  \n",
        "  if S is not None:\n",
        "    prob_s_given_gamma = compute_p_s_given_gamma(S, Theta, dist) ## p(s_n|g=[c,c']) -> NxCxC\n",
        "    prob_data_given_gamma_d1 += prob_s_given_gamma ## p(data_n|d=1) -> NxCxC\n",
        "\n",
        "  #p_data = torch.cat([prob_data_given_z_d0 + log_delta[0], prob_data_given_gamma_d1.reshape(X.shape[0], -1) + log_delta[1]], dim=1)\n",
        "  prob_data = torch.hstack([prob_data_given_z_d0 + log_delta[0], prob_data_given_gamma_d1.reshape(Y.shape[0], -1) + log_delta[1]]) ## p(data)\n",
        "  prob_data_norm = torch.logsumexp(prob_data, dim=1)\n",
        "\n",
        "  r = prob_data_given_z_d0.T + log_delta[0] - prob_data_norm ## p(d=0,z=c|data)\n",
        "  v = prob_data_given_gamma_d1.T + log_delta[1] - prob_data_norm ## p(gamma=[c,c']|data)\n",
        "\n",
        "  ## normalize\n",
        "  prob_data_given_d0 = torch.logsumexp(prob_data_given_z_d0, dim=1) ## p(data_n|d=0)_N\n",
        "  prob_singlet = torch.exp(prob_data_given_d0 + log_delta[0] - prob_data_norm)\n",
        "\n",
        "  ## average negative likelihood scores\n",
        "  cost = -torch.logsumexp(prob_data, dim=1).mean()\n",
        "\n",
        "  return cost, prob_singlet, r.T, v.T"
      ],
      "metadata": {
        "id": "rHm7hgzvJMlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepLoader(Y, S, incSimDat, incSimCellSize, BatchSize):\n",
        "  \n",
        "  if incSimDat:\n",
        "    if incSimCellSize:\n",
        "      fake_Y, fake_S, fake_L = simulate_data(Y, S) ## use real data to simulate singlets/doublets\n",
        "    else:\n",
        "      fake_Y, fake_S, fake_L = simulate_data(Y, None) ## use real data to simulate singlets/doublets\n",
        "\n",
        "    if S is None:\n",
        "      df = ConcatDataset(Y, fake_Y, fake_L)\n",
        "    else:\n",
        "      if fake_S is None:\n",
        "        df = ConcatDataset(Y, S, fake_Y, fake_L)\n",
        "      else:\n",
        "        df = ConcatDataset(Y, S, fake_Y, fake_S, fake_L)\n",
        "  else:\n",
        "    if S is None:\n",
        "      df = ConcatDataset(Y)\n",
        "    else:\n",
        "      df = ConcatDataset(Y, S)\n",
        "\n",
        "  return torch.utils.data.DataLoader(df, batch_size=BatchSize, shuffle=True)\n",
        "\n",
        "def batchSet(batch, incSimDat):\n",
        "  if incSimDat:\n",
        "    if len(batch) == 3:\n",
        "      return bat[0], None, bat[1], None, bat[2]\n",
        "      #bY = bat[0]; bS = None; bFY = bat[1]; bFS = None; bFL = bat[2]\n",
        "    elif len(batch) == 4:\n",
        "      return bat[0], bat[1], bat[2], None, bat[3]\n",
        "      #bY = bat[0]; bS = bat[1]; bFY = bat[2]; bFS = None; bFL = bat[3]\n",
        "    else:\n",
        "      return bat[0], bat[1], bat[2], bat[3], bat[4]\n",
        "      #bY = bat[0]; bS = bat[1]; bFY = bat[2]; bFS = bat[3]; bFL = bat[4]\n",
        "  else:\n",
        "    if len(batch) == 1:\n",
        "      return bat[0], None, None, None, None\n",
        "      #bY = bat[0]; bS = None; bFY = None; bFS = None; bFL = None\n",
        "    else:\n",
        "      return bat[0], bat[1], None, None, None\n",
        "      #bY = bat[0]; bS = bat[1]; bFY = None; bFS = None; bFL = None\n",
        "\n",
        "def batchTrain(batch, Theta, dist, spillover_rate=None, incSimDat=False, regularized_val=1000):\n",
        "\n",
        "  bY, bS, bFY, bFS, bFL = batchSet(batch, incSimDat)\n",
        "\n",
        "  opt.zero_grad()\n",
        "\n",
        "  #rnll, _, _, _ = nll(bY, bS, Theta, noiseModel, False)\n",
        "  rnll, _, _, _ = compute_nll_posteriors_p_singlet(bY, bS, Theta, dist, spillover_rate)\n",
        "\n",
        "  if incSimDat:\n",
        "    #fnll, p_fake_singlet, _, _ = nll(bFY, bFS, Theta, noiseModel, False)\n",
        "    fnll, p_fake_singlet, _, _  = compute_nll_posteriors_p_singlet(bFY, bFS, Theta, dist, None)\n",
        "    floss = nn.BCELoss()(p_fake_singlet, bFL) ## want to min \n",
        "    closs = rnll + regularized_val * floss\n",
        "    closs.backward()\n",
        "    opt.step()\n",
        "    return closs.detach(), rnll.detach(), fnll.detach(), floss.detach()\n",
        "  else:\n",
        "    rnll.backward()\n",
        "    opt.step()\n",
        "    return rnll.detach(), 0, 0, 0"
      ],
      "metadata": {
        "id": "QpnlQ7_KDN8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCQ67TSKChgY",
        "outputId": "e090931a-44eb-45c7-ae2c-4c027fa961ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scanpy\n",
        "import scanpy as sc\n",
        "#imc_data = sc.read_h5ad(\"DAMM/data/mouse_5k.h5ad\")\n",
        "imc_data = sc.read_h5ad(\"DAMM/data/basel_cohort_single_cell_expression.h5ad\")\n",
        "SPILLOVER_MAT = torch.load(\"DAMM/data/basel_mask_spillover\") ## should we winsorize this?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJyJwE6YCnt2",
        "outputId": "ba5c446e-97d7-494c-e60c-8f7010cba73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = imc_data.X.copy() ## expressions\n",
        "S = winsorize(imc_data.obs['size'], limits=[0, 0.01]).data ## winsorize cell sizes\n",
        "\n",
        "no, nf = imc_data.shape #number cells (observations) & proteins (features)\n",
        "\n",
        "for i in range(nf):\n",
        "  X[:,i] = winsorize(X[:,i], limits=[0, 0.01]).data ## winsorize cell expressions"
      ],
      "metadata": {
        "id": "PP1pbApoCuuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NC = 5\n",
        "MAX_EPOCH = 50\n",
        "BATCH_SIZE = 128\n",
        "noiseModel = 'student'\n",
        "TOL = 1e-3 #converagence criterion\n",
        "\n",
        "incSimDat = True\n",
        "incSimCellSize = True\n",
        "regularized_val = 1000 ## or None\n",
        "\n",
        "spillover_rate = None"
      ],
      "metadata": {
        "id": "PVD4ZRBSCsdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Theta = kmeans_init(X, S, NC)\n",
        "Y = torch.tensor(X)\n",
        "S = torch.tensor(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spJ3gSP9Dttp",
        "outputId": "4af5b22b-3893-4580-d858-dd67eb5b7a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optim.Adam(Theta.values())\n",
        "train_loader = prepLoader(Y, S, incSimDat, incSimCellSize, BATCH_SIZE)\n",
        "\n",
        "loss = []\n",
        "for epoch in range(MAX_EPOCH):\n",
        "\n",
        "  with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "  \n",
        "    rnlls = 0; fnlls = 0; floss = 0; tloss = 0\n",
        "    for j, bat in enumerate(tepoch):\n",
        "      \n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "      \n",
        "      tot_loss, real_nll, fake_nll, fake_loss = batchTrain(bat, Theta, noiseModel, spillover_rate, incSimDat, regularized_val)\n",
        "\n",
        "      tloss += tot_loss\n",
        "      rnlls += real_nll\n",
        "      fnlls += fake_nll\n",
        "      floss += fake_loss\n",
        "\n",
        "      tepoch.set_postfix(loss=tloss.item())\n",
        "      #print(tot_loss.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      loss.append(tloss)\n",
        "      print('Epoch: {}: Loss: {}'.format(epoch, loss[-1]))\n",
        "      \n",
        "      if epoch > 10 and abs(np.mean(loss[-5:]) - np.mean(loss[-6:-1])) < TOL:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz1t3qiwsFmD",
        "outputId": "548e2b24-3bc2-4253-aef3-cb18e5eb1bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:   0%|          | 0/6271 [00:00<?, ?batch/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "Epoch 0: 100%|██████████| 6271/6271 [02:33<00:00, 40.74batch/s, loss=3.3e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0: Loss: 3298017.100899173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 6271/6271 [02:33<00:00, 40.74batch/s, loss=2.63e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1: Loss: 2629183.510330238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 6271/6271 [02:33<00:00, 40.77batch/s, loss=2.57e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2: Loss: 2573556.566849665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 6271/6271 [02:34<00:00, 40.68batch/s, loss=2.54e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3: Loss: 2536742.445507876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 6271/6271 [02:34<00:00, 40.68batch/s, loss=2.51e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4: Loss: 2511660.669029004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 6271/6271 [02:34<00:00, 40.51batch/s, loss=2.49e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5: Loss: 2493489.019274751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 6271/6271 [02:36<00:00, 40.01batch/s, loss=2.48e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6: Loss: 2478865.200222315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 6271/6271 [02:39<00:00, 39.25batch/s, loss=2.47e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7: Loss: 2466543.5282160193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 6271/6271 [02:40<00:00, 39.10batch/s, loss=2.46e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8: Loss: 2455889.392090912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 6271/6271 [02:39<00:00, 39.21batch/s, loss=2.45e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9: Loss: 2446730.4346180256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 6271/6271 [02:40<00:00, 39.16batch/s, loss=2.44e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10: Loss: 2438111.218512453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 6271/6271 [02:39<00:00, 39.21batch/s, loss=2.43e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11: Loss: 2430531.9071918493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 6271/6271 [02:39<00:00, 39.25batch/s, loss=2.42e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12: Loss: 2423731.063210821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 6271/6271 [02:37<00:00, 39.72batch/s, loss=2.42e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13: Loss: 2417694.50114292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 6271/6271 [02:38<00:00, 39.55batch/s, loss=2.41e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14: Loss: 2412651.173956381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 6271/6271 [02:37<00:00, 39.84batch/s, loss=2.41e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15: Loss: 2408465.874113013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 6271/6271 [02:37<00:00, 39.90batch/s, loss=2.41e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16: Loss: 2405046.8494217135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 6271/6271 [02:41<00:00, 38.90batch/s, loss=2.4e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17: Loss: 2401316.35489583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 6271/6271 [02:40<00:00, 39.04batch/s, loss=2.4e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18: Loss: 2398817.323984304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 6271/6271 [02:44<00:00, 38.19batch/s, loss=2.4e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19: Loss: 2395927.1943968036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 6271/6271 [02:42<00:00, 38.55batch/s, loss=2.39e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20: Loss: 2394189.3150067767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|██████████| 6271/6271 [02:43<00:00, 38.27batch/s, loss=2.39e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21: Loss: 2391782.726567285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|██████████| 6271/6271 [02:43<00:00, 38.41batch/s, loss=2.39e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22: Loss: 2390278.9470099215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████| 6271/6271 [02:40<00:00, 39.02batch/s, loss=2.39e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23: Loss: 2388578.8661501883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|██████████| 6271/6271 [02:39<00:00, 39.23batch/s, loss=2.39e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24: Loss: 2386892.7617484326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|██████████| 6271/6271 [02:41<00:00, 38.88batch/s, loss=2.39e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25: Loss: 2385855.4580163746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████| 6271/6271 [02:42<00:00, 38.67batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26: Loss: 2384517.7738934355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|██████████| 6271/6271 [02:41<00:00, 38.72batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 27: Loss: 2383774.015577328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|██████████| 6271/6271 [02:41<00:00, 38.91batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 28: Loss: 2382391.3270837753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████| 6271/6271 [02:44<00:00, 38.06batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 29: Loss: 2382263.007541943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|██████████| 6271/6271 [02:42<00:00, 38.68batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30: Loss: 2381219.5095238658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31: 100%|██████████| 6271/6271 [02:43<00:00, 38.44batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31: Loss: 2380738.1850750176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32: 100%|██████████| 6271/6271 [02:43<00:00, 38.38batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 32: Loss: 2380322.318315875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33: 100%|██████████| 6271/6271 [02:43<00:00, 38.28batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 33: Loss: 2379787.3650349546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34: 100%|██████████| 6271/6271 [02:43<00:00, 38.40batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 34: Loss: 2378892.1018270184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35: 100%|██████████| 6271/6271 [02:44<00:00, 38.16batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 35: Loss: 2378842.952868787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36: 100%|██████████| 6271/6271 [02:46<00:00, 37.69batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 36: Loss: 2378262.217773454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37: 100%|██████████| 6271/6271 [02:44<00:00, 38.16batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 37: Loss: 2377887.591316088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38: 100%|██████████| 6271/6271 [02:46<00:00, 37.76batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 38: Loss: 2377561.401906977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39: 100%|██████████| 6271/6271 [02:45<00:00, 37.81batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 39: Loss: 2377008.484215345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40: 100%|██████████| 6271/6271 [02:45<00:00, 37.86batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40: Loss: 2376917.3875858905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41: 100%|██████████| 6271/6271 [02:47<00:00, 37.42batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 41: Loss: 2376402.3200523793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42: 100%|██████████| 6271/6271 [02:47<00:00, 37.55batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 42: Loss: 2376109.8330418933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43: 100%|██████████| 6271/6271 [02:45<00:00, 37.84batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 43: Loss: 2376221.25752942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44: 100%|██████████| 6271/6271 [02:45<00:00, 37.92batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 44: Loss: 2375699.3041182575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45: 100%|██████████| 6271/6271 [02:43<00:00, 38.31batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 45: Loss: 2375280.1971438434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46: 100%|██████████| 6271/6271 [02:47<00:00, 37.35batch/s, loss=2.37e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 46: Loss: 2374839.968107286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47: 100%|██████████| 6271/6271 [02:46<00:00, 37.70batch/s, loss=2.38e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 47: Loss: 2375129.5498166466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48: 100%|██████████| 6271/6271 [02:44<00:00, 38.18batch/s, loss=2.37e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 48: Loss: 2374466.155406171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49: 100%|██████████| 6271/6271 [02:44<00:00, 38.20batch/s, loss=2.37e+6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 49: Loss: 2374124.78787979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DSdVbkjXabeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1OlTdatqabVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DxV6Gu_fabLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PYhla36uERSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}