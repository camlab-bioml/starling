{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "real_all_versions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPI4flZKqsRylBpACVpiON/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camlab-bioml/2021_IMC_Jett/blob/main/real_all_versions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFGULyyUOoN9"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade\n",
        "\n",
        "%pip install scanpy\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import random\n",
        "\n",
        "def compute_p_y_given_z(Y, Theta, reg=1e-6):\n",
        "  \"\"\" Returns NxC\n",
        "  p(y_n | z_n = c)\n",
        "  \"\"\"\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma']) + reg\n",
        "\n",
        "  dist_Y = D.Normal(mu, sigma)\n",
        "  return dist_Y.log_prob(Y.reshape(Y.shape[0], 1, NF)).sum(2) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_z(S, Theta, reg=1e-6):\n",
        "  \"\"\" Returns NxC\n",
        "  p(s_n | z_n = c)\n",
        "  \"\"\"\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega']) + reg\n",
        "\n",
        "  dist_S = D.Normal(psi, omega)\n",
        "  return dist_S.log_prob(S.reshape(-1,1)) \n",
        "\n",
        "def compute_p_y_given_gamma(Y, Theta, reg=1e-6):\n",
        "  \"\"\" NxCxC\n",
        "  p(y_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma']) + reg\n",
        "\n",
        "  mu2 = mu.reshape(1, NC, NF)\n",
        "  mu2 = (mu2 + mu2.permute(1, 0, 2)) / 2.0 # C x C x G matrix \n",
        "\n",
        "  sigma2 = sigma.reshape(1, NC, NF)\n",
        "  sigma2 = (sigma2 + sigma2.permute(1,0,2)) / 2.0\n",
        "\n",
        "  dist_Y2 = D.Normal(mu2, sigma2)\n",
        "  return  dist_Y2.log_prob(Y.reshape(-1, 1, 1, NF)).sum(3) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_gamma(S, Theta, reg=1e-6):\n",
        "  \"\"\" NxCxC\n",
        "  p(s_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega']) + reg\n",
        "\n",
        "  psi2 = psi.reshape(-1,1)\n",
        "  psi2 = psi2 + psi2.T\n",
        "\n",
        "  omega2 = omega.reshape(-1,1)\n",
        "  omega2 = omega2 + omega2.T\n",
        "\n",
        "  dist_S2 = D.Normal(psi2, omega2)\n",
        "  return dist_S2.log_prob(S.reshape(-1, 1, 1))\n",
        "\n",
        "def compute_r_v_2(Y, S, Theta):\n",
        "  \"\"\"Need to compute\n",
        "  p(gamma = [c,c'], d= 1 | Y,S)\n",
        "  p(z = c, d=0 | Y,S)\n",
        "  \"\"\"\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "\n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta)\n",
        "  p_s_given_z = compute_p_s_given_z(S, Theta)\n",
        "\n",
        "  p_data_given_z_d0 = p_y_given_z + p_s_given_z + log_pi\n",
        "  p_data_given_d0 = torch.logsumexp(p_data_given_z_d0, dim=1) # this is p(data|d=0)\n",
        "\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta)\n",
        "  p_s_given_gamma = compute_p_s_given_gamma(S, Theta)\n",
        "\n",
        "  p_data_given_gamma_d1 = (p_y_given_gamma + p_s_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "\n",
        "  # p_data_given_d1 = torch.logsumexp(p_data_given_gamma_d1, dim=1)\n",
        "\n",
        "  p_data = torch.cat([p_data_given_z_d0 + log_delta[0], p_data_given_gamma_d1 + log_delta[1]], dim=1)\n",
        "  p_data = torch.logsumexp(p_data, dim=1)\n",
        "\n",
        "  r = p_data_given_z_d0.T + log_delta[0] - p_data\n",
        "  v = p_data_given_gamma_d1.T + log_delta[1] - p_data\n",
        "\n",
        "  p_singlet = torch.exp(p_data_given_d0 + log_delta[0] - p_data)\n",
        "\n",
        "  return r.T, v.T.reshape(-1,NC,NC), p_data.sum(), p_singlet #, p_assign, p_assign1\n",
        "\n",
        "## for EM version\n",
        "def Q(Theta, Y, S, r, v, ignored_indices):\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "\n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta)\n",
        "  p_s_given_z = compute_p_s_given_z(S, Theta)\n",
        "\n",
        "  log_rd0z = p_s_given_z + p_y_given_z + log_pi + log_delta[0]\n",
        "\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta)\n",
        "  p_s_given_gamma = compute_p_s_given_gamma(S, Theta)\n",
        "\n",
        "  log_rd1g = p_y_given_gamma + p_s_given_gamma + log_tau + log_delta[1] # can use torch.triu to get upper triangle\n",
        "\n",
        "  #remove_indices = np.tril_indices(nc, -1) ## remove indices\n",
        "  #log_rd1g[:, remove_indices[0], remove_indices[1]] = float(\"NaN\")\n",
        "\n",
        "  q1 = log_rd0z * r.exp() #; q1[torch.isnan(q1)] = 0.0\n",
        "  q2 = log_rd1g * v.exp() #; q2[torch.isnan(q2)] = 0.0\n",
        "\n",
        "  #print(\"{} {} {}\".format(log_rd1g.shape, v.shape, q2.shape))\n",
        "  return q1.sum() + q2.sum()\n",
        "\n",
        "def ll(Y, S, Theta):\n",
        "  \"\"\"compute\n",
        "  p(gamma = [c,c'], d= 1 | Y,S)\n",
        "  p(z = c, d=0 | Y,S)\n",
        "  \"\"\"\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "\n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta)\n",
        "  p_s_given_z = compute_p_s_given_z(S, Theta)\n",
        "\n",
        "  p_data_given_z_d0 = p_y_given_z + p_s_given_z + log_pi\n",
        "  p_data_given_d0 = torch.logsumexp(p_data_given_z_d0, dim=1) # this is p(data|d=0)\n",
        "\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta)\n",
        "  p_s_given_gamma = compute_p_s_given_gamma(S, Theta)\n",
        "\n",
        "  p_data_given_gamma_d1 = (p_y_given_gamma + p_s_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "\n",
        "  # p_data_given_d1 = torch.logsumexp(p_data_given_gamma_d1, dim=1)\n",
        "\n",
        "  p_data = torch.cat([p_data_given_z_d0 + log_delta[0], p_data_given_gamma_d1 + log_delta[1]], dim=1)\n",
        "  #p_data = torch.logsumexp(p_data, dim=1)\n",
        "\n",
        "  #r = p_data_given_z_d0.T + log_delta[0] - p_data\n",
        "  #v = p_data_given_gamma_d1.T + log_delta[1] - p_data\n",
        "\n",
        "  #p_singlet = torch.exp(p_data_given_d0 + log_delta[0] - p_data)\n",
        "\n",
        "  #return r.T, v.T.reshape(-1,nc,nc), -p_data, p_singlet\n",
        "\n",
        "  return torch.logsumexp(p_data, dim=1).sum()\n",
        "\n",
        "## for VI version\n",
        "def compute_joint_probs(Theta, Y, S):\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "  \n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta)\n",
        "  p_s_given_z = compute_p_s_given_z(S, Theta)\n",
        "\n",
        "  log_rzd0 = p_s_given_z + p_y_given_z + log_pi + log_delta[0]\n",
        "\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta)\n",
        "  p_s_given_gamma = compute_p_s_given_gamma(S, Theta)\n",
        "\n",
        "  log_vgd1 = p_y_given_gamma + p_s_given_gamma + log_tau + log_delta[1]\n",
        "\n",
        "  #remove_indices = np.tril_indices(nc, -1) ## remove indices\n",
        "  #log_rd1g[:, remove_indices[0], remove_indices[1]] = float(\"NaN\")\n",
        "\n",
        "  #q1 = r.exp() * log_rd0z #; q1[torch.isnan(q1)] = 0.0\n",
        "  #q2 = v.exp() * log_rd1g #; q2[torch.isnan(q2)] = 0.0\n",
        "\n",
        "  return log_rzd0, log_vgd1.reshape(Y.shape[0], NC*NC)\n",
        "\n",
        "class BasicForwardNet(nn.Module):\n",
        "  \"\"\"Encoder for when data is input without any encoding\"\"\"\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim = 20, hidden_layer = 10):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input = nn.Linear(input_dim, hidden_dim)\n",
        "    #self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        \n",
        "    self.linear1 = nn.ModuleList(\n",
        "        [nn.Linear(hidden_dim, hidden_dim) for i in range(hidden_layer)]\n",
        "    )\n",
        "\n",
        "    self.output = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "    #out = F.relu(self.input(x))\n",
        "    out = F.leaky_relu(self.input(x))\n",
        "\n",
        "    #out = F.relu(self.linear1(out))\n",
        "\n",
        "    for net in self.linear1:\n",
        "      out = F.relu(net(out))\n",
        "    \n",
        "    out = self.output(out)\n",
        "        \n",
        "    return F.softmax(out, dim=1), F.log_softmax(out, dim=1) ## r/v/d log_r/log_v/log_d\n",
        "\n",
        "\n",
        "## train function for toy data\n",
        "def torch_em(Y, S, Theta):\n",
        "  \n",
        "  #wandb.init(project='emr_{}'.format(PROJECT_NAME))\n",
        "  #config = wandb.config\n",
        "  #config.data_type = 'sub_real'\n",
        "  \n",
        "  opt = optim.Adam(Theta.values())\n",
        "         \n",
        "  ls = []\n",
        "  for i in range(N_ITER * N_ITER_OPT):\n",
        "    #print(i)\n",
        "    \n",
        "    # E Step:\n",
        "    with torch.no_grad():\n",
        "      r, v, L, p_singlet = compute_r_v_2(Y, S, Theta)\n",
        "            \n",
        "    # M step (i.e. maximizing Q):\n",
        "    #for j in range(N_ITER_OPT):\n",
        "    opt.zero_grad()\n",
        "    q = -Q(Theta, Y, S, r, v, None)\n",
        "    q.backward()\n",
        "    opt.step()\n",
        "      \n",
        "    #if i % (10 - 1) == 0:\n",
        "    #  print(\"L: {}; {}\".format(L, Theta['log_psi'].exp()))\n",
        "      \n",
        "    # Check for convergence\n",
        "    if i > 0 and abs(ls[-1] - L) < TOL:\n",
        "      print(L)\n",
        "      print(F.log_softmax(Theta['is_delta'], 0).exp())\n",
        "      print(Theta['log_psi'].exp())\n",
        "      break\n",
        "    \n",
        "    ls.append(L)\n",
        "    #wandb.log({'ll': L, 'Q': -q})\n",
        "  \n",
        "  return ls\n",
        "  #columns = [\"before\", \"after\"]\n",
        "  #xs = [i for i in range(NC)]\n",
        "  #ys = [psi_init, Theta['log_psi'].exp().detach().numpy()]\n",
        "  #wandb.log({\"em_b_a\" : wandb.plot.line_series(\n",
        "  #  xs=xs,ys=ys,keys=columns, title = \"cell size\", xname=\"cluster #\")})\n",
        "\n",
        "## train function for toy data\n",
        "def torch_mle(Y, S, Theta):\n",
        "  \n",
        "  opt = optim.Adam(Theta.values())\n",
        "  loss = []\n",
        "  for epoch in range(N_ITER * N_ITER_OPT):\n",
        "    opt.zero_grad()\n",
        "    nll = -ll(Y, S, Theta) #nll\n",
        "    nll.backward()\n",
        "    opt.step()\n",
        "\n",
        "    #if epoch % (100 - 1) == 0:\n",
        "      #print(\"L: {}; {}; {}\".format(nlls, F.log_softmax(Theta['is_delta'], 0).exp(), F.log_softmax(Theta['is_pi'], 0).exp()))\n",
        "      \n",
        "      #print(\"nll: {}; {}\".format(nll, Theta['log_psi'].exp()))\n",
        "    \n",
        "    if epoch > 0 and abs(loss[-1] - nll) < TOL:\n",
        "      print(nll)\n",
        "      print(F.log_softmax(Theta['is_delta'], 0).exp())\n",
        "      #print(Theta['log_psi'].exp())\n",
        "      break\n",
        "  \n",
        "    loss.append(nll)\n",
        "    #wandb.log({'nll': nll})\n",
        "  \n",
        "  #columns = [\"before\", \"after\"]\n",
        "  #xs = [i for i in range(NC)]\n",
        "  #ys = [psi_init, Theta['log_psi'].exp().detach().numpy()]\n",
        "  #wandb.log({\"mle_b_a\" : wandb.plot.line_series(\n",
        "  #  xs=xs,ys=ys,keys=columns, title = \"cell size\", xname=\"cluster #\")})\n",
        "  return loss\n",
        "  \n",
        "## train function for toy data\n",
        "def torch_vi(Y, S, Theta):\n",
        "    \n",
        "    r_net = BasicForwardNet(P, NC)\n",
        "    v_net = BasicForwardNet(P, NC ** 2)\n",
        "    d_net = BasicForwardNet(P, 2)\n",
        "        \n",
        "    params = list(Theta.values()) + list(r_net.parameters()) + list(v_net.parameters()) + list(d_net.parameters())\n",
        "    opt = optim.AdamW(params)\n",
        "\n",
        "    YS = torch.hstack((Y,S.reshape(-1,1))).float()\n",
        "    YS1 = (YS - YS.mean(0)) / YS.std(0)\n",
        "\n",
        "    loss = []\n",
        "    for epoch in range(N_ITER * N_ITER_OPT):\n",
        "    \n",
        "        opt.zero_grad()\n",
        "        r, log_r = r_net(YS1)\n",
        "        v, log_v = v_net(YS1)\n",
        "        d, log_d = d_net(YS1)\n",
        "  \n",
        "        ## row sums to 1 (from neural net)\n",
        "        log_q0 = log_d[:,0].reshape(-1,1) + log_r ## like r in em version\n",
        "        log_q1 = log_d[:,1].reshape(-1,1) + log_v ## like v in em version\n",
        "    \n",
        "        log_rzd0, log_vgd1 = compute_joint_probs(Theta, Y, S)\n",
        "\n",
        "        entro = (d * log_d).sum() + (r * log_r).sum() + (v * log_v).sum()\n",
        "        recon = (log_q0.exp() * log_rzd0).sum() + (log_q1.exp() * log_vgd1).sum()\n",
        "        nelbo = (entro - recon).sum()\n",
        "        nelbo.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        #if epoch % (100 - 1) == 0:\n",
        "        #    print(\"nelbo: {}; {}; {}\".format(nelbo, F.log_softmax(Theta['is_delta'], 0).exp(), F.log_softmax(Theta['is_pi'], 0).exp()))\n",
        "  \n",
        "        if epoch > 0 and abs(loss[-1] - nelbo) < TOL:\n",
        "            print(nelbo)\n",
        "            print(F.log_softmax(Theta['is_delta'], 0).exp())\n",
        "            break\n",
        "           \n",
        "        loss.append(nelbo)\n",
        "    \n",
        "    return nelbo"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER2gMlaZPFo1"
      },
      "source": [
        "def make_plot(init_mu, init_psi, Theta, model_type, run):\n",
        "  \n",
        "  for j in range(init_mu.shape[0]):\n",
        "    plt.plot(init_mu[j], label=\"before\")\n",
        "    plt.plot(Theta['log_mu'][j].exp().detach().numpy(), label=\"after\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"proteins\")\n",
        "    plt.title(\"{} cluster id {}\".format(model_type, j))\n",
        "    #plt.savefig(\"/Users/jettlee/Desktop/res/{}1_r{}_c{}.png\".format(model_type, run, j))\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(init_psi, label=\"before\")\n",
        "    plt.plot(Theta['log_psi'].exp().detach().numpy(), label=\"after\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"cluster id\")\n",
        "    plt.title(\"{} cellsizes\".format(model_type))\n",
        "    #plt.savefig(\"/Users/jettlee/Desktop/res/{}2_r{}.png\".format(model_type, run))\n",
        "    plt.show()\n",
        "        \n",
        "    mu_psi = torch.hstack((Theta['log_mu'], Theta['log_psi'].reshape(-1,1))).detach().numpy()\n",
        "    df = pd.DataFrame((mu_psi - mu_psi.mean(0)) / mu_psi.std(0), columns = np.hstack((adata.var_names, 'size')))    \n",
        "    #df = pd.DataFrame(mu_psi, columns = np.hstack((adata.var_names, 'size')))    \n",
        "    #sns_plot = sns.heatmap(df, xticklabels=True)    \n",
        "    ax = sns.heatmap(df, xticklabels=True)\n",
        "    ax.figure.tight_layout()\n",
        "    plt.xlabel(\"proteins\")\n",
        "    plt.ylabel(\"cluster ID\")\n",
        "    plt.title(\"{} run {}\".format(model_type, run))\n",
        "    #plt.savefig(\"/Users/jettlee/Desktop/res/{}3_r{}.png\".format(model_type, run))\n",
        "    plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvnddn2zO0l2",
        "outputId": "b0243122-8ef3-476a-d9f6-79d8f6b628e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/')\n",
        "\n",
        "PATH = '/Users/jettlee/Desktop/DAMM/'\n",
        "#PATH = '/home/campbell/yulee/DAMM/'\n",
        "\n",
        "import scanpy as sc\n",
        "#adata = sc.read_h5ad(\"{}data/basel_zuri_subsample.h5ad\".format(PATH))\n",
        "adata = sc.read_h5ad(\"basel_zuri_subsample.h5ad\")\n",
        "\n",
        "adata = adata[:,['EGFR', 'ECadherin', 'ER', 'GATA3','Histone_H3_1', \n",
        " 'Ki67', 'SMA', 'Vimentin', 'cleaved_Parp', 'Her2',\n",
        " 'p53', 'panCytokeratin', 'CD19', 'PR', 'Myc', 'Fibronectin', 'CK14',\n",
        " 'Slug', 'CD20', 'vWF', 'Histone_H3_2', 'CK5', 'CD44', 'CD45', 'CD68',\n",
        " 'CD3', 'CAIX', 'CK8/18', 'CK7', '80ArArArAr80Di', \n",
        " 'phospho Histone', 'phospho S6', 'phospho mTOR']]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNw2UlD8O6nG"
      },
      "source": [
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "NC = 8 # number of clusters?\n",
        "NF = adata.shape[1] #YY.shape[1]\n",
        "NO = adata.shape[0] #YY.shape[0]\n",
        "P = NF + 1 #NF + one more dim for cellsizes\n",
        "\n",
        "N_ITER = 1000\n",
        "N_ITER_OPT = 100\n",
        "TOL = 1e-2\n",
        "N_INIT = 10"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE65AhGZQYFm"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "for ii in range(5):\n",
        "    \n",
        "    print(\"run {}\".format(ii))\n",
        "    \n",
        "    cell_sel = np.random.choice(adata.shape[0], size=1000)\n",
        "    adata = adata[cell_sel,:]\n",
        "    \n",
        "    YY = adata.X\n",
        "    YY = np.array(np.arcsinh(YY / 5.))\n",
        "    for i in range(NF):\n",
        "        YY[:,i] = winsorize(YY[:,i], limits=[0, 0.01]).data #fixed this\n",
        "    SS = adata.obs.Area\n",
        "    SS = winsorize(SS, limits=[0, 0.01]).data\n",
        "\n",
        "    kms = KMeans(NC).fit(YY)\n",
        "    init_labels = kms.labels_\n",
        "    init_label_class = np.unique(init_labels)\n",
        "\n",
        "    mu_init = np.array([YY[init_labels == c,:].mean(0) for c in init_label_class])\n",
        "    sigma_init = np.array([YY[init_labels == c,:].std(0) for c in init_label_class])\n",
        "    psi_init = np.array([SS[init_labels == c].mean() for c in init_label_class])\n",
        "    omega_init = np.array([SS[init_labels == c].std() for c in init_label_class])\n",
        "    pi_init = np.array([np.mean(init_labels == c) for c in init_label_class])\n",
        "    \n",
        "    tau_init = np.ones((NC,NC))\n",
        "    tau_init = tau_init / tau_init.sum()\n",
        "    \n",
        "    mu_psi0 = np.hstack((mu_init, psi_init.reshape(-1,1)))\n",
        "\n",
        "    Theta = {\n",
        "    'log_mu': np.log(mu_init + 1e-6),\n",
        "    'log_sigma': np.log(sigma_init + 1e-6), #np.zeros_like(sigma_init),\n",
        "    'log_psi': np.log(psi_init + 1e-6),\n",
        "    'log_omega': np.log(omega_init + 1e-6),\n",
        "    'is_delta': np.log([0.9, 1-0.9]),\n",
        "    'is_pi': np.log(pi_init),\n",
        "    'is_tau': np.log(tau_init)\n",
        "    }\n",
        "    \n",
        "    Theta0 = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "    Theta1 = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "    Theta2 = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "    Theta3 = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "    Theta4 = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "    Theta5 = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "    #Theta0['log_psi'].requires_grad = False\n",
        "    #Theta1['log_psi'].requires_grad = False\n",
        "    Theta0['is_delta'].requires_grad = False\n",
        "    Theta2['is_delta'].requires_grad = False\n",
        "    Theta4['is_delta'].requires_grad = False\n",
        "\n",
        "    Y = torch.tensor(YY)\n",
        "    S = torch.tensor(SS)\n",
        "    \n",
        "    if ii == 0:\n",
        "        #em0 = torch_em(Y, S, Theta0)\n",
        "        em1 = torch_em(Y, S, Theta1)\n",
        "        make_plot(mu_init, psi_init, Theta1, \"em\", ii+1)\n",
        "\n",
        "        #mle0 = torch_mle(Y, S, Theta2)\n",
        "        mle1 = torch_mle(Y, S, Theta3)\n",
        "        make_plot(mu_init, psi_init, Theta3, \"mle\", ii+1)\n",
        "    \n",
        "    #vi0 = torch_vi(Y, S, Theta4)\n",
        "    vi1 = torch_vi(Y, S, Theta5)\n",
        "    make_plot(mu_init, psi_init, Theta5, \"vi\", ii+1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHNLaMzbhBD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}