{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "non_function_version_071421",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlV8Jf5qoMbUy57gXZgrGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camlab-bioml/2021_IMC_Jett/blob/main/non_function_version_071421.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le7qDLI9FM6G"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as datasets\n",
        "\n",
        "def _ics(logL, n_obs, n_features, n_clusters): #, n, p, c\n",
        "  params = ( (((n_features * n_features) - n_features)/2 + 2 * n_features + 3) * (((n_clusters * n_clusters) - n_clusters)/2 + 2 * n_clusters) ) - 1\n",
        "  return 2 * (params - logL), -2 * logL + params * np.log(n_obs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SahaX89nFlkZ"
      },
      "source": [
        "## create data\n",
        "\n",
        "n_obs = 10000\n",
        "n_clusters = 3\n",
        "n_features = 2\n",
        "\n",
        "## ground true expressions ##\n",
        "'''\n",
        "true_expression_means = torch.tensor([\n",
        "    [1, 2],\n",
        "    [4, 3],\n",
        "    [7, 9]\n",
        "])\n",
        "'''\n",
        "\n",
        "true_expression_means = torch.randint(1, 11, (n_clusters, n_features))\n",
        "print(true_expression_means)\n",
        "\n",
        "true_expression_covs = torch.tensor([\n",
        "    [[.01, 0], [0, .01]],\n",
        "    [[.01, 0], [0, .01]],\n",
        "    [[.01, 0], [0, .01]]\n",
        "])\n",
        "\n",
        "#true_size_means = torch.tensor([.4, .5, .6])\n",
        "true_size_means = torch.rand(3)\n",
        "print(true_size_means)\n",
        "\n",
        "true_size_stds = torch.tensor([.05, .05, .05])\n",
        "true_size_stds\n",
        "\n",
        "## other ground true for generating data ##\n",
        "d_ws = torch.tensor([.95, .05])\n",
        "z_ws = torch.tensor([1 / 4, 1 / 2, 1 / 4])\n",
        "g_ws = torch.tensor([0.0667, 0.1333, 0.2000, 0, 0.1000, 0.2667, 0, 0 0.2333])\n",
        "\n",
        "gs = np.sum(np.random.choice(len(d_ws), size = n_obs, p = d_ws))\n",
        "zs = n_obs - gs\n",
        "\n",
        "## simulate data\n",
        "x = np.zeros((zs, n_features+4))\n",
        "for i in range(zs):\n",
        "  z = np.random.choice(n_clusters, size = 1, p = z_ws)[0]\n",
        "  x[i] = np.append(np.random.multivariate_normal(true_expression_means[z], true_expression_covs[z]), [np.random.normal(true_size_means[z], true_size_stds[z]), 0, z, z+6])\n",
        "  \n",
        "xxx = np.zeros((gs, n_features+4))\n",
        "for i in range(gs):\n",
        "\n",
        "  g = np.random.choice(6, size = 1, p = g_ws)[0]\n",
        "    \n",
        "  if g == 0:\n",
        "    idx = [0,0]\n",
        "  elif g == 1:\n",
        "    idx = [0,1]\n",
        "  elif g == 2:\n",
        "    idx = [0,2]\n",
        "  elif g == 3:\n",
        "    idx = [1,1]\n",
        "  elif g == 4:\n",
        "    idx = [1,2]\n",
        "  else:\n",
        "    idx = [2,2]\n",
        "  \n",
        "  xxx[i] = np.append(np.random.multivariate_normal( (true_expression_means[idx[0]] + true_expression_means[idx[1]]), (true_expression_covs[idx[0]] + true_expression_covs[idx[1]]) ),\n",
        "                     [np.random.normal( (true_size_means[idx[0]] + true_size_means[idx[1]]), (true_size_stds[idx[0]] + true_size_stds[idx[1]]) ), 1, g, g])\n",
        "  \n",
        "xx = np.append(x, xxx).reshape(n_obs,6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgXMyC6UFqiB"
      },
      "source": [
        "## initialization\n",
        "\n",
        "n_obs = N\n",
        "n_features = 2\n",
        "n_clusters = 3\n",
        "\n",
        "#torch.manual_seed(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "emu_c = torch.tensor([\n",
        "    [1, 2], # 1 2\n",
        "    [4, 3], # 4 3\n",
        "    [7, 9]\n",
        "]) #, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "smu_c = torch.tensor([.4, .5, .6]) #, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "eco_c = 0.05 * torch.eye(n_features).tile(n_clusters, 1, 1)\n",
        "sco_c = 0.05 * torch.ones(n_clusters, dtype=torch.float)\n",
        "\n",
        "smu_cc = torch.zeros(n_clusters, n_clusters)\n",
        "sco_cc = torch.zeros(n_clusters, n_clusters)\n",
        "\n",
        "emu_cc = torch.zeros(n_clusters, n_clusters, n_features)\n",
        "eco_cc = torch.zeros(n_clusters, n_clusters, n_features, n_features)\n",
        "\n",
        "for j in range(n_clusters):\n",
        "  for k in range(n_clusters):\n",
        "    smu_cc[j,k] = smu_c[j] + smu_c[k]\n",
        "    sco_cc[j,k] = sco_c[j] + sco_c[k]\n",
        "\n",
        "    emu_cc[j,k] = (emu_c[j] + emu_c[k])\n",
        "    eco_cc[j,k] = (eco_c[j] + eco_c[k])\n",
        "\n",
        "#smu_c = torch.tensor(smu_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "#sco_c = torch.tensor(sco_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "#emu_c = torch.tensor(emu_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "#eco_c = torch.tensor(eco_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "#smu_cc = torch.tensor(smu_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "#sco_cc = torch.tensor(sco_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "#emu_cc = torch.tensor(emu_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "#eco_cc = torch.tensor(eco_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "####\n",
        "\n",
        "pi_d0 = torch.tensor(0.9)\n",
        "\n",
        "pi_c = torch.empty(n_clusters).fill_(1. / (n_clusters))\n",
        "\n",
        "pi_cc = torch.triu(torch.ones(n_clusters, n_clusters))\n",
        "pi_cc = pi_cc / torch.sum(pi_cc)\n",
        "pi_cc[pi_cc == 0] = float('NaN')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eAowDrqCJqP"
      },
      "source": [
        "## helper function\n",
        "def _estimate_mean_cov_t1v1(X, S, r_ij, reg=1e-6):\n",
        "\n",
        "  #n, p = X.shape\n",
        "  #n, c = r_ij.shape\n",
        "\n",
        "  smut = torch.zeros(n_clusters)\n",
        "  scot = torch.zeros(n_clusters)\n",
        "\n",
        "  emut = torch.zeros(n_clusters, n_features)\n",
        "  ecot = torch.zeros(n_clusters, n_features, n_features)\n",
        "\n",
        "  n_c = torch.sum(r_ij, dim=0) #+ reg # (c)\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "    e_n = torch.round(n_c[j])\n",
        "    idx = r_ij[:,j].argsort()[-e_n.int():]\n",
        "    smut[j] = torch.mean(S[idx], 0)\n",
        "    emut[j] = torch.mean(X[idx], 0)\n",
        "    if e_n > 1:\n",
        "      ecot[j] = torch.tensor(np.cov(X[idx].T, ddof=0)) + reg * torch.eye(n_features)\n",
        "      scot[j] = torch.std(S[idx]) + reg\n",
        "    else:\n",
        "      ecot[j] = reg * torch.eye(n_features)\n",
        "      scot[j] = reg\n",
        "  return n_c, smut, scot, emut, ecot\n",
        "\n",
        "\n",
        "def _estimate_mean_cov_t2v1(X, S, r_ijk, reg=1e-6):\n",
        "\n",
        "  #n, p = X.shape\n",
        "  #n, c, c = r_ijk.shape\n",
        "\n",
        "  smut = torch.zeros(n_clusters, n_clusters)\n",
        "  scot = torch.zeros(n_clusters, n_clusters)\n",
        "\n",
        "  emut = torch.zeros(n_clusters, n_clusters, n_features)\n",
        "  ecot = torch.zeros(n_clusters, n_clusters, n_features, n_features)\n",
        "\n",
        "  n_cc = torch.sum(r_ijk, dim=0) #+ reg # (cxc)\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "    for k in range(n_clusters):\n",
        "      if not torch.isnan(n_cc[j,k]):\n",
        "        e_n = torch.round(n_cc[j,k])\n",
        "        idx = r_ijk[:,j,k].argsort()[-e_n.int():]\n",
        "        smut[j,k] = torch.mean(S[idx], 0)\n",
        "        emut[j,k] = torch.mean(X[idx], 0)\n",
        "        if e_n > 1:\n",
        "          scot[j,k] = torch.std(S[idx]) + reg\n",
        "          ecot[j,k] = torch.tensor(np.cov(X[idx].T, ddof=0)) + reg * torch.eye(n_features)\n",
        "        else:\n",
        "          scot[j,k] = reg\n",
        "          ecot[j,k] = reg * torch.eye(n_features)\n",
        "  return n_cc, smut, scot, emut, ecot"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB0qMJJENWYa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DnwHRXMB1Zk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_YSCr75B1jA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzCGwSpxSpMj",
        "outputId": "407d7432-caa9-4825-fe91-afdb502a047a"
      },
      "source": [
        "X = torch.tensor(xx[:,:2])\n",
        "S = torch.tensor(xx[:,2])\n",
        "\n",
        "## analytical version 1\n",
        "n_epochs = 1000\n",
        "tot = 1e-4\n",
        "iter = 0\n",
        "llv = [0.0]\n",
        "\n",
        "while iter < n_epochs:\n",
        "\n",
        "  log_pi_d0 = torch.log(pi_d0)\n",
        "  log_pi_d1 = torch.log(1 - pi_d0)\n",
        "\n",
        "  log_pi_c = torch.log(pi_c)\n",
        "  log_pi_cc = torch.log(pi_cc)\n",
        "\n",
        "  ### E-step:\n",
        "  log_post_top0 = torch.zeros(n_clusters, n_obs)\n",
        "  log_post_top1 = torch.zeros(n_clusters, n_clusters, n_obs)\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "  \n",
        "    el0 = D.MultivariateNormal(emu_c[j], eco_c[j]).log_prob(X.float())\n",
        "    sl0 = D.Normal(smu_c[j], sco_c[j]).log_prob(S.float())\n",
        "    log_post_top0[j] = log_pi_d0 + log_pi_c[j] + el0 + sl0\n",
        "    \n",
        "    for k in range(n_clusters):\n",
        "      el1 = D.MultivariateNormal((emu_c[j] + emu_c[k])/2, (eco_c[j] + eco_c[k])/2).log_prob(X.float())\n",
        "      sl1 = D.Normal(smu_c[j] + smu_c[k], sco_c[j] + sco_c[k]).log_prob(S.float())\n",
        "      #https://stats.stackexchange.com/questions/99363/mean-of-covariance-matrices\n",
        "      #https://stats.stackexchange.com/questions/214174/calculating-the-covariance-matrix-for-the-mean-of-variables\n",
        "\n",
        "      if torch.isnan(pi_cc[j,k]): #lower triangular nan\n",
        "        #log_post_top1[j,k] = float(\"-Inf\")\n",
        "        log_post_top1[j,k] = float(\"NaN\")\n",
        "      else:\n",
        "        log_post_top1[j,k] = log_pi_d1 + log_pi_cc[j,k] + el1 + sl1\n",
        "\n",
        "  log_post_top1 = log_post_top1.reshape(n_clusters * n_clusters, n_obs) #reshape\n",
        "  \n",
        "  ignored_indices = torch.isnan(torch.logsumexp(log_post_top1, 1))\n",
        "  assert(ignored_indices.sum() == (n_clusters * n_clusters - n_clusters) / 2)\n",
        "\n",
        "  log_post_tot = torch.logsumexp(torch.vstack((log_post_top0, log_post_top1[~ignored_indices])),0)\n",
        "  loss = -torch.mean(log_post_tot)\n",
        "\n",
        "  log_post_bot0 = torch.logsumexp(log_post_top0, 0) #n\n",
        "  log_post_z = (log_post_top0 - log_post_bot0).T #nxc (rjk)\n",
        "\n",
        "  log_post_bot1 = torch.logsumexp(log_post_top1[~ignored_indices], 0) #n\n",
        "  log_post_g = (log_post_top1 - log_post_bot1).T #nx(cxc) (rijk)\n",
        "\n",
        "  log_post_d0 = log_post_bot0 - log_post_tot\n",
        "  log_post_d1 = log_post_bot1 - log_post_tot\n",
        "\n",
        "  log_post_dz = log_post_d0[:,None] + log_post_z\n",
        "  log_post_dg = log_post_d1[:,None] + log_post_g\n",
        "\n",
        "  ### M-step:\n",
        "  pi_d0 = torch.exp(torch.logsumexp(log_post_d0, 0)) / n_obs\n",
        "\n",
        "  r_ij = torch.exp(log_post_dz)\n",
        "  n_c, smu_c, sco_c, emu_c, eco_c = _estimate_mean_cov_t1v1(X, S, r_ij)\n",
        "  pi_c = n_c / n_obs\n",
        "\n",
        "  r_ijk = torch.exp(log_post_dg).reshape(n_obs, n_clusters, n_clusters)\n",
        "  n_cc, smu_cc, sco_cc, emu_cc, eco_cc = _estimate_mean_cov_t2v1(X, S, r_ijk)\n",
        "  pi_cc = n_cc / n_obs\n",
        "      \n",
        "  print('Iteration', iter + 1, 'Likelihood: ', -loss, pi_d0)\n",
        "\n",
        "  if abs(llv[-1] + loss) < tot:\n",
        "    break\n",
        "      \n",
        "  llv.append(-loss)\n",
        "  iter += 1\n",
        "  \n",
        "aic, bic = _ics(-loss, n_obs, n_features, n_clusters)\n",
        "#return llv[1:], aic, bic"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 Likelihood:  tensor(-19.5002) tensor(0.9472)\n",
            "Iteration 2 Likelihood:  tensor(-39.4023) tensor(0.9478)\n",
            "Iteration 3 Likelihood:  tensor(-28.0840) tensor(0.9478)\n",
            "Iteration 4 Likelihood:  tensor(-28.0838) tensor(0.9478)\n",
            "Iteration 5 Likelihood:  tensor(-28.0838) tensor(0.9478)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpHIxiAHTzT2",
        "outputId": "2da845f8-0669-4a71-a452-18c7ec9a5e17"
      },
      "source": [
        "pi_d0"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9502)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8twQOiTT2bl",
        "outputId": "e2036d42-22e0-42a0-ed7f-725c9c256790"
      },
      "source": [
        "pi_c"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2389, 0.4748, 0.2365])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJLiYtI9T6UV",
        "outputId": "f1df5859-f336-45a5-f767-e310233062f6"
      },
      "source": [
        "pi_cc"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.2797e-14, 2.2858e-03, 5.2973e-04],\n",
              "        [       nan, 4.5988e-16, 1.3684e-02],\n",
              "        [       nan,        nan, 3.3310e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY16fDR4T_DD",
        "outputId": "3ebcb62b-1818-42f4-a112-dc23ee8da5a5"
      },
      "source": [
        "## check number of points in each cluster\n",
        "for i in range(9):\n",
        "  print(sum(xx[:,5] == i)/N)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0028\n",
            "0.0077\n",
            "0.0112\n",
            "0.006\n",
            "0.0115\n",
            "0.0116\n",
            "0.2389\n",
            "0.4748\n",
            "0.2355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D74f3oiZUIvk",
        "outputId": "abe68409-a969-47b1-aff0-bf7841be96f3"
      },
      "source": [
        "bic"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(702.3376)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU-R9NTOcWJZ",
        "outputId": "4b36bf30-266c-4184-ea0d-54d38b0d2b45"
      },
      "source": [
        "emu_c"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9993, 1.9981],\n",
              "        [4.0007, 3.0024],\n",
              "        [7.0055, 9.0083]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFfduRLlZI_e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8dxEg67B3JH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCqKYrcYB4Ct"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMu6PSbpQNbv"
      },
      "source": [
        "def _estimate_mean_cov_t1v2(X, S, log_rij, reg=1e-6):\n",
        "\n",
        "  n_obs, n_features = X.shape\n",
        "  n_obs, n_clusters = log_rij.shape\n",
        "\n",
        "  smut = torch.zeros(n_clusters)\n",
        "  scot = torch.zeros(n_clusters)\n",
        "\n",
        "  emut = torch.zeros(n_clusters, n_features)\n",
        "  ecot = torch.zeros(n_clusters, n_features, n_features)\n",
        "\n",
        "  #n_c = torch.sum(r_ij, dim=0) + reg # (c)\n",
        "\n",
        "  log_n_c = torch.logsumexp(log_rij, dim=0)\n",
        "  #n_c = torch.exp(log_n_c) + reg\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "    e_n = torch.round(torch.exp(log_n_c[j]))\n",
        "    idx = torch.exp(log_rij[:,j]).argsort()[-e_n.int():]\n",
        "    smut[j] = torch.mean(S[idx], 0)\n",
        "    emut[j] = torch.mean(X[idx], 0)\n",
        "    if e_n > 1:\n",
        "      ecot[j] = torch.tensor(np.cov(X[idx].T, ddof=0)) + reg * torch.eye(n_features)\n",
        "      scot[j] = torch.std(S[idx]) + reg\n",
        "    else:\n",
        "      ecot[j] = reg * torch.eye(n_features)\n",
        "      scot[j] = reg\n",
        "  return log_n_c, smut, scot, emut, ecot\n",
        "\n",
        "def _estimate_mean_cov_t2v2(X, S, log_rijk, reg=1e-6):\n",
        "\n",
        "  n_obs, n_features = X.shape\n",
        "  n_obs, n_clusters, n_clusters = log_rijk.shape\n",
        "\n",
        "  smut = torch.zeros(n_clusters, n_clusters)\n",
        "  scot = torch.zeros(n_clusters, n_clusters)\n",
        "\n",
        "  emut = torch.zeros(n_clusters, n_clusters, n_features)\n",
        "  ecot = torch.zeros(n_clusters, n_clusters, n_features, n_features)\n",
        "\n",
        "  #n_cc = torch.sum(r_ijk, dim=0) + reg # (cxc)\n",
        "  log_n_cc = torch.logsumexp(log_rijk, dim=0) # (cxc)\n",
        "  #n_cc = torch.exp(log_n_cc) + reg\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "    for k in range(n_clusters):\n",
        "      if not torch.isnan(torch.exp(log_n_cc[j,k])):\n",
        "        e_n = torch.round(torch.exp(log_n_cc[j,k]))\n",
        "        idx = torch.exp(log_rijk[:,j,k]).argsort()[-e_n.int():]\n",
        "        smut[j,k] = torch.mean(S[idx], 0)\n",
        "        emut[j,k] = torch.mean(X[idx], 0)\n",
        "        if e_n > 1:\n",
        "          scot[j,k] = torch.std(S[idx]) + reg\n",
        "          ecot[j,k] = torch.tensor(np.cov(X[idx].T, ddof=0)) + reg * torch.eye(n_features)\n",
        "        else:\n",
        "          scot[j,k] = reg\n",
        "          ecot[j,k] = reg * torch.eye(n_features)\n",
        "  return log_n_cc, smut, scot, emut, ecot"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e15VYzbaEsll",
        "outputId": "77f20f0a-3f0b-41e1-d859-56b7bec7d64b"
      },
      "source": [
        "X = torch.tensor(xx[:,:2])\n",
        "S = torch.tensor(xx[:,2])\n",
        "\n",
        "## analytical version 2\n",
        "n_epochs = 1000\n",
        "tot = 1e-4\n",
        "iter = 0\n",
        "llv = [0.0]\n",
        "\n",
        "log_pi_d0 = torch.log(pi_d0)\n",
        "log_pi_d1 = torch.log(1 - pi_d0)\n",
        "\n",
        "log_pi_c = torch.log(pi_c)\n",
        "log_pi_cc = torch.log(pi_cc)\n",
        "\n",
        "while iter < n_epochs:\n",
        "\n",
        "  ### E-step:\n",
        "  log_post_top0 = torch.zeros(n_clusters, n_obs)\n",
        "  log_post_top1 = torch.zeros(n_clusters, n_clusters, n_obs)\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "  \n",
        "    el0 = D.MultivariateNormal(emu_c[j], eco_c[j]).log_prob(X.float())\n",
        "    sl0 = D.Normal(smu_c[j], sco_c[j]).log_prob(S.float())\n",
        "    log_post_top0[j] = log_pi_d0 + log_pi_c[j] + el0 + sl0\n",
        "    \n",
        "    for k in range(n_clusters):\n",
        "      el1 = D.MultivariateNormal((emu_c[j] + emu_c[k])/2, (eco_c[j] + eco_c[k])/2).log_prob(X.float())\n",
        "      sl1 = D.Normal(smu_c[j] + smu_c[k], sco_c[j] + sco_c[k]).log_prob(S.float())\n",
        "      #https://stats.stackexchange.com/questions/99363/mean-of-covariance-matrices\n",
        "      #https://stats.stackexchange.com/questions/214174/calculating-the-covariance-matrix-for-the-mean-of-variables\n",
        "\n",
        "      if torch.isnan(pi_cc[j,k]): #lower triangular nan\n",
        "        log_post_top1[j,k] = float(\"NaN\")\n",
        "      else:\n",
        "        log_post_top1[j,k] = log_pi_d1 + log_pi_cc[j,k] + el1 + sl1\n",
        "\n",
        "  log_post_top1 = log_post_top1.reshape(n_clusters * n_clusters, n_obs) #reshape\n",
        "\n",
        "  ## make sure lower triangular matrix is NaN (a doublet can only be assigned to cluster 1 and 2 not 2 and 1, imposed ordering)\n",
        "  ignored_indices = torch.isnan(torch.logsumexp(log_post_top1, 1))\n",
        "  assert(ignored_indices.sum() == (n_clusters * n_clusters - n_clusters) / 2)\n",
        "\n",
        "  log_post_tot = torch.logsumexp(torch.vstack((log_post_top0, log_post_top1[~ignored_indices])),0)\n",
        "  loss = -torch.mean(log_post_tot)\n",
        "\n",
        "  log_post_bot0 = torch.logsumexp(log_post_top0, 0) #n\n",
        "  log_post_z = (log_post_top0 - log_post_bot0).T #nxc (rjk)\n",
        "\n",
        "  log_post_bot1 = torch.logsumexp(log_post_top1[~ignored_indices], 0) #n\n",
        "  log_post_g = (log_post_top1 - log_post_bot1).T #nx(cxc) (rijk)\n",
        "\n",
        "  log_post_d0 = log_post_bot0 - log_post_tot\n",
        "  log_post_d1 = log_post_bot1 - log_post_tot\n",
        "\n",
        "  log_post_dz = log_post_d0[:,None] + log_post_z\n",
        "  log_post_dg = log_post_d1[:,None] + log_post_g\n",
        "\n",
        "  ## M-step\n",
        "  log_n_c, smu_c, sco_c, emu_c, eco_c = _estimate_mean_cov_t1v2(X, S, log_post_dz) \n",
        "  log_pi_c = log_n_c - torch.log(torch.tensor(n_obs))\n",
        "  \n",
        "  log_n_cc, smu_cc, sco_cc, emu_cc, eco_cc = _estimate_mean_cov_t2v2(X, S, log_post_dg.reshape(n_obs, n_clusters, n_clusters))\n",
        "  log_pi_cc = log_n_cc - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "  log_pi_d0 = torch.logsumexp(log_post_d0, 0) - torch.log(torch.tensor(n_obs))\n",
        "  log_pi_d1 = torch.logsumexp(log_post_d1, 0) - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "  print('Iteration', iter + 1, 'Likelihood: ', -loss, torch.exp(log_pi_d0))\n",
        "\n",
        "  if abs(llv[-1] + loss) < tot:\n",
        "    break\n",
        "      \n",
        "  llv.append(-loss)\n",
        "  iter += 1\n",
        "  \n",
        "aic, bic = _ics(-loss, n_obs, n_features, n_clusters)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 Likelihood:  tensor(-19.5002) tensor(0.9472)\n",
            "Iteration 2 Likelihood:  tensor(-39.4023) tensor(0.9478)\n",
            "Iteration 3 Likelihood:  tensor(-28.0840) tensor(0.9478)\n",
            "Iteration 4 Likelihood:  tensor(-28.0838) tensor(0.9478)\n",
            "Iteration 5 Likelihood:  tensor(-28.0838) tensor(0.9478)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeUaa87994Qe",
        "outputId": "2d078117-d93a-42d2-b491-11eea5cb2a34"
      },
      "source": [
        "torch.exp(log_pi_d0)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9478)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnq7dmHa95z5",
        "outputId": "27b63e76-9697-405f-9688-2252cb3c7adf"
      },
      "source": [
        "torch.exp(log_pi_c)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2407, 0.4704, 0.2367])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQiHlomD966G",
        "outputId": "902458e0-0137-49c5-f483-049ed0f361ef"
      },
      "source": [
        "torch.exp(log_pi_cc)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7904e-13, 2.5565e-03, 1.1120e-03],\n",
              "        [       nan, 4.0974e-24, 1.3031e-02],\n",
              "        [       nan,        nan, 3.5516e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-jT7ZYfpZlz",
        "outputId": "d5ebbc81-54ca-4a79-bc28-57b37cbcdd99"
      },
      "source": [
        "## check number of points in each cluster\n",
        "for i in range(9):\n",
        "  print(sum(xx[:,5] == i)/N)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0036\n",
            "0.0075\n",
            "0.0095\n",
            "0.0056\n",
            "0.0142\n",
            "0.0124\n",
            "0.2407\n",
            "0.4704\n",
            "0.2361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSTJaXEpspFa",
        "outputId": "bcb3a4bc-66f6-4400-d1db-3a0ea0ef7d88"
      },
      "source": [
        "emu_c"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9993, 1.9981],\n",
              "        [4.0007, 3.0024],\n",
              "        [7.0050, 9.0075]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "553jOLO1-CbW",
        "outputId": "8e614f90-e34b-4fd0-97a8-46080a2291f0"
      },
      "source": [
        "smu_c"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4002, 0.5007, 0.6041])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45i0Dl8E-CmU",
        "outputId": "a39fa503-497d-443e-a0fa-b8804914e050"
      },
      "source": [
        "bic"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(704.6602)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBSotnKQTJlu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD4APkuUFWXp"
      },
      "source": [
        ""
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR8CQz6dYAFi"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V475ZRjpYCGg"
      },
      "source": [
        "X = torch.tensor(xx[:,:2])\n",
        "S = torch.tensor(xx[:,2])\n",
        "\n",
        "n_epochs = 1000\n",
        "tot = 1e-4\n",
        "iter = 0\n",
        "llv = [0.0]\n",
        "\n",
        "while iter < 1:\n",
        "\n",
        "  log_pi_d0 = torch.log(pi_d0)\n",
        "  log_pi_d1 = torch.log(1 - pi_d0)\n",
        "\n",
        "  log_pi_c = torch.log(pi_c)\n",
        "  log_pi_cc = torch.log(pi_cc)\n",
        "\n",
        "  ### E-step:\n",
        "  log_post_top0 = torch.zeros(n_clusters, n_obs)\n",
        "  log_post_top1 = torch.zeros(n_clusters, n_clusters, n_obs)\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "  \n",
        "    el0 = D.MultivariateNormal(emu_c[j], eco_c[j]).log_prob(X.float())\n",
        "    sl0 = D.Normal(smu_c[j], sco_c[j]).log_prob(S.float())\n",
        "    log_post_top0[j] = log_pi_d0 + log_pi_c[j] + el0 + sl0\n",
        "    \n",
        "    for k in range(n_clusters):\n",
        "      el1 = D.MultivariateNormal((emu_c[j] + emu_c[k])/2, (eco_c[j] + eco_c[k])/2).log_prob(X.float())\n",
        "      sl1 = D.Normal(smu_c[j] + smu_c[k], sco_c[j] + sco_c[k]).log_prob(S.float())\n",
        "      #https://stats.stackexchange.com/questions/99363/mean-of-covariance-matrices\n",
        "      #https://stats.stackexchange.com/questions/214174/calculating-the-covariance-matrix-for-the-mean-of-variables\n",
        "\n",
        "      if torch.isnan(pi_cc[j,k]): #lower triangular nan\n",
        "        #log_post_top1[j,k] = float(\"-Inf\")\n",
        "        log_post_top1[j,k] = float(\"NaN\")\n",
        "      else:\n",
        "        log_post_top1[j,k] = log_pi_d1 + log_pi_cc[j,k] + el1 + sl1\n",
        "\n",
        "  iter += 1\n",
        "\n",
        "log_post_top1 = log_post_top1.reshape(n_clusters * n_clusters, n_obs) #reshape\n",
        "  \n",
        "ignored_indices = torch.isnan(torch.logsumexp(log_post_top1, 1))\n",
        "assert(ignored_indices.sum() == (n_clusters * n_clusters - n_clusters) / 2)\n",
        "\n",
        "log_post_tot = torch.logsumexp(torch.vstack((log_post_top0, log_post_top1[~ignored_indices])),0)\n",
        "loss = -torch.mean(log_post_tot)\n",
        "\n",
        "log_post_bot0 = torch.logsumexp(log_post_top0, 0) #n\n",
        "log_post_z = (log_post_top0 - log_post_bot0).T #nxc (rjk)\n",
        "\n",
        "log_post_bot1 = torch.logsumexp(log_post_top1[~ignored_indices], 0) #n\n",
        "log_post_g = (log_post_top1 - log_post_bot1).T #nx(cxc) (rijk)\n",
        "\n",
        "log_post_d0 = log_post_bot0 - log_post_tot\n",
        "log_post_d1 = log_post_bot1 - log_post_tot\n",
        "\n",
        "log_post_dz = log_post_d0[:,None] + log_post_z\n",
        "log_post_dg = log_post_d1[:,None] + log_post_g\n",
        "\n",
        "### M-step:\n",
        "#log_pi_d0 = torch.logsumexp(log_post_d0, 0) - torch.log(torch.tensor(n_obs))\n",
        "pi_d0 = torch.exp(torch.logsumexp(log_post_d0, 0)) / n_obs\n",
        "\n",
        "r_ij = torch.exp(log_post_dz)\n",
        "n_c, smu_c, sco_c, emu_c, eco_c = _estimate_mean_cov_t1v1(X, S, r_ij)\n",
        "#log_pi_c = torch.log(n_c) - torch.log(torch.tensor(n_obs))\n",
        "pi_c = n_c / n_obs\n",
        "\n",
        "r_ijk = torch.exp(log_post_dg).reshape(n_obs, n_clusters, n_clusters)\n",
        "n_cc, smu_cc, sco_cc, emu_cc, eco_cc = _estimate_mean_cov_t2v1(X, S, r_ijk)\n",
        "#log_pi_cc = torch.log(n_cc) - torch.log(torch.tensor(n_obs))\n",
        "pi_cc = n_cc / n_obs"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DKnOGXTYUoH"
      },
      "source": [
        ""
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSkMWrg7SdPM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO_fckWqjAU1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTJ2xUj8Sdlm",
        "outputId": "90e0ce82-779f-4886-fa07-a4c5a7f74766"
      },
      "source": [
        "## initialization\n",
        "\n",
        "n_obs = N\n",
        "n_features = 2\n",
        "n_clusters = 3\n",
        "\n",
        "#torch.manual_seed(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "emu_c = torch.tensor([\n",
        "    [2, 4], # 1 2\n",
        "    [3, 5], # 4 3\n",
        "    [5, 7] #7 9\n",
        "], dtype=torch.float) #, requires_grad=True, , device=device)\n",
        "\n",
        "smu_c = torch.tensor([.3, .4, .5], dtype=torch.float) #, requires_grad=True,  device=device)\n",
        "\n",
        "eco_c = 0.05 * torch.eye(n_features).tile(n_clusters, 1, 1)\n",
        "sco_c = 0.05 * torch.ones(n_clusters, dtype=torch.float)\n",
        "\n",
        "smu_cc = torch.zeros(n_clusters, n_clusters, dtype=torch.float)\n",
        "sco_cc = torch.zeros(n_clusters, n_clusters, dtype=torch.float)\n",
        "\n",
        "emu_cc = torch.zeros(n_clusters, n_clusters, n_features, dtype=torch.float)\n",
        "eco_cc = torch.zeros(n_clusters, n_clusters, n_features, n_features, dtype=torch.float)\n",
        "\n",
        "for j in range(n_clusters):\n",
        "  for k in range(n_clusters):\n",
        "    if k >= j:\n",
        "      smu_cc[j,k] = smu_c[j] + smu_c[k]\n",
        "      sco_cc[j,k] = sco_c[j] + sco_c[k]\n",
        "\n",
        "      emu_cc[j,k] = (emu_c[j] + emu_c[k])\n",
        "      eco_cc[j,k] = (eco_c[j] + eco_c[k])\n",
        "\n",
        "smu_c = torch.tensor(smu_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "sco_c = torch.tensor(sco_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "      \n",
        "emu_c = torch.tensor(emu_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "eco_c = torch.tensor(eco_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "      \n",
        "smu_cc = torch.tensor(smu_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "sco_cc = torch.tensor(sco_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "      \n",
        "emu_cc = torch.tensor(emu_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "eco_cc = torch.tensor(eco_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "      \n",
        "####\n",
        "\n",
        "pi_d0 = torch.tensor(0.9)\n",
        "\n",
        "pi_c = torch.empty(n_clusters).fill_(1. / (n_clusters))\n",
        "\n",
        "pi_cc = torch.triu(torch.ones(n_clusters, n_clusters))\n",
        "pi_cc = pi_cc / torch.sum(pi_cc)\n",
        "pi_cc[pi_cc == 0] = float('NaN')\n",
        "\n",
        "def _ics(logL, n_obs, n_features, n_clusters): #, n, p, c\n",
        "  params = ( (((n_features * n_features) - n_features)/2 + 2 * n_features + 3) * (((n_clusters * n_clusters) - n_clusters)/2 + 2 * n_clusters) ) - 1\n",
        "  return 2 * (params - logL), -2 * logL + params * np.log(n_obs)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3GfEy3Klpol"
      },
      "source": [
        "## torch.optim version\n",
        "\n",
        "X = torch.tensor(xx[:,:2])\n",
        "S = torch.tensor(xx[:,2])\n",
        "\n",
        "#parameters = [emu_c, eco_c, smu_c, sco_c]\n",
        "#parameters = [emu_cc, eco_cc, smu_cc, sco_cc]\n",
        "parameters = [emu_c, eco_c, smu_c, sco_c, emu_cc, eco_cc, smu_cc, sco_cc]\n",
        "#opt = optim.SGD(parameters, lr=0.01)\n",
        "opt = optim.Adam(parameters)\n",
        "\n",
        "tot = 1e-3\n",
        "llv = [0.0]\n",
        "\n",
        "log_pi_d0 = torch.log(pi_d0)\n",
        "log_pi_d1 = torch.log(1-pi_d0)\n",
        "log_pi_c = torch.log(pi_c)\n",
        "log_pi_cc = torch.log(pi_cc)\n",
        "\n",
        "iter = 0\n",
        "n_epochs = 1000\n",
        "while iter < n_epochs:\n",
        "\n",
        "  log_post_top0 = torch.zeros(n_clusters, n_obs)\n",
        "  log_post_top1 = torch.zeros(n_clusters, n_clusters, n_obs)\n",
        "\n",
        "  for j in range(n_clusters):  \n",
        "    el0 = D.MultivariateNormal(emu_c[j], eco_c[j]).log_prob(X.float())\n",
        "    sl0 = D.Normal(smu_c[j], sco_c[j]).log_prob(S.float())\n",
        "    log_post_top0[j] = log_pi_d0 + log_pi_c[j] + el0 + sl0\n",
        "\n",
        "    for k in range(n_clusters):\n",
        "      el1 = D.MultivariateNormal(emu_cc[j,k]/2, eco_cc[j,k]/2).log_prob(X.float())\n",
        "      sl1 = D.Normal(smu_cc[j,k], sco_cc[j,k]).log_prob(S.float())\n",
        "            \n",
        "      if torch.isnan(pi_cc[j,k]): #lower triangular nan\n",
        "        log_post_top1[j,k] = float(\"NaN\")\n",
        "      else:\n",
        "        log_post_top1[j,k] = log_pi_d1 + log_pi_cc[j,k] + el1 + sl1\n",
        "\n",
        "  log_post_top1 = log_post_top1.reshape(n_clusters * n_clusters, n_obs) #reshape\n",
        "\n",
        "  ignored_indices = torch.isnan(torch.logsumexp(log_post_top1, 1))\n",
        "  assert(ignored_indices.sum() == (n_clusters * n_clusters - n_clusters) / 2)\n",
        "  \n",
        "  log_post_tot = torch.logsumexp(torch.vstack((log_post_top0, log_post_top1[~ignored_indices])),0)\n",
        "  loss = -torch.mean(log_post_tot)\n",
        "\n",
        "  opt.zero_grad()\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    parameters[0].clamp_(tot)\n",
        "    ##parameters[1].clamp_(tot)\n",
        "    parameters[2].clamp_(tot)\n",
        "    parameters[3].clamp_(tot)\n",
        "    parameters[4].clamp_(tot)\n",
        "    ##parameters[5].clamp_(tot)\n",
        "    parameters[6].clamp_(tot)\n",
        "    parameters[7].clamp_(tot)\n",
        "\n",
        "    eco_c += torch.eye(n_features) * tot\n",
        "    eco_cc += torch.eye(n_features) * tot\n",
        "\n",
        "    log_post_bot0 = torch.logsumexp(log_post_top0, 0) #n\n",
        "    log_post_bot1 = torch.logsumexp(log_post_top1[~ignored_indices], 0) #n\n",
        "    \n",
        "    log_post_d0 = log_post_bot0 - log_post_tot\n",
        "    log_post_d1 = log_post_bot1 - log_post_tot\n",
        "\n",
        "    log_post_z = (log_post_top0 - log_post_bot0).T #nxc (rjk)\n",
        "    log_post_g = (log_post_top1 - log_post_bot1).T #nxc (rijk)\n",
        "\n",
        "    log_post_dz = log_post_d0[:,None] + log_post_z\n",
        "    log_post_dg = log_post_d1[:,None] + log_post_g\n",
        "\n",
        "    log_n_c = torch.logsumexp(log_post_dz, 0)\n",
        "    log_pi_c = log_n_c - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "    log_n_cc = torch.logsumexp(log_post_dg, 0).reshape(n_clusters, n_clusters)\n",
        "    log_pi_cc = log_n_cc - torch.log(torch.tensor(n_obs))  # cxc matrix\n",
        "\n",
        "    log_pi_d0 = torch.logsumexp(log_post_d0, 0) - torch.log(torch.tensor(n_obs))\n",
        "    log_pi_d1 = torch.logsumexp(log_post_d1, 0) - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "  print('Iteration', iter + 1, 'Likelihood: ', -loss, torch.exp(log_pi_d0))\n",
        "\n",
        "  if abs(llv[-1] + loss) < tot:\n",
        "    break\n",
        "      \n",
        "  llv.append(-loss)\n",
        "  iter += 1\n",
        "\n",
        "aic, bic = _ics(-loss, n_obs, n_features, n_clusters)\n",
        "\n",
        "#print(epoch, pi_d0, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUmlAf07jqpe",
        "outputId": "8341f9cd-4718-4718-8e23-91d3bd6a75c5"
      },
      "source": [
        "torch.exp(log_pi_d0)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9976)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twaV0WiNkUKm",
        "outputId": "3fde86e3-90b3-4b1d-f9d4-8d124da61428"
      },
      "source": [
        "torch.exp(log_pi_c)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2782, 0.4357, 0.2837])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFpDgKm_kkZI",
        "outputId": "d6109a56-6e3b-4e78-9bd1-19a6201caf2d"
      },
      "source": [
        "torch.exp(log_pi_cc)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0024, 0.0000, 0.0000],\n",
              "        [   nan, 0.0000, 0.0000],\n",
              "        [   nan,    nan, 0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TxvebH8o5Dd",
        "outputId": "6756e66b-7b92-4390-9f1a-7f1673a96e35"
      },
      "source": [
        "## check number of points in each cluster\n",
        "for i in range(9):\n",
        "  print(sum(xx[:,5] == i)/N)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0028\n",
            "0.0077\n",
            "0.0112\n",
            "0.006\n",
            "0.0115\n",
            "0.0116\n",
            "0.2389\n",
            "0.4748\n",
            "0.2355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3jIdug8VcfW",
        "outputId": "c6a583c8-5f2d-436c-ecb7-b590cfef89cf"
      },
      "source": [
        "emu_c"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7086, 3.6069],\n",
              "        [3.3877, 4.6038],\n",
              "        [5.4044, 7.3912]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWmH3cD8kxHy",
        "outputId": "b9dab3fa-9bff-4501-b1c3-819e6836d0f0"
      },
      "source": [
        "smu_c"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4002, 0.5007, 0.6034], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GSMFDxJ6NRF",
        "outputId": "446efa58-d6ae-41b7-afdb-1955f343194a"
      },
      "source": [
        "bic"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(656.7866, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "iWLFYd0SLxAM",
        "outputId": "fe899d83-66de-4486-9128-264b6cc99299"
      },
      "source": [
        "plt.plot(llv[1:])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbb008f49d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WHhBOQ6k0kS"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRlksvkrDLWM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdBYfreSffR0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P8gYL3rkksR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Tq79AyACRn"
      },
      "source": [
        "## The Log-Likelihood:\n",
        "Typically, we find $\\theta$ which maximizes the log-likelihood $\\log[P(X|\\theta)]$. Assuming independence, it looks like the following:\n",
        "\\begin{align}\n",
        "\\ell(\\theta) = \\log P(X|\\theta)\n",
        "& = \\log P(x_1,...,x_n|\\theta) \\\\\n",
        "& = \\log \\prod^n_{i=1} P(x_i|\\theta) = \\sum^n_{i=1} \\log P(x_i|\\theta) \n",
        "\\end{align}\n",
        "\n",
        "However, we can only compute $P(X,Z|\\theta)$ due to the dependency on $Z$, we must marginalize out $Z$ to compute $P(X|\\theta)$, then we want to maximize the following log-likelihood:\n",
        "\\begin{align}\n",
        "\\sum^n_{i=1} \\log P(x_i|\\theta) = \\sum^n_{i=1} \\log P(x_i|\\theta) = \\sum^n_{i=1} \\log \\sum^c_{j=1} P(x_i, z_j|\\theta)\n",
        "\\end{align}\n",
        "\n",
        "This quantity is more difficult to maximize because we have to marginalize or sum over the latent variable $Z$ for all $n$ data points. Therefore, we need to compute a proxy function instead.\n",
        "\n",
        "## Complete Log-Likelihood:\n",
        "The complete data log-likelihood is maximized below which at first assumes that for each data point $x_i$ we have a known discrete latent assignment $z_i$.\n",
        "\n",
        "\\begin{align}\n",
        "\\sum^n_{i=1} \\log P(x_i, z_i|\\theta) = \\sum^n_{i=1} \\log [P(z_i|\\theta)P(x_i|z_i,\\theta)]\n",
        "\\end{align}\n",
        "\n",
        "We no longer have to sum across $Z$ but we don't know $z_i$. Therefore, we try to guess at $z_i$ by maximizing $Q(\\theta, \\theta^*)$ (i.e., the expectation of the complete log-likelihood with respect to $Z|X,\\theta^*$, which allows us to fill in the values of $z_i$\n",
        "\n",
        "## Computing $Q(\\theta, \\theta^*)$:\n",
        "\n",
        "\\begin{align}\n",
        "Q(\\theta, \\theta^*) \n",
        "& = E_{Z|X,\\theta^*} [\\sum^n_{i=1} \\log P(z_i|\\theta)P(x_i|z_i,\\theta)] \\\\\n",
        "& = \\sum^n_{i=1} E_{Z|X,\\theta^*} [[\\log \\sum^c_{j=1}[P(z_i=c|\\theta)P(x_i|z_i=c,\\theta)]^{I(z_i=c)}] \\\\\n",
        "& = \\sum^n_{i=1} \\sum^c_{j=1} P(z_i=c|X,\\theta^*)\\log[P(z_i=c|\\theta)P(x_i|z_i=c,\\theta)]\n",
        "\\end{align}\n",
        "\n",
        "where $I$ is the indicator function and can be used to evaluate the expecation because we assume that $z_i$ is discrete. \n",
        "\n",
        "In the equation above, the left-most term is the soft latent assignments and the right-most term is the log product of the prior of $Z$ and the conditional P.M.F. The right-most term can be separated into two terms allowing for the maximization of the mixture weights (prior of $Z$) and the distribution parameters of the P.M.F.\n",
        "\n",
        "## Expectation-Maximization (EM) Algorithm:\n",
        "The EM algorithm aims to maximze the expected complete data log-looklihood by interating the following two steps until convergence:\n",
        "1. Given the parameters $\\theta^*$ from the previous iteration, evaluate $Q$ function.\n",
        "2. Maximize this $Q$ function in terms of $\\theta$\n",
        "\n",
        "## Convergence Proof:\n",
        "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness\n",
        "\n",
        "https://see.stanford.edu/materials/aimlcs229/cs229-notes8.pdf\n",
        "\n",
        "## EM for GMM:\n",
        "\n",
        "\\begin{align}\n",
        "P(x_i|\\theta) & = \\sum^n_{j=1} P(z_i=c) P(x_i|z_i=c,\\theta), \\\\\n",
        "& x_i|z_i \\sim N(\\mu_c, \\Sigma_c), \\\\\n",
        "& z_i \\sim Categorical(\\pi).\n",
        "\\end{align}\n",
        "\n",
        "1. Compute $Q(\\theta, \\theta^*)$:\n",
        "\\begin{align}\n",
        "Q(\\theta, \\theta^*) \n",
        "& = \\sum^n_{i=1} \\sum^c_{j=1} P(z_i=c|x_i,\\theta^*)\\log[P(z_i=c|\\theta)P(x_i|z_i=c,\\theta)] \\\\\n",
        "& =  \\sum^n_{i=1} \\sum^c_{j=1} [r_{ij} \\log P(z_i=c|\\theta) + r_{ij} \\log P(x_i|z_i=c,\\theta)] \\\\\n",
        "& = \\sum^n_{i=1} \\sum^c_{j=1} [r_{ij} \\log \\pi_{j} + r_{ij} \\log N(x_i; \\mu_j, \\Sigma_j)]\n",
        "\\end{align}\n",
        "\n",
        "2. Update $\\theta^t = argmax (\\theta, \\theta^*) = (\\pi^t, \\mu^t, \\Sigma^t)$:\n",
        "\n",
        "The $Q$ function is like a weighted normal distribution MLE problem, we could maximize $\\theta$ by differentiating the formula above and provide closed-form formulas.\n",
        "\n",
        "\\begin{align}\n",
        "Q(\\theta, \\theta^*) \n",
        "& = \\sum^n_{i=1} \\sum^c_{j=1} r_{ij} [\\log \\pi_{j} + \\log N(x_i; \\mu_j, \\Sigma_j)] \\\\\n",
        "& \\implies r_{ij} [\\log \\pi_j + \\frac{1}{2}\\log(|\\Sigma_{j}|^{-1}) -\n",
        "    \\frac{1}{2}(y_i-\\mu_j)^T\\Sigma^{-1}_{j}(y_i-\\mu_j) + constant] \\\\\n",
        "\\end{align}\n",
        "\n",
        "then we differentiate the forumula for each cluster c,\n",
        "\n",
        "$\\pi^{t}_j = \\frac{\\sum r^{(t-1)}_{ij}}{n}$, which was applied lagrangian multiplier to solve $w_j$.\n",
        "\n",
        "$\\mu^{t}_j = \\frac{\\sum r^{(t-1)}_{ij} y_i} {\\sum r^{(t-1)}_{ij}}$, which was solved by taking MLE of $\\mu_j$.\n",
        "\n",
        "$\\Sigma^{t}_j = \\frac{\\sum r^{(t-1)}_{ij} (y_i - \\mu^{(t-1)}_j) (y_i - \\mu^{(t-1)}_j)^T} {\\sum z^{(t-1)}_{ij}}$, which was solved by MLE of $\\Sigma_j$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUypNrDe2qy-"
      },
      "source": [
        "## EM for Doublet Gaussian Mixutre Model (DGMM):\n",
        "\n",
        "\\begin{align}\n",
        "P(y_i, s_i|\\theta) \n",
        "= & \\sum^n_{i=1} [P(d_i=0) P(z_i=c) P(y_i|z_i=c,\\theta) P(s_i|z_i=c,\\theta) \\\\\n",
        "& + P(d_i=1) P(\\gamma_i=[c,c']) P(y_i|\\gamma_i=[c,c'],\\theta) P(s_i|\\gamma_i=[c,c'],\\theta)], \\\\\n",
        "& d_i \\sim Bernoulli(p), \\\\\n",
        "& z_i \\sim Categorical(\\pi_c), \\\\\n",
        "& \\gamma_i \\sim Categorical(\\pi_{cxc}), \\\\\n",
        "& y_i|z_i \\sim N(\\mu_c, \\Sigma_c), \\\\ \n",
        "& s_i|z_i \\sim N(\\psi_c, \\omega_c), \\\\\n",
        "& y_i|\\gamma_i=[c,c'] \\sim N((\\mu_{c_j} + \\mu_{c_k})/2, (\\Sigma_{c_j} + \\Sigma_{c_k})/2), \\\\\n",
        "& s_i|\\gamma_i=[c,c'] \\sim N(\\psi_{c_j} + \\psi_{c_k} , \\omega_{c_j} + \\omega_{c_k}).\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKHF9TTV2qA8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uKmeDqPDLgV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaDLKqqwDi_L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xwmfii6Djpk"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as datasets\n",
        "\n",
        "def _ics(logL, n_obs, n_features, n_clusters): #, n, p, c\n",
        "  params = ( (((n_features * n_features) - n_features)/2 + 2 * n_features + 3) * (((n_clusters * n_clusters) - n_clusters)/2 + 2 * n_clusters) ) - 1\n",
        "  return 2 * (params - logL), -2 * logL + params * np.log(n_obs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKyr6HCji8PV"
      },
      "source": [
        "def _estimate_mean_cov_t1v2(X, S, log_rij, reg=1e-6):\n",
        "\n",
        "  n_obs, n_features = X.shape\n",
        "  n_obs, n_clusters = log_rij.shape\n",
        "\n",
        "  smut = torch.zeros(n_clusters)\n",
        "  scot = torch.zeros(n_clusters)\n",
        "\n",
        "  emut = torch.zeros(n_clusters, n_features)\n",
        "  ecot = torch.zeros(n_clusters, n_features, n_features)\n",
        "\n",
        "  #n_c = torch.sum(r_ij, dim=0) + reg # (c)\n",
        "\n",
        "  log_n_c = torch.logsumexp(log_rij, dim=0)\n",
        "  #n_c = torch.exp(log_n_c) + reg\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "    e_n = torch.round(torch.exp(log_n_c[j]))\n",
        "    idx = torch.exp(log_rij[:,j]).argsort()[-e_n.int():]\n",
        "    smut[j] = torch.mean(S[idx], 0)\n",
        "    emut[j] = torch.mean(X[idx], 0)\n",
        "    if e_n > 1:\n",
        "      ecot[j] = torch.tensor(np.cov(X[idx].T, ddof=0)) + reg * torch.eye(n_features)\n",
        "      scot[j] = torch.std(S[idx]) + reg\n",
        "    else:\n",
        "      ecot[j] = reg * torch.eye(n_features)\n",
        "      scot[j] = reg\n",
        "  return log_n_c, smut, scot, emut, ecot\n",
        "\n",
        "def _estimate_mean_cov_t2v2(X, S, log_rijk, reg=1e-6):\n",
        "\n",
        "  n_obs, n_features = X.shape\n",
        "  n_obs, n_clusters, n_clusters = log_rijk.shape\n",
        "\n",
        "  smut = torch.zeros(n_clusters, n_clusters)\n",
        "  scot = torch.zeros(n_clusters, n_clusters)\n",
        "\n",
        "  emut = torch.zeros(n_clusters, n_clusters, n_features)\n",
        "  ecot = torch.zeros(n_clusters, n_clusters, n_features, n_features)\n",
        "\n",
        "  #n_cc = torch.sum(r_ijk, dim=0) + reg # (cxc)\n",
        "  log_n_cc = torch.logsumexp(log_rijk, dim=0) # (cxc)\n",
        "  #n_cc = torch.exp(log_n_cc) + reg\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "    for k in range(n_clusters):\n",
        "      if not torch.isnan(torch.exp(log_n_cc[j,k])):\n",
        "        e_n = torch.round(torch.exp(log_n_cc[j,k]))\n",
        "        idx = torch.exp(log_rijk[:,j,k]).argsort()[-e_n.int():]\n",
        "        smut[j,k] = torch.mean(S[idx], 0)\n",
        "        emut[j,k] = torch.mean(X[idx], 0)\n",
        "        if e_n > 1:\n",
        "          scot[j,k] = torch.std(S[idx]) + reg\n",
        "          ecot[j,k] = torch.tensor(np.cov(X[idx].T, ddof=0)) + reg * torch.eye(n_features)\n",
        "        else:\n",
        "          scot[j,k] = reg\n",
        "          ecot[j,k] = reg * torch.eye(n_features)\n",
        "  return log_n_cc, smut, scot, emut, ecot"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoHTgixlDLq1"
      },
      "source": [
        "## analytical version (function)\n",
        "def em_v1(X, S, hyperparameters, parameters, n_epochs = 1000, tot = 1e-6):\n",
        "  \n",
        "  n_obs, n_features = X.shape\n",
        "  n_clusters = len(hyperparameters[1])\n",
        "\n",
        "  log_pi_d0 = torch.log(hyperparameters[0])\n",
        "  log_pi_d1 = torch.log(1-hyperparameters[0])\n",
        "  log_pi_c = torch.log(hyperparameters[1])\n",
        "  log_pi_cc = torch.log(hyperparameters[2])\n",
        "\n",
        "  emu_c = parameters[0]\n",
        "  eco_c = parameters[1]\n",
        "  smu_c = parameters[2]\n",
        "  sco_c = parameters[3]\n",
        "\n",
        "  emu_cc = parameters[4]\n",
        "  eco_cc = parameters[5]\n",
        "  smu_cc = parameters[6]\n",
        "  sco_cc = parameters[7]\n",
        "\n",
        "  iter = 0\n",
        "  llv = [0.0]\n",
        "  while iter < n_epochs:\n",
        "\n",
        "    ### E-step:\n",
        "    log_post_top0 = torch.zeros(n_clusters, n_obs)\n",
        "    log_post_top1 = torch.zeros(n_clusters, n_clusters, n_obs)\n",
        "\n",
        "    for j in range(n_clusters):\n",
        "  \n",
        "      el0 = D.MultivariateNormal(emu_c[j], eco_c[j]).log_prob(X.float())\n",
        "      sl0 = D.Normal(smu_c[j], sco_c[j]).log_prob(S.float())\n",
        "\n",
        "      log_post_top0[j] = log_pi_d0 + log_pi_c[j] + el0 + sl0\n",
        "    \n",
        "      for k in range(n_clusters):\n",
        "        if torch.isnan(log_pi_cc[j,k]): #lower triangular nan\n",
        "          log_post_top1[j,k] = float(\"NaN\")\n",
        "        else:\n",
        "          el1 = D.MultivariateNormal((emu_c[j] + emu_c[k])/2, (eco_c[j] + eco_c[k])/2).log_prob(X.float())\n",
        "          sl1 = D.Normal(smu_c[j] + smu_c[k], sco_c[j] + sco_c[k]).log_prob(S.float())\n",
        "          log_post_top1[j,k] = log_pi_d1 + log_pi_cc[j,k] + el1 + sl1\n",
        "\n",
        "          #https://stats.stackexchange.com/questions/99363/mean-of-covariance-matrices\n",
        "          #https://stats.stackexchange.com/questions/214174/calculating-the-covariance-matrix-for-the-mean-of-variables\n",
        "\n",
        "    log_post_top1 = log_post_top1.reshape(n_clusters * n_clusters, n_obs) #reshape\n",
        "\n",
        "    ## make sure lower triangular matrix is NaN (a doublet can only be assigned to cluster 1 and 2 not 2 and 1, imposed ordering)\n",
        "    ignored_indices = torch.isnan(torch.logsumexp(log_post_top1, 1))\n",
        "    assert(ignored_indices.sum() == (n_clusters * n_clusters - n_clusters) / 2)\n",
        "\n",
        "    log_post_tot = torch.logsumexp(torch.vstack((log_post_top0, log_post_top1[~ignored_indices])),0)\n",
        "    loss = -torch.mean(log_post_tot)\n",
        "\n",
        "    log_post_bot0 = torch.logsumexp(log_post_top0, 0) #n\n",
        "    log_post_z = (log_post_top0 - log_post_bot0).T #nxc (rjk)\n",
        "\n",
        "    log_post_bot1 = torch.logsumexp(log_post_top1[~ignored_indices], 0) #n\n",
        "    log_post_g = (log_post_top1 - log_post_bot1).T #nx(cxc) (rijk)\n",
        "\n",
        "    log_post_d0 = log_post_bot0 - log_post_tot\n",
        "    log_post_d1 = log_post_bot1 - log_post_tot\n",
        "\n",
        "    log_post_dz = log_post_d0[:,None] + log_post_z\n",
        "    log_post_dg = log_post_d1[:,None] + log_post_g\n",
        "\n",
        "    ## M-step\n",
        "    log_n_c, smu_c, sco_c, emu_c, eco_c = _estimate_mean_cov_t1v2(X, S, log_post_dz)\n",
        "    log_pi_c = log_n_c - torch.log(torch.tensor(n_obs))\n",
        "  \n",
        "    log_n_cc, smu_cc, sco_cc, emu_cc, eco_cc = _estimate_mean_cov_t2v2(X, S, log_post_dg.reshape(n_obs, n_clusters, n_clusters))\n",
        "    log_pi_cc = log_n_cc - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "    log_pi_d0 = torch.logsumexp(log_post_d0, 0) - torch.log(torch.tensor(n_obs))\n",
        "    log_pi_d1 = torch.logsumexp(log_post_d1, 0) - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "    if abs(llv[-1] + loss) < tot:\n",
        "      break\n",
        "      \n",
        "    llv.append(-loss)\n",
        "    iter += 1\n",
        "\n",
        "  aic, bic = _ics(-loss, n_obs, n_features, n_clusters)\n",
        "  #print('Iteration', iter + 1, 'Likelihood: ', -loss, torch.exp(log_pi_d0), aic, bic)\n",
        "  print('Version 1', -loss, torch.exp(log_pi_d0), aic, bic)\n",
        "\n",
        "  out_hyperparams = [torch.exp(log_pi_d0), torch.exp(log_pi_c), torch.exp(log_pi_cc)]\n",
        "  out_params = [smu_c, sco_c, emu_c, eco_c, smu_cc, sco_cc, emu_cc, eco_cc]\n",
        "  return llv[1:], aic, bic, out_hyperparams, parameters"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHTHLSqllKTD"
      },
      "source": [
        "## torch.optim version (function)\n",
        "def em_v2(X, S, hyperparameters, parameters, n_epochs = 1000, tot = 1e-4):\n",
        "\n",
        "  n_obs, n_features = X.shape\n",
        "  n_clusters = len(hyperparameters[1])\n",
        "\n",
        "  log_pi_d0 = torch.log(hyperparameters[0])\n",
        "  log_pi_d1 = torch.log(1-hyperparameters[0])\n",
        "  log_pi_c = torch.log(hyperparameters[1])\n",
        "  log_pi_cc = torch.log(hyperparameters[2])\n",
        "\n",
        "  #opt = optim.SGD(parameters, lr=0.01)\n",
        "  opt = optim.Adam(parameters, lr=0.01)\n",
        "\n",
        "  iter = 0\n",
        "  llv = [0.0]\n",
        "  while iter < n_epochs:\n",
        "\n",
        "    log_post_top0 = torch.zeros(n_clusters, n_obs)\n",
        "    log_post_top1 = torch.zeros(n_clusters, n_clusters, n_obs)\n",
        "\n",
        "    for j in range(n_clusters):  \n",
        "      #el0 = D.MultivariateNormal(emu_c[j], eco_c[j]).log_prob(X.float())\n",
        "      #sl0 = D.Normal(smu_c[j], sco_c[j]).log_prob(S.float())\n",
        "\n",
        "      el0 = D.MultivariateNormal(parameters[0][j], parameters[1][j]).log_prob(X.float())\n",
        "      sl0 = D.Normal(parameters[2][j], parameters[3][j]).log_prob(S.float())\n",
        "\n",
        "      log_post_top0[j] = log_pi_d0 + log_pi_c[j] + el0 + sl0\n",
        "\n",
        "      for k in range(n_clusters):\n",
        "        if torch.isnan(log_pi_cc[j,k]): #lower triangular nan\n",
        "          log_post_top1[j,k] = float(\"NaN\")\n",
        "        else:\n",
        "          #el1 = D.MultivariateNormal(emu_cc[j,k]/2, eco_cc[j,k]/2).log_prob(X.float())\n",
        "          #sl1 = D.Normal(smu_cc[j,k], sco_cc[j,k]).log_prob(S.float())\n",
        "        \n",
        "          el1 = D.MultivariateNormal(parameters[4][j,k], parameters[5][j,k]).log_prob(X.float())\n",
        "          sl1 = D.Normal(parameters[6][j,k], parameters[7][j,k]).log_prob(S.float())\n",
        "          log_post_top1[j,k] = log_pi_d1 + log_pi_cc[j,k] + el1 + sl1\n",
        "\n",
        "    log_post_top1 = log_post_top1.reshape(n_clusters * n_clusters, n_obs) #reshape\n",
        "    \n",
        "    ignored_indices = torch.isnan(torch.logsumexp(log_post_top1, 1))\n",
        "    assert(ignored_indices.sum() == (n_clusters * n_clusters - n_clusters) / 2)\n",
        "  \n",
        "    log_post_tot = torch.logsumexp(torch.vstack((log_post_top0, log_post_top1[~ignored_indices])),0)\n",
        "    loss = -torch.mean(log_post_tot)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      parameters[0].clamp_(tot)\n",
        "      ##parameters[1].clamp_(tot)\n",
        "      parameters[2].clamp_(tot)\n",
        "      parameters[3].clamp_(tot)\n",
        "      parameters[4].clamp_(tot)\n",
        "      ##parameters[5].clamp_(tot)\n",
        "      parameters[6].clamp_(tot)\n",
        "      parameters[7].clamp_(tot)\n",
        "\n",
        "      parameters[1] += torch.eye(n_features) * 0.01\n",
        "      parameters[5] += torch.eye(n_features) * 0.01\n",
        "\n",
        "      log_post_bot0 = torch.logsumexp(log_post_top0, 0) #n\n",
        "      log_post_bot1 = torch.logsumexp(log_post_top1[~ignored_indices], 0) #n\n",
        "    \n",
        "      log_post_d0 = log_post_bot0 - log_post_tot\n",
        "      log_post_d1 = log_post_bot1 - log_post_tot\n",
        "      \n",
        "      log_post_z = (log_post_top0 - log_post_bot0).T #nxc (rjk)\n",
        "      log_post_g = (log_post_top1 - log_post_bot1).T #nx(cc) (rijk)\n",
        "\n",
        "      log_post_dz = log_post_d0[:,None] + log_post_z\n",
        "      log_post_dg = log_post_d1[:,None] + log_post_g\n",
        "\n",
        "      log_n_c = torch.logsumexp(log_post_dz, 0)\n",
        "      log_pi_c = log_n_c - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "      log_n_cc = torch.logsumexp(log_post_dg, 0).reshape(n_clusters, n_clusters)\n",
        "      log_pi_cc = log_n_cc - torch.log(torch.tensor(n_obs))  # cxc matrix\n",
        "\n",
        "      log_pi_d0 = torch.logsumexp(log_post_d0, 0) - torch.log(torch.tensor(n_obs))\n",
        "      log_pi_d1 = torch.logsumexp(log_post_d1, 0) - torch.log(torch.tensor(n_obs))\n",
        "\n",
        "    if abs(llv[-1] + loss) < tot:\n",
        "      break\n",
        "      \n",
        "    llv.append(-loss)\n",
        "    iter += 1\n",
        "\n",
        "  aic, bic = _ics(-loss, n_obs, n_features, n_clusters)\n",
        "  print('Version 2', -loss, torch.exp(log_pi_d0), aic, bic)\n",
        "\n",
        "  out_hyperparams = [torch.exp(log_pi_d0), torch.exp(log_pi_c), torch.exp(log_pi_cc)]\n",
        "  return llv[1:], aic, bic, out_hyperparams, parameters"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkmuEQff5nyO"
      },
      "source": [
        "def generateData(n_clusters = 3, n_obs = 10000, n_features = 2):\n",
        "\n",
        "  ## set ground true expressions ##\n",
        "\n",
        "  '''\n",
        "  true_expression_means = torch.tensor([\n",
        "      [1, 2],\n",
        "      [4, 3],\n",
        "      [7 , 9]\n",
        "  ])\n",
        "  '''\n",
        "  true_expression_means = torch.randint(1, 11, (n_clusters, n_features))\n",
        "  #print(true_expression_means)\n",
        "\n",
        "  true_expression_covs = torch.tensor([\n",
        "      [[.01, 0], [0, .01]],\n",
        "      [[.01, 0], [0, .01]],\n",
        "      [[.01, 0], [0, .01]]\n",
        "  ])\n",
        "\n",
        "  #true_size_means = torch.tensor([.4, .5, .6])\n",
        "  true_size_means = torch.rand(n_clusters)\n",
        "  #print(true_size_means)\n",
        "\n",
        "  true_size_stds = torch.tensor([.05, .05, .05])\n",
        "  #true_size_stds\n",
        "\n",
        "  ## other ground true for generating data ##\n",
        "  d_ws = torch.tensor([.95, .05])\n",
        "  \n",
        "  #z_ws = torch.tensor([1 / 4, 1 / 2, 1 / 4])\n",
        "  z_ws = np.random.rand(n_clusters)\n",
        "  z_ws /= z_ws.sum()\n",
        "  #print(z_ws.sum())\n",
        "\n",
        "  #g_ws = torch.tensor([0.0667, 0.1333, 0.2000, 0.1000, 0.2667, 0.2333])\n",
        "  n_events = int((n_clusters * n_clusters - n_clusters)/2 + n_clusters)\n",
        "  g_ws = np.random.rand(n_events)\n",
        "  g_ws /= g_ws.sum()\n",
        "  #print(g_ws.sum())\n",
        "  \n",
        "  gs = np.sum(np.random.choice(2, size = n_obs, p = d_ws))\n",
        "  zs = n_obs - gs\n",
        "\n",
        "  ## simulate data\n",
        "  x = np.zeros((zs, n_features+4))\n",
        "  for i in range(zs):\n",
        "    z = np.random.choice(n_clusters, size = 1, p = z_ws)[0]\n",
        "    x[i] = np.append(np.random.multivariate_normal(true_expression_means[z], true_expression_covs[z]), [np.random.normal(true_size_means[z], true_size_stds[z]), 0, z, z+6])\n",
        "  \n",
        "  xxx = np.zeros((gs, n_features+4))\n",
        "  for i in range(gs):\n",
        "\n",
        "    g = np.random.choice(6, size = 1, p = g_ws)[0]\n",
        "    \n",
        "    if g == 0:\n",
        "      idx = [0,0]\n",
        "    elif g == 1:\n",
        "      idx = [0,1]\n",
        "    elif g == 2:\n",
        "      idx = [0,2]\n",
        "    elif g == 3:\n",
        "      idx = [1,1]\n",
        "    elif g == 4:\n",
        "      idx = [1,2]\n",
        "    else:\n",
        "      idx = [2,2]\n",
        "  \n",
        "    xxx[i] = np.append(np.random.multivariate_normal( (true_expression_means[idx[0]] + true_expression_means[idx[1]]), (true_expression_covs[idx[0]] + true_expression_covs[idx[1]]) ),\n",
        "                     [np.random.normal( (true_size_means[idx[0]] + true_size_means[idx[1]]), (true_size_stds[idx[0]] + true_size_stds[idx[1]]) ), 1, g, g])\n",
        "  \n",
        "  xx = np.append(x, xxx).reshape(n_obs,6)\n",
        "\n",
        "  return torch.tensor(xx[:,:2]), torch.tensor(xx[:,2])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1oREOkMfoOv"
      },
      "source": [
        "## initialization\n",
        "def initialization(n_clusters, X, S):\n",
        "\n",
        "  n_obs, n_features = X.shape\n",
        "\n",
        "  #torch.manual_seed(seed_num)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "  #pi_d0 = torch.tensor(0.9)\n",
        "  pi_d0 = torch.tensor(np.random.uniform(.9, 1))\n",
        "  \n",
        "  pi_c = torch.empty(n_clusters).fill_(1. / (n_clusters))\n",
        "  \n",
        "  pi_cc = torch.triu(torch.ones(n_clusters, n_clusters))\n",
        "  pi_cc = pi_cc / torch.sum(pi_cc)\n",
        "  pi_cc[pi_cc == 0] = float('NaN')\n",
        "\n",
        "  emu_c = X[np.random.choice(n_obs, n_clusters, replace=False)].float()\n",
        "\n",
        "  '''\n",
        "  emu_c = torch.tensor([\n",
        "    [2, 4], # 1 2\n",
        "    [3, 5], # 4 3\n",
        "    [5, 7] #7 9\n",
        "  ], dtype=torch.float) #, requires_grad=True, , device=device)\n",
        "  '''\n",
        "\n",
        "  smu_c = S[np.random.choice(n_obs, n_clusters, replace=False)].float()\n",
        "  #smu_c = torch.tensor([.3, .4, .5], dtype=torch.float) #, requires_grad=True,  device=device)\n",
        "\n",
        "  eco_c = 0.05 * torch.eye(n_features).tile(n_clusters, 1, 1)\n",
        "  sco_c = 0.05 * torch.ones(n_clusters, dtype=torch.float)\n",
        "\n",
        "  smu_cc = torch.zeros(n_clusters, n_clusters, dtype=torch.float)\n",
        "  sco_cc = torch.zeros(n_clusters, n_clusters, dtype=torch.float)\n",
        "\n",
        "  emu_cc = torch.zeros(n_clusters, n_clusters, n_features, dtype=torch.float)\n",
        "  eco_cc = torch.zeros(n_clusters, n_clusters, n_features, n_features, dtype=torch.float)\n",
        "\n",
        "  for j in range(n_clusters):\n",
        "    for k in range(n_clusters):\n",
        "      if k >= j:\n",
        "        smu_cc[j,k] = smu_c[j] + smu_c[k]\n",
        "        sco_cc[j,k] = sco_c[j] + sco_c[k]\n",
        "\n",
        "        emu_cc[j,k] = (emu_c[j] + emu_c[k])\n",
        "        eco_cc[j,k] = (eco_c[j] + eco_c[k])\n",
        "\n",
        "  params1 = [emu_c, eco_c, smu_c, sco_c, emu_cc, eco_cc, smu_cc, sco_cc]\n",
        "\n",
        "  smu_c = torch.tensor(smu_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "  sco_c = torch.tensor(sco_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "      \n",
        "  emu_c = torch.tensor(emu_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "  eco_c = torch.tensor(eco_c, requires_grad=True, dtype=torch.float, device=device)\n",
        "      \n",
        "  smu_cc = torch.tensor(smu_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "  sco_cc = torch.tensor(sco_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "      \n",
        "  emu_cc = torch.tensor(emu_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "  eco_cc = torch.tensor(eco_cc, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "  hyperparams = [pi_d0, pi_c, pi_cc]\n",
        "  params2 = [emu_c, eco_c, smu_c, sco_c, emu_cc, eco_cc, smu_cc, sco_cc]\n",
        "\n",
        "  return hyperparams, params1, params2"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdoSgTht2PEn",
        "outputId": "4ee06268-9101-4fb4-bc1b-6e673000b1a6"
      },
      "source": [
        "n_trials = 5\n",
        "res = np.zeros((n_trials, 6))\n",
        "for trial in range(n_trials):\n",
        "\n",
        "  # generate true data\n",
        "  X, S = generateData(n_clusters = 3, n_obs = 10000, n_features = 2)\n",
        "\n",
        "  # model different clusters\n",
        "  for sn_clust in range(2, 6):\n",
        "    \n",
        "    #seed_id = torch.randint(0, 100, (1,))\n",
        "    inits = initialization(sn_clust, X, S)\n",
        "    try:\n",
        "      fit1 = em_v1(X, S, inits[0], inits[1], n_epochs = 5000)\n",
        "      fit2 = em_v2(X, S, inits[0], inits[2], n_epochs = 5000)\n",
        "      print(np.append(fit1[1:3], fit2[1:3]))\n",
        "      res[trial] = np.append(fit1[1:3], [fit2[1:3], trial, sn_clust])\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Version 1 tensor(-7.9144) tensor(0.8516) tensor(93.8288) tensor(375.0320)\n",
            "Version 2 tensor(-3.6576, grad_fn=<NegBackward>) tensor(0.9809) tensor(85.3152, grad_fn=<MulBackward0>) tensor(366.5184, grad_fn=<AddBackward0>)\n",
            "[93.82877349853516 375.03204345703125\n",
            " tensor(85.3152, grad_fn=<MulBackward0>)\n",
            " tensor(366.5184, grad_fn=<AddBackward0>)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Version 1 tensor(0.6449) tensor(1.) tensor(140.7102) tensor(652.6443)\n",
            "Version 2 tensor(-3.7174, grad_fn=<NegBackward>) tensor(0.9885) tensor(149.4349, grad_fn=<MulBackward0>) tensor(661.3690, grad_fn=<AddBackward0>)\n",
            "[140.710205078125 652.6443481445312\n",
            " tensor(149.4349, grad_fn=<MulBackward0>)\n",
            " tensor(661.3690, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(0.6524) tensor(1.) tensor(220.6951) tensor(1021.0429)\n",
            "Version 2 tensor(-0.7254, grad_fn=<NegBackward>) tensor(0.8309) tensor(223.4509, grad_fn=<MulBackward0>) tensor(1023.7986, grad_fn=<AddBackward0>)\n",
            "[220.6951446533203 1021.0429077148438\n",
            " tensor(223.4509, grad_fn=<MulBackward0>)\n",
            " tensor(1023.7986, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(1.5598) tensor(0.9961) tensor(314.8804) tensor(1461.3246)\n",
            "Version 1 tensor(-131.5813) tensor(0.9465) tensor(341.1627) tensor(622.3660)\n",
            "Version 2 tensor(-4.2519, grad_fn=<NegBackward>) tensor(0.9467) tensor(86.5038, grad_fn=<MulBackward0>) tensor(367.7071, grad_fn=<AddBackward0>)\n",
            "[341.16265869140625 622.365966796875\n",
            " tensor(86.5038, grad_fn=<MulBackward0>)\n",
            " tensor(367.7071, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-154.1319) tensor(0.9465) tensor(450.2637) tensor(962.1979)\n",
            "Version 2 tensor(-4.1023, grad_fn=<NegBackward>) tensor(0.9465) tensor(150.2046, grad_fn=<MulBackward0>) tensor(662.1387, grad_fn=<AddBackward0>)\n",
            "[450.2637023925781 962.1978759765625\n",
            " tensor(150.2046, grad_fn=<MulBackward0>)\n",
            " tensor(662.1387, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-37.2704) tensor(0.9628) tensor(296.5408) tensor(1096.8885)\n",
            "Version 1 tensor(-154.1311) tensor(0.9465) tensor(626.2622) tensor(1772.7063)\n",
            "Version 1 tensor(-237.8937) tensor(0.3778) tensor(553.7874) tensor(834.9907)\n",
            "Version 2 tensor(-2.5343, grad_fn=<NegBackward>) tensor(0.9070) tensor(83.0686, grad_fn=<MulBackward0>) tensor(364.2718, grad_fn=<AddBackward0>)\n",
            "[553.7874145507812 834.99072265625 tensor(83.0686, grad_fn=<MulBackward0>)\n",
            " tensor(364.2718, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-3.2267) tensor(0.9576) tensor(148.4533) tensor(660.3875)\n",
            "Version 2 tensor(-3.0029, grad_fn=<NegBackward>) tensor(0.9077) tensor(148.0058, grad_fn=<MulBackward0>) tensor(659.9400, grad_fn=<AddBackward0>)\n",
            "[148.45330810546875 660.387451171875\n",
            " tensor(148.0058, grad_fn=<MulBackward0>)\n",
            " tensor(659.9400, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(1.6160) tensor(1.) tensor(218.7679) tensor(1019.1157)\n",
            "Version 1 tensor(1.9924) tensor(1.) tensor(314.0152) tensor(1460.4592)\n",
            "Version 2 tensor(-3.2565, grad_fn=<NegBackward>) tensor(0.5289) tensor(324.5131, grad_fn=<MulBackward0>) tensor(1470.9572, grad_fn=<AddBackward0>)\n",
            "[314.0151672363281 1460.459228515625\n",
            " tensor(324.5131, grad_fn=<MulBackward0>)\n",
            " tensor(1470.9572, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-1.4243) tensor(0.9935) tensor(80.8487) tensor(362.0520)\n",
            "Version 2 tensor(-3.1284, grad_fn=<NegBackward>) tensor(0.9605) tensor(84.2568, grad_fn=<MulBackward0>) tensor(365.4601, grad_fn=<AddBackward0>)\n",
            "[80.84868621826172 362.0519714355469\n",
            " tensor(84.2568, grad_fn=<MulBackward0>)\n",
            " tensor(365.4601, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-1.4243) tensor(0.9935) tensor(144.8487) tensor(656.7828)\n",
            "Version 2 tensor(-5.7758, grad_fn=<NegBackward>) tensor(0.9886) tensor(153.5516, grad_fn=<MulBackward0>) tensor(665.4857, grad_fn=<AddBackward0>)\n",
            "[144.84869384765625 656.7828369140625\n",
            " tensor(153.5516, grad_fn=<MulBackward0>)\n",
            " tensor(665.4857, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(1.9203) tensor(1.) tensor(218.1594) tensor(1018.5072)\n",
            "Version 2 tensor(-5.3518, grad_fn=<NegBackward>) tensor(0.9623) tensor(232.7036, grad_fn=<MulBackward0>) tensor(1033.0514, grad_fn=<AddBackward0>)\n",
            "[218.15940856933594 1018.5072021484375\n",
            " tensor(232.7036, grad_fn=<MulBackward0>)\n",
            " tensor(1033.0514, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(1.9208) tensor(1.) tensor(314.1584) tensor(1460.6025)\n",
            "Version 2 tensor(-3.4036, grad_fn=<NegBackward>) tensor(0.9589) tensor(324.8072, grad_fn=<MulBackward0>) tensor(1471.2513, grad_fn=<AddBackward0>)\n",
            "[314.1584167480469 1460.6025390625\n",
            " tensor(324.8072, grad_fn=<MulBackward0>)\n",
            " tensor(1471.2513, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-3.6625) tensor(0.9640) tensor(85.3250) tensor(366.5283)\n",
            "Version 2 tensor(-3.5454, grad_fn=<NegBackward>) tensor(0.9582) tensor(85.0907, grad_fn=<MulBackward0>) tensor(366.2940, grad_fn=<AddBackward0>)\n",
            "[85.3249740600586 366.52825927734375\n",
            " tensor(85.0907, grad_fn=<MulBackward0>)\n",
            " tensor(366.2940, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-8.2876) tensor(0.9571) tensor(158.5751) tensor(670.5093)\n",
            "Version 2 tensor(-2.4077, grad_fn=<NegBackward>) tensor(0.9519) tensor(146.8155, grad_fn=<MulBackward0>) tensor(658.7496, grad_fn=<AddBackward0>)\n",
            "[158.5751190185547 670.50927734375\n",
            " tensor(146.8155, grad_fn=<MulBackward0>)\n",
            " tensor(658.7496, grad_fn=<AddBackward0>)]\n",
            "Version 1 tensor(-8.0812) tensor(0.9571) tensor(238.1624) tensor(1038.5103)\n",
            "Version 1 tensor(1.0521) tensor(0.9877) tensor(315.8957) tensor(1462.3398)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErHu-Up_T5HL",
        "outputId": "bb039116-7643-490e-b869-054cce15cb4c"
      },
      "source": [
        "res"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}