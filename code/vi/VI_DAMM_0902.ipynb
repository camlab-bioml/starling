{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VI_DAMM_0902.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOToFfeMQI4rpSlfgpfwIqi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camlab-bioml/2021_IMC_Jett/blob/main/VI_DAMM_0902.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8bx7aq-lq2Q"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as datasets\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBiqVkEyrNjF"
      },
      "source": [
        "#%%capture\n",
        "#!pip install wandb --upgrade"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJxAa9PdpRiY"
      },
      "source": [
        "#import wandb\n",
        "#wandb.login()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxK61unVln3j"
      },
      "source": [
        "## generate data\n",
        "def generateData(n_clusters = 3, n_obs = 10000, n_features = 2):\n",
        "\n",
        "  #n_clusters = 3; n_obs = 100; n_features = 2\n",
        "  \n",
        "  ## set truth expression means/covariances (multivariate) ##\n",
        "  mu = np.random.rand(n_clusters, n_features)\n",
        "  # mu = np.sort(mu, 0) ## sort expressions\n",
        "  sigma = 0.001 * np.identity(n_features) ## variance-covariance matrix\n",
        "\n",
        "  ## set truth cell size means/variances (univariate) ##\n",
        "  psi = [np.random.normal(100, 25) for i in range(n_clusters)]\n",
        "  #psi = np.arange(90, 90 + 5 * n_clusters, 5)\n",
        "  psi = np.sort(psi, 0)\n",
        "  omega = 1 ## standard deviation\n",
        "  ###\n",
        "\n",
        "  ## set latent variables distributions ##\n",
        "  lambda_arr = np.random.binomial(1, .95, n_obs) # p=.95 (a cell belongs to singlet or doublet) \n",
        "\n",
        "  n_singlet = np.sum(lambda_arr == 1) ## number of cells in singlet clusters\n",
        "  n_doublet = np.sum(lambda_arr == 0) ## number of cells in doublet clusters\n",
        "  \n",
        "  lambda0_arr = n_singlet / n_obs ## proportion of cells belong to singlet\n",
        "  lambda1_arr = n_doublet / n_obs ## proportion of cells belong to doublet\n",
        "\n",
        "  #pi_arr = np.sort(np.random.sample(n_clusters))\n",
        "  pi_arr = np.sort(np.random.rand(n_clusters))\n",
        "  pi_arr /= pi_arr.sum()\n",
        "\n",
        "  n_doublet_clusters = int((n_clusters * n_clusters - n_clusters)/2 + n_clusters)\n",
        "  #tau_arr = np.sort(np.random.sample(n_doublet_clusters))\n",
        "  tau_arr = np.sort(np.random.rand(n_doublet_clusters))\n",
        "  tau_arr /= tau_arr.sum()\n",
        "\n",
        "  ## draw cells based on defined parameters theta1 = (mu, sigma, psi, omega) & theta2 = (lambda, pi, tau)\n",
        "  x = np.zeros((n_singlet, n_features+5))\n",
        "  for i in range(n_singlet):\n",
        "    selected_cluster = np.random.choice(n_clusters, size = 1, p = pi_arr)[0] ## select a single cell cluster\n",
        "    x[i] = np.append(np.random.multivariate_normal(mu[selected_cluster], sigma),\n",
        "                     [np.random.normal(psi[selected_cluster], omega), \n",
        "                      0, selected_cluster, 0, selected_cluster + n_doublet_clusters])\n",
        "  \n",
        "  x[x < 0] = 1e-4\n",
        "  lookups = np.triu_indices(n_clusters) # wanted indices\n",
        "  xx = np.zeros((n_doublet, n_features+5))\n",
        "  for i in range(n_doublet):\n",
        "    selected_cluster = np.random.choice(n_doublet_clusters, p = tau_arr)\n",
        "\n",
        "    indx1 = lookups[0][selected_cluster]\n",
        "    indx2 = lookups[1][selected_cluster]\n",
        "\n",
        "    xx[i] = np.append(np.random.multivariate_normal( (mu[indx1] + mu[indx2])/2, (sigma + sigma)/2 ),\n",
        "                     [np.random.normal( (psi[indx1] + psi[indx2]), omega+omega ), \n",
        "                      1, indx1, indx2, selected_cluster])\n",
        "  xx[xx < 0] = 1e-4\n",
        "  xxx = np.append(x, xx).reshape(n_obs, n_features+5)\n",
        "\n",
        "  truth_theta = {\n",
        "    'log_mu': np.log(mu),\n",
        "    'log_sigma': np.log(sigma),\n",
        "    'log_psi': np.log(psi),\n",
        "    'log_omega': np.log(omega),\n",
        "    \"log_lambda0\": np.log(lambda0_arr),\n",
        "    'log_pi': np.log(pi_arr),\n",
        "    'log_tau': np.log(tau_arr)\n",
        "  }\n",
        "\n",
        "  return xxx[:,:n_features], xxx[:,n_features], xxx, truth_theta\n",
        "\n",
        "  #return torch.tensor(xxx[:,:n_features]), torch.tensor(xxx[:,n_features]), torch.tensor(xxx), [mu, sigma, psi, omega], [lambda0_arr, pi_arr, tau_arr]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onvr3wPluMep"
      },
      "source": [
        "def compute_p_y_given_z(Y, Theta):\n",
        "  \"\"\" Returns NxC\n",
        "  p(y_n | z_n = c)\n",
        "  \"\"\"\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma'])\n",
        "\n",
        "  dist_Y = D.Normal(mu, sigma)\n",
        "  return dist_Y.log_prob(Y.reshape(Y.shape[0], 1, nf)).sum(2) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_z(S, Theta):\n",
        "  \"\"\" Returns NxC\n",
        "  p(s_n | z_n = c)\n",
        "  \"\"\"\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega'])\n",
        "\n",
        "  dist_S = D.Normal(psi, omega)\n",
        "  return dist_S.log_prob(S.reshape(-1,1)) \n",
        "\n",
        "def compute_p_y_given_gamma(Y, Theta):\n",
        "  \"\"\" NxCxC\n",
        "  p(y_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma'])\n",
        "\n",
        "  mu2 = mu.reshape(1, nc, nf)\n",
        "  mu2 = (mu2 + mu2.permute(1, 0, 2)) / 2.0 # C x C x G matrix \n",
        "\n",
        "  sigma2 = sigma.reshape(1, nc, nf)\n",
        "  sigma2 = (sigma2 + sigma2.permute(1,0,2)) / 2.0\n",
        "\n",
        "  dist_Y2 = D.Normal(mu2, sigma2)\n",
        "  return  dist_Y2.log_prob(Y.reshape(-1, 1, 1, nf)).sum(3) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_gamma(S, Theta):\n",
        "  \"\"\" NxCxC\n",
        "  p(s_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega'])\n",
        "\n",
        "  psi2 = psi.reshape(-1,1)\n",
        "  psi2 = psi2 + psi2.T\n",
        "\n",
        "  omega2 = omega.reshape(-1,1)\n",
        "  omega2 = omega2 + omega2.T\n",
        "\n",
        "  dist_S2 = D.Normal(psi2, omega2)\n",
        "  return dist_S2.log_prob(S.reshape(-1, 1, 1))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb4OWe1ouixI"
      },
      "source": [
        "class BasicForwardNet(nn.Module):\n",
        "  \"\"\"Encoder for when data is input without any encoding\"\"\"\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim = 5):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input = nn.Linear(input_dim, hidden_dim)\n",
        "    self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.output = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.input(x))\n",
        "    out = F.relu(self.linear1(out))\n",
        "    out = self.output(out)\n",
        "        \n",
        "    return F.softmax(out, dim=1), F.log_softmax(out, dim=1) ## r/v/d log_r/log_v/log_d"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDKzLK7fvUMS"
      },
      "source": [
        "def compute_joint_probs(Theta, Y, S):\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'])\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1)).reshape(nc,nc)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'])\n",
        "  \n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta)\n",
        "  p_s_given_z = compute_p_s_given_z(S, Theta)\n",
        "\n",
        "  log_rzd0 = p_s_given_z + p_y_given_z + log_pi + log_delta[0]\n",
        "\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta)\n",
        "  p_s_given_gamma = compute_p_s_given_gamma(S, Theta)\n",
        "\n",
        "  log_vgd1 = p_y_given_gamma + p_s_given_gamma + log_tau + log_delta[1]\n",
        "\n",
        "  #remove_indices = np.tril_indices(nc, -1) ## remove indices\n",
        "  #log_rd1g[:, remove_indices[0], remove_indices[1]] = float(\"NaN\")\n",
        "\n",
        "  #q1 = r.exp() * log_rd0z #; q1[torch.isnan(q1)] = 0.0\n",
        "  #q2 = v.exp() * log_rd1g #; q2[torch.isnan(q2)] = 0.0\n",
        "\n",
        "  return log_rzd0, log_vgd1.reshape(Y.shape[0], nc*nc)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyZnekKcJY4E"
      },
      "source": [
        "#F.sigmoid(torch.tensor([.3, .6]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xx9Xqzt2Agp",
        "outputId": "9c9f870e-d54f-4151-fbb9-f84c96df6902"
      },
      "source": [
        "nc = 5; no = 1000; nf = 10\n",
        "\n",
        "Y, S, XX, theta_true = generateData(n_clusters = nc, n_obs = no, n_features = nf)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dyDPpWuwu02"
      },
      "source": [
        "N_INIT = 20\n",
        "\n",
        "Y = np.array(Y)\n",
        "S = np.array(S)\n",
        "\n",
        "kms = [KMeans(nc).fit(Y) for i in range(N_INIT)]\n",
        "inertias = [k.inertia_ for k in kms]\n",
        "km = kms[np.argmin(np.array(inertias))] ## selected \"best\" kmeans based on inertia score\n",
        "init_labels = km.labels_\n",
        "\n",
        "mu_init = np.array([Y[init_labels == i,:].mean(0) for i in np.unique(init_labels)])\n",
        "sigma_init = np.array([Y[init_labels == i,:].std(0) for i in np.unique(init_labels)])\n",
        "psi_init = np.array([S[init_labels == i].mean() for i in np.unique(init_labels)])\n",
        "omega_init = np.array([S[init_labels == i].std() for i in np.unique(init_labels)])\n",
        "pi_init = np.array([np.mean(init_labels == i) for i in np.unique(init_labels)])\n",
        "tau_init = np.ones((nc,nc))\n",
        "tau_init = tau_init / tau_init.sum()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApeDIuoCvRk5"
      },
      "source": [
        "P = Y.shape[1] + 1\n",
        "r_net = BasicForwardNet(P, nc)\n",
        "v_net = BasicForwardNet(P, nc ** 2)\n",
        "d_net = BasicForwardNet(P, 2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIHkZCfW4cA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06928708-f64b-462e-bbc5-789147754715"
      },
      "source": [
        "Theta = {\n",
        "    'log_mu': np.log(mu_init) + 0.05 * np.random.randn(mu_init.shape[0], mu_init.shape[1]),\n",
        "    'log_sigma': np.log(sigma_init), #np.zeros_like(sigma_init),\n",
        "    'log_psi': np.log(psi_init),\n",
        "    'log_omega': np.log(omega_init),\n",
        "    \"is_delta\": F.log_softmax(torch.tensor([0.95, 1-0.95])),\n",
        "    'is_pi': F.log_softmax(torch.tensor(pi_init)),\n",
        "    'is_tau': F.log_softmax(torch.tensor(tau_init))\n",
        "}\n",
        "Theta = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}\n",
        "\n",
        "Theta['is_delta'].requires_grad = False\n",
        "#Theta['is_pi'].requires_grad = False\n",
        "#Theta['is_tau'].requires_grad = False"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JJj8U-O3Chf"
      },
      "source": [
        "Y = torch.tensor(Y)\n",
        "S = torch.tensor(S)\n",
        "YS = torch.hstack((Y,S.reshape(-1,1))).float()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_e_N_--2lWU"
      },
      "source": [
        "N_ITER = 100000\n",
        "lr = 1e-3\n",
        "tol = 1e-6\n",
        "params = list(Theta.values()) + list(r_net.parameters()) + list(v_net.parameters()) + list(d_net.parameters())\n",
        "opt = optim.AdamW(params, lr=lr)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdeRpKNYEVoP"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4f_Y5KOE9jh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "d8b6e7d8-8b09-4e2b-95bd-38b21a35fd32"
      },
      "source": [
        "wandb.init(project='jett-vi',\n",
        "           config={\n",
        "    \"N_EPOCHS\": N_ITER,\n",
        "    \"LR\": lr,\n",
        "    \"TOL\": tol,\n",
        "    'MODEL_TYPE': 'vi',\n",
        "    'DATA_TYPE': 'toy_data'\n",
        "    })"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.1<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">astral-smoke-45</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/yujulee/jett-vi\" target=\"_blank\">https://wandb.ai/yujulee/jett-vi</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/yujulee/jett-vi/runs/11exwzcl\" target=\"_blank\">https://wandb.ai/yujulee/jett-vi/runs/11exwzcl</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210902_174029-11exwzcl</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f2f723cbcd0>"
            ],
            "text/html": [
              "<h1>Run(11exwzcl)</h1><iframe src=\"https://wandb.ai/yujulee/jett-vi/runs/11exwzcl\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lkz1HEc3pmm"
      },
      "source": [
        "#elbo1 = q0 * (log_rzd0 - log_q0)\n",
        "  #elbo2 = q1 * (log_vgd1 - log_q1)\n",
        "  #nelbo = elbo1.sum() + elbo2.sum()\n",
        "  #-elbo, entro, recon\n",
        "\n",
        "  #assert( ((d_net(YS)[0][:,1].reshape(-1,1) * v_net(YS)[0]).sum(1) + (d_net(YS)[0][:,0].reshape(-1,1) * r_net(YS)[0]).sum(1)).sum() == 1000. )\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDtVIym2y4eS",
        "outputId": "a953dcfe-d1d9-4cbf-b6aa-61da9509de89"
      },
      "source": [
        "F.log_softmax(Theta['is_delta']).exp()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7109, 0.2891])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXPBFeNGYhjq",
        "outputId": "bdc02787-4000-4eb7-c5f9-4544885e81dc"
      },
      "source": [
        "np.exp(theta_true['log_lambda0'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.948"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZThbK27Z_-sG",
        "outputId": "e5b8e091-45ad-44ba-f8df-394fba0124bb"
      },
      "source": [
        "F.log_softmax(Theta['is_pi']).exp()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2379, 0.2205, 0.2082, 0.1656, 0.1678], dtype=torch.float64,\n",
              "       grad_fn=<ExpBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_hzSFsLMBZz",
        "outputId": "26221d8e-7f8c-4a65-9f6d-cf6c84a02025"
      },
      "source": [
        "np.exp(theta_true['log_pi'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02103361, 0.02438582, 0.22756926, 0.31859506, 0.40841625])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WS9NxwqNxqJ"
      },
      "source": [
        "#F.log_softmax(v.mean(0).reshape(-1)).reshape(nc, nc).exp()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRpGlo4nAF6F",
        "outputId": "754b3c72-5a1a-4d5c-d557-ca165e6bfbfe"
      },
      "source": [
        "F.log_softmax(Theta['is_tau'].reshape(-1)).reshape(nc, nc).exp()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
              "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
              "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
              "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
              "        [0.0400, 0.0400, 0.0400, 0.0400, 0.0400]], dtype=torch.float64,\n",
              "       grad_fn=<ExpBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdbkCJKWMLud",
        "outputId": "26a53b91-07e5-4bd0-ebcd-c1929bf5858e"
      },
      "source": [
        "np.exp(theta_true['log_tau'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00630912, 0.01578501, 0.04260968, 0.0430699 , 0.04610766,\n",
              "       0.0507849 , 0.06659788, 0.07802901, 0.08070339, 0.08388491,\n",
              "       0.08659892, 0.09785327, 0.09790554, 0.10138304, 0.10237778])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjEgccaxPYHj"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPWjfsZnAWXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623da538-86f0-4b32-e162-29822fb3b16c"
      },
      "source": [
        "Theta['log_mu'].exp()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9598, 0.4289, 0.2520, 0.5270, 0.6664, 0.4022, 0.0882, 0.3180, 0.2526,\n",
              "         0.0664],\n",
              "        [0.2401, 0.0269, 0.5955, 0.9111, 0.0357, 0.2994, 0.5842, 0.9004, 0.4905,\n",
              "         0.1744],\n",
              "        [0.3114, 0.1989, 0.9038, 0.0275, 0.5197, 0.1303, 0.1461, 0.8533, 0.0637,\n",
              "         0.0738],\n",
              "        [0.1133, 0.4906, 0.2214, 0.1646, 0.1766, 0.2295, 0.5400, 0.2331, 0.3784,\n",
              "         0.6832],\n",
              "        [0.3939, 0.6383, 0.8990, 0.5421, 0.1434, 0.1496, 0.6564, 0.8707, 0.1226,\n",
              "         0.2588]], dtype=torch.float64, grad_fn=<ExpBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIidtiH6m4gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e8cfb0-0d00-486c-e0d9-1c1ecdbf69c8"
      },
      "source": [
        "np.exp(theta_true['log_mu'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00904657, 0.55598951, 0.09080346, 0.09733775, 0.11390822,\n",
              "        0.23035873, 0.60357522, 0.1050592 , 0.38603035, 0.83302096],\n",
              "       [0.43674605, 0.76201288, 0.95740535, 0.49211773, 0.08576602,\n",
              "        0.1148113 , 0.71157188, 0.96032855, 0.0777623 , 0.2623704 ],\n",
              "       [0.31573514, 0.18051475, 0.95159539, 0.00943698, 0.47352374,\n",
              "        0.11908228, 0.14722634, 0.87920615, 0.06163368, 0.07974902],\n",
              "       [0.24253041, 0.02465594, 0.59050483, 0.93315681, 0.0313658 ,\n",
              "        0.30368694, 0.53299172, 0.91242219, 0.48882501, 0.18550026],\n",
              "       [0.96780974, 0.44234748, 0.26163233, 0.55654946, 0.70978338,\n",
              "        0.39712541, 0.08835898, 0.30419818, 0.24552656, 0.06124418]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xonq-Eg_M2ha"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quUNlLG8M2p_"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG99DhX6M2zE",
        "outputId": "62cd01d7-7998-43b6-e871-75495eb7f03f"
      },
      "source": [
        "loss = []\n",
        "for i in range(N_ITER):\n",
        "  \n",
        "  opt.zero_grad()\n",
        "\n",
        "  r, log_r = r_net(YS)\n",
        "  v, log_v = v_net(YS)\n",
        "  d, log_d = d_net(YS)\n",
        "  \n",
        "  ## row sums to 1 (from neural net)\n",
        "  log_q0 = log_d[:,0].reshape(-1,1) + log_r ## like r in em version\n",
        "  log_q1 = log_d[:,1].reshape(-1,1) + log_v ## like v in em version\n",
        "\n",
        "  log_rzd0, log_vgd1 = compute_joint_probs(Theta, Y, S)\n",
        "\n",
        "  entro = (log_q0.exp() * log_q0).sum() + (log_q1.exp() * log_q1).sum()\n",
        "  recon = (log_q0.exp() * log_rzd0).sum() + (log_q1.exp() * log_vgd1).sum()\n",
        "  nelbo = entro - recon\n",
        "\n",
        "  #nelbo = (log_q0.exp() * (log_q0 - log_rzd0)).sum() + (log_q1.exp() * (log_q1 - log_vgd1)).sum()\n",
        "\n",
        "  nelbo.backward()\n",
        "  opt.step()\n",
        "  \n",
        "  wandb.log({\n",
        "    'ITER': i + 1, \n",
        "    'nelbo': nelbo.detach(),\n",
        "    'entropy': entro.detach(),\n",
        "    'reconstruction_loss': recon.detach(),\n",
        "    'log_mu': Theta['log_mu'],\n",
        "    'log_sigma': Theta['log_sigma'], #np.zeros_like(sigma_init),\n",
        "    'log_psi': Theta['log_psi'],\n",
        "    'log_omega': Theta['log_omega'],\n",
        "    \"is_delta\": Theta['is_delta'],\n",
        "    'is_pi': Theta['is_pi'],\n",
        "    'is_tau': Theta['is_tau'],\n",
        "    'r': r,\n",
        "    'v': v,\n",
        "    'd': d,\n",
        "    })\n",
        "  \n",
        "  if i % (1000 - 1) == 0:\n",
        "    #print(\"NELBO: {}; lambda: {}; pi: {}\".format(nelbo.detach(), F.log_softmax(Theta['is_delta'].detach()).exp(), F.log_softmax(Theta['is_pi'].detach()).exp()))\n",
        "    #print(\"NELBO: {}; entro: {}; recon: {}; pi: {}\".format(nelbo.detach(), entro.detach(), recon.detach(), F.log_softmax(Theta['is_pi'].detach()).exp()))\n",
        "    print(\"NELBO: {}; pi: {}\".format(nelbo.detach(), F.log_softmax(Theta['is_pi'].detach()).exp()))\n",
        "  \n",
        "  if i > 0 and abs(loss[-1] - nelbo.detach()) < tol:\n",
        "    break\n",
        "           \n",
        "  loss.append(nelbo.detach())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NELBO: 212923.44381713963; pi: tensor([0.2382, 0.2203, 0.2081, 0.1655, 0.1680], dtype=torch.float64)\n",
            "NELBO: 7045.969665988164; pi: tensor([0.2507, 0.2076, 0.1962, 0.1645, 0.1810], dtype=torch.float64)\n",
            "NELBO: 5304.534763378304; pi: tensor([0.2502, 0.2073, 0.1960, 0.1650, 0.1814], dtype=torch.float64)\n",
            "NELBO: 4817.916598155215; pi: tensor([0.2496, 0.2072, 0.1960, 0.1655, 0.1818], dtype=torch.float64)\n",
            "NELBO: 4685.253952916848; pi: tensor([0.2491, 0.2070, 0.1959, 0.1659, 0.1821], dtype=torch.float64)\n",
            "NELBO: 4622.829624605565; pi: tensor([0.2486, 0.2069, 0.1959, 0.1663, 0.1823], dtype=torch.float64)\n",
            "NELBO: 4575.669052501761; pi: tensor([0.2481, 0.2068, 0.1959, 0.1667, 0.1826], dtype=torch.float64)\n",
            "NELBO: 4530.657024383784; pi: tensor([0.2476, 0.2067, 0.1959, 0.1671, 0.1828], dtype=torch.float64)\n",
            "NELBO: 4481.51007124214; pi: tensor([0.2471, 0.2066, 0.1959, 0.1674, 0.1830], dtype=torch.float64)\n",
            "NELBO: 4432.970539441295; pi: tensor([0.2466, 0.2065, 0.1960, 0.1677, 0.1832], dtype=torch.float64)\n",
            "NELBO: 4389.674720773625; pi: tensor([0.2461, 0.2065, 0.1960, 0.1680, 0.1834], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz3KZmKwzT0B"
      },
      "source": [
        "array([0.09716075, 0.12602853, 0.15238882, 0.2619916 , 0.36243031])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3I63rvamwS-"
      },
      "source": [
        "ddd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDB14-aCM27b"
      },
      "source": [
        "abs(loss[-1] - nelbo.detach())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VzrJHz9RP1a"
      },
      "source": [
        "i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIQ2GhmhRKO3"
      },
      "source": [
        "loss[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-OiBi_ERTAC"
      },
      "source": [
        "nelbo.detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uFyXRli1kOY"
      },
      "source": [
        "lus = np.triu_indices(nc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-1bmqwf1kBs"
      },
      "source": [
        "#from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "part1 = int(0.7 * XX.shape[0])\n",
        "part2 = int((XX.shape[0] - part1) / 2)\n",
        "part3 = XX.shape[0] - part1 - part2\n",
        "\n",
        "train, valid, test = random_split(torch.Tensor(XX), [part1, part2, part3], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "trainloader = DataLoader(train, batch_size=256, shuffle=True)\n",
        "validloader = DataLoader(valid, batch_size=256, shuffle=False)\n",
        "testloader = DataLoader(test, batch_size=256, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdL2OQLQKi86"
      },
      "source": [
        "'''\n",
        "for epoch in range(N_ITER):\n",
        "  \n",
        "  for j, batch_data in enumerate(trainloader):\n",
        "    \n",
        "    bY = torch.tensor(batch_data[:,:nf])\n",
        "    bS = torch.tensor(batch_data[:,nf])\n",
        "    bYS = torch.hstack((bY,bS.reshape(-1,1))).float()\n",
        "\n",
        "    #print(bY.shape)\n",
        "    #print(bS.shape)\n",
        "    opt.zero_grad()\n",
        "\n",
        "    r, log_r = r_net(bYS)\n",
        "    v, log_v = v_net(bYS)\n",
        "    d, log_d = d_net(bYS)\n",
        "  \n",
        "    loss = -ELBO(Theta, bY, bS, r, log_r, v, log_v, d, log_d)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  \n",
        "  if epoch % (1000 - 1) == 0:\n",
        "    print(loss.detach())\n",
        "    #print(F.log_softmax(d.mean(0)).exp())\n",
        "    #print(F.log_softmax(Theta['is_pi']).exp())\n",
        "    #print(loss.detach())\n",
        "    #print(Theta['is_delta'].detach().exp())\n",
        "    #print(Theta['is_pi'].detach().exp())\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9PuVTIn-nG_"
      },
      "source": [
        "F.log_softmax(Theta['is_delta']).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjMcn4sP-wft"
      },
      "source": [
        "F.log_softmax(Theta['is_pi']).exp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M824L26-_G4D"
      },
      "source": [
        "F.log_softmax(Theta['is_tau'].reshape(-1)).reshape(nc, nc).exp()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}