{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlem_nso.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7C0QxgRFIjr8gPxI+8dbH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camlab-bioml/2021_IMC_Jett/blob/main/mlem_nso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59kYye7gFJWr"
      },
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "import random\n",
        "\n",
        "def compute_p_y_given_z(Y, Theta, dist='normal', reg=1e-6):\n",
        "  \n",
        "  \"\"\" Returns NxC\n",
        "  p(y_n | z_n = c)\n",
        "  \"\"\"\n",
        "  \n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma']) + reg\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_Y = D.Normal(mu, sigma)\n",
        "  else:\n",
        "    dist_Y = D.StudentT(mu, sigma)\n",
        "\n",
        "  return dist_Y.log_prob(Y.reshape(Y.shape[0], 1, NF)).sum(2) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_z(S, Theta, dist='normal', reg=1e-6):\n",
        "  \n",
        "  \"\"\" Returns NxC\n",
        "  p(s_n | z_n = c)\n",
        "  \"\"\"\n",
        "  \n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega']) + reg\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_S = D.Normal(psi, omega)\n",
        "  else:\n",
        "    dist_S = D.StudentT(psi, omega)\n",
        "\n",
        "  return dist_S.log_prob(S.reshape(-1,1)) \n",
        "\n",
        "def compute_p_y_given_gamma(Y, Theta, dist='normal', reg=1e-6):\n",
        "  \n",
        "  \"\"\" NxCxC\n",
        "  p(y_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "\n",
        "  mu = torch.exp(Theta['log_mu'])\n",
        "  sigma = torch.exp(Theta['log_sigma']) + reg\n",
        "\n",
        "  mu2 = mu.reshape(1, NC, NF)\n",
        "  mu2 = (mu2 + mu2.permute(1, 0, 2)) / 2.0 # C x C x G matrix \n",
        "\n",
        "  sigma2 = sigma.reshape(1, NC, NF)\n",
        "  sigma2 = (sigma2 + sigma2.permute(1,0,2)) / 2.0\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_Y2 = D.Normal(mu2, sigma2)\n",
        "  else:\n",
        "    dist_Y2 = D.StudentT(mu2, sigma2)\n",
        "\n",
        "  return  dist_Y2.log_prob(Y.reshape(-1, 1, 1, NF)).sum(3) # <- sum because IID over G\n",
        "\n",
        "def compute_p_s_given_gamma(S, Theta, dist='normal', reg=1e-6):\n",
        "  \n",
        "  \"\"\" NxCxC\n",
        "  p(s_n | gamma_n = [c,c'])\n",
        "  \"\"\"\n",
        "  \n",
        "  psi = torch.exp(Theta['log_psi'])\n",
        "  omega = torch.exp(Theta['log_omega']) + reg\n",
        "\n",
        "  psi2 = psi.reshape(-1,1)\n",
        "  psi2 = psi2 + psi2.T\n",
        "\n",
        "  omega2 = omega.reshape(-1,1)\n",
        "  omega2 = omega2 + omega2.T\n",
        "\n",
        "  if dist == 'normal':\n",
        "    dist_S2 = D.Normal(psi2, omega2)\n",
        "  else:\n",
        "    dist_S2 = D.StudentT(psi2, omega2)\n",
        "\n",
        "  return dist_S2.log_prob(S.reshape(-1, 1, 1))\n",
        "\n",
        "def _ics(logL, n_obs, n_features, n_clusters, incCellSize=True): #, n, p, c\n",
        "  #params = ( (((n_features * n_features) - n_features)/2 + 2 * n_features + 3) * (((n_clusters * n_clusters) - n_clusters)/2 + 2 * n_clusters) ) - 1\n",
        "  param_mu = n_clusters * n_features\n",
        "  param_sigma = n_clusters * n_features\n",
        "  \n",
        "  param_delta = 1\n",
        "  param_pi = n_clusters - 1\n",
        "  param_tau = ((n_clusters * n_clusters) - n_clusters)/2 + n_clusters - 1\n",
        "  \n",
        "  if incCellSize:\n",
        "    param_psi = n_clusters\n",
        "    param_omega = n_clusters\n",
        "    params = param_mu + param_sigma + param_psi + param_omega + param_delta + param_pi + param_tau\n",
        "  else: \n",
        "    params = param_mu + param_sigma + param_delta + param_pi + param_tau\n",
        "\n",
        "  return 2 * (params - logL), -2 * logL + params * np.log(n_obs)\n",
        "\n",
        "def ll(Y, S, Theta, dist, incCellSize):\n",
        "  \n",
        "  \"\"\"compute\n",
        "  p(gamma = [c,c'], d= 1 | Y,S)\n",
        "  p(z = c, d=0 | Y,S)\n",
        "  \"\"\"\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "\n",
        "  ## singlet calculation\n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta, dist)\n",
        "\n",
        "  if incCellSize: # singlet case\n",
        "    p_s_given_z = compute_p_s_given_z(S, Theta, dist)\n",
        "    p_data_given_z_d0 = p_y_given_z + p_s_given_z + log_pi\n",
        "  else:\n",
        "    p_data_given_z_d0 = p_y_given_z + log_pi\n",
        "\n",
        "  p_data_given_d0 = torch.logsumexp(p_data_given_z_d0, dim=1) # this is p(data|d=0)\n",
        "\n",
        "  ## doublet calculation\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta, dist)\n",
        "\n",
        "  if incCellSize: # doublet case\n",
        "    p_s_given_gamma = compute_p_s_given_gamma(S, Theta, dist)\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + p_s_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "  else:\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "\n",
        "  p_data = torch.cat([p_data_given_z_d0 + log_delta[0], p_data_given_gamma_d1 + log_delta[1]], dim=1)\n",
        "\n",
        "  return torch.logsumexp(p_data, dim=1).sum()\n",
        "\n",
        "def compute_r_v_2(Y, S, Theta, dist, incCellSize):\n",
        "  \n",
        "  \"\"\"Need to compute\n",
        "  p(gamma = [c,c'], d= 1 | Y,S)\n",
        "  p(z = c, d=0 | Y,S)\n",
        "  \"\"\"\n",
        "  \n",
        "  #lookups = np.triu_indices(nc) # wanted indices\n",
        "\n",
        "  log_pi = F.log_softmax(Theta['is_pi'], 0)\n",
        "  log_tau = F.log_softmax(Theta['is_tau'].reshape(-1), 0).reshape(NC,NC)\n",
        "  log_delta = F.log_softmax(Theta['is_delta'], 0)\n",
        "\n",
        "  ## singlet calculation\n",
        "  p_y_given_z = compute_p_y_given_z(Y, Theta, incCellSize, dist)\n",
        "\n",
        "  if incCellSize: # singlet case\n",
        "    p_s_given_z = compute_p_s_given_z(S, Theta, dist)\n",
        "    p_data_given_z_d0 = p_y_given_z + p_s_given_z + log_pi\n",
        "  else:\n",
        "    p_data_given_z_d0 = p_y_given_z + log_pi\n",
        "\n",
        "  p_data_given_d0 = torch.logsumexp(p_data_given_z_d0, dim=1) # this is p(data|d=0)\n",
        "\n",
        "  ## doublet calculation\n",
        "  p_y_given_gamma = compute_p_y_given_gamma(Y, Theta, dist)\n",
        "  \n",
        "  if incCellSize: # doublet case\n",
        "    p_s_given_gamma = compute_p_s_given_gamma(S, Theta, dist)\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + p_s_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "  else:\n",
        "    p_data_given_gamma_d1 = (p_y_given_gamma + log_tau).reshape(Y.shape[0], -1)\n",
        "  \n",
        "  ## LL\n",
        "  p_data = torch.cat([p_data_given_z_d0 + log_delta[0], p_data_given_gamma_d1 + log_delta[1]], dim=1)\n",
        "  p_data = torch.logsumexp(p_data, dim=1)\n",
        "\n",
        "  ## singlet & doublet probability\n",
        "  r = p_data_given_z_d0.T + log_delta[0] - p_data\n",
        "  v = p_data_given_gamma_d1.T + log_delta[1] - p_data\n",
        "\n",
        "  ## normalize\n",
        "  p_singlet = torch.exp(p_data_given_d0 + log_delta[0] - p_data)\n",
        "\n",
        "  return r.T, v.T.reshape(-1,NC,NC), p_data, p_singlet\n",
        "\n",
        "def mlem_ncs(Y, Theta, dist):\n",
        "  \n",
        "  wandb.init(project=\"mlem_{}_nc{}\".format(PROJECT_NAME, NC))\n",
        "  \n",
        "  lookups = np.triu_indices(NC) # wanted indices\n",
        "  uwanted = np.tril_indices(NC, -1)\n",
        "  \n",
        "  opt = optim.Adam(Theta.values(), lr=LEARNING_RATE)\n",
        "\n",
        "  trainloader = DataLoader(Y, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "  loss = []\n",
        "  for epoch in range(N_ITER * N_ITER_OPT):\n",
        "    \n",
        "    nlls = 0\n",
        "    for j, train_batch in enumerate(trainloader):\n",
        "      \n",
        "      opt.zero_grad() \n",
        "      nll = -ll(train_batch, Theta, dist, incCellSize=False)\n",
        "      nll.backward()\n",
        "      opt.step()\n",
        "            \n",
        "      nlls += nll\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      aic, bic = _ics(-nlls, Y.shape[0], NF, NC, incCellSize=False) #, n, p, c\n",
        "\n",
        "      wandb.log({\n",
        "        'nll': nlls, \n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "      })\n",
        "\n",
        "      if epoch > 15 and abs(np.mean(loss[-10:]) - np.mean(loss[-11:-1])) < TOL:\n",
        "        print(nlls)\n",
        "        print(F.log_softmax(Theta['is_delta'], 0).exp())\n",
        "        break\n",
        "            \n",
        "      loss.append(nlls)\n",
        "    \n",
        "  #with torch.no_grad():\n",
        "    \n",
        "  #  r, v, L, p_singlet = compute_r_v_2(Y, Theta, dist)\n",
        "\n",
        "  #  ugt = torch.tensor(v[:,lookups[0], lookups[1]]).exp()\n",
        "  #  lt = torch.tensor(v[:,uwanted[0], uwanted[1]]).exp()\n",
        "  #  ugt[:,lookups[0] != lookups[1]] = ugt[:,lookups[0] != lookups[1]] + lt             \n",
        "  #  p_cluster = torch.hstack((ugt, torch.tensor(r).exp()))\n",
        "\n",
        "  #return {'theta': Theta, 'p_singlet': p_singlet}\n",
        "  #return p_singlet, p_cluster\n",
        "\n",
        "def mlem_ycs(Y, S, Theta, dist, incCellSize):\n",
        "  \n",
        "  wandb.init(project=\"mlem_{}_nc{}\".format(PROJECT_NAME, NC))\n",
        "  \n",
        "  lookups = np.triu_indices(NC) # wanted indices\n",
        "  uwanted = np.tril_indices(NC, -1)\n",
        "  \n",
        "  opt = optim.Adam(Theta.values(), lr=LEARNING_RATE)\n",
        "  \n",
        "  XX = torch.hstack((Y, S.reshape(-1,1))).float()\n",
        "  trainloader = DataLoader(torch.tensor(XX), batch_size=BATCH_SIZE, shuffle=True)\n",
        "  #validloader = DataLoader(valid, batch_size=1280, shuffle=False)\n",
        "  #testloader = DataLoader(test, batch_size=1280, shuffle=False)\n",
        "    \n",
        "  loss = []\n",
        "  for epoch in range(N_ITER * N_ITER_OPT):\n",
        "    \n",
        "    nlls = 0\n",
        "    for j, train_batch in enumerate(trainloader):\n",
        "      \n",
        "      bY = train_batch[:,:NF]\n",
        "      bS = train_batch[:,NF]\n",
        "      \n",
        "      opt.zero_grad()  \n",
        "      nll = -ll(bY, bS, Theta, dist, incCellSize=True)\n",
        "      nll.backward()\n",
        "      opt.step()\n",
        "            \n",
        "      nlls += nll\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      aic, bic = _ics(-nlls, Y.shape[0], NF, NC, incCellSize=True) #, n, p, c\n",
        "\n",
        "      wandb.log({\n",
        "        'nll': nlls, \n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "      })\n",
        "\n",
        "      if epoch > 15 and abs(np.mean(loss[-10:]) - np.mean(loss[-11:-1])) < TOL:\n",
        "        print(nlls)\n",
        "        print(F.log_softmax(Theta['is_delta'], 0).exp())\n",
        "        print(Theta['log_psi'].exp())\n",
        "        break\n",
        "            \n",
        "      loss.append(nlls)\n",
        "    \n",
        "  #with torch.no_grad():\n",
        "    \n",
        "  #  r, v, L, p_singlet = compute_r_v_2(Y, S, Theta, dist)\n",
        "\n",
        "  #  ugt = torch.tensor(v[:,lookups[0], lookups[1]]).exp()\n",
        "  #  lt = torch.tensor(v[:,uwanted[0], uwanted[1]]).exp()\n",
        "  #  ugt[:,lookups[0] != lookups[1]] = ugt[:,lookups[0] != lookups[1]] + lt             \n",
        "  #  p_cluster = torch.hstack((ugt, torch.tensor(r).exp()))\n",
        "\n",
        "  #return p_singlet, p_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ffmS-ruFaAY"
      },
      "source": [
        "!pip install scanpy\n",
        "import scanpy as sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N79MVxJFxhi",
        "outputId": "02661b2f-76c3-4f75-ea01-af5e0a7d3470"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y67MfezzG_K"
      },
      "source": [
        "## mouse data\n",
        "adata = sc.read_h5ad(\"DAMM/data/mouse_single_cell_expression.h5ad\")\n",
        "\n",
        "included_names = ['B220', 'CCR7', 'CD11b', 'CD11c', 'CD19', 'CD28', 'CD3', 'CD31', 'CD4',\n",
        " 'CD45', 'CD49b', 'CD68', 'CD73', 'CD8', 'CTLA4', 'FOXP3', 'GATA3', 'GFP', \n",
        " 'GranzymeB', 'HA', 'ICOS', 'IL7Ra', 'Ly6G', 'MHCII', 'PD1', 'PDL1', 'PNAd', \n",
        " 'Perforin', 'RFP', 'S100A8-9', 'TBET', 'TCF1', 'YAP', 'iNOS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2KZqd8aMXwx"
      },
      "source": [
        "'''\n",
        "## human data\n",
        "adata = sc.read_h5ad(\"DAMM/data/basel_zuri_subsample.h5ad\")\n",
        "adata = sc.read_h5ad(\"DAMM/data/basel_zuri.h5ad\")\n",
        "included_names = ['EGFR', 'ECadherin', 'ER', 'GATA3', 'Histone_H3_1', 'Ki67', 'SMA', \n",
        "'Vimentin', 'cleaved_Parp', 'Her2', 'p53', 'panCytokeratin', 'CD19', 'PR', 'Myc', \n",
        "'Fibronectin', 'CK14', 'Slug', 'CD20', 'vWF', 'Histone_H3_2', 'CK5', 'CD44', 'CD45', \n",
        "'CD68', 'CD3', 'CAIX', 'CK8/18', 'CK7', 'phospho Histone', 'phospho S6', 'phospho mTOR']\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA3UE-PJFie4"
      },
      "source": [
        "adata = adata[:,included_names]\n",
        "\n",
        "YY = adata.X\n",
        "YY = np.array(np.arcsinh(YY / 5.))\n",
        "\n",
        "NO, NF = YY.shape #number obs & features \n",
        "\n",
        "for i in range(NF):\n",
        "  YY[:,i] = winsorize(YY[:,i], limits=[0, 0.01]).data\n",
        "\n",
        "SS = adata.obs['size']\n",
        "#SS = adata.obs['Area'] ## for human data\n",
        "SS = winsorize(SS, limits=[0, 0.01]).data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EPIKHBBF_6U"
      },
      "source": [
        "NC = 25 # number of clusters\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1280\n",
        "N_ITER = 10000\n",
        "N_ITER_OPT = 500\n",
        "TOL = 1e-3 #converagence criterion\n",
        "\n",
        "## w&b api key\n",
        "PROJECT_NAME = 'mouse_nso_ncs_stuT'\n",
        "wandb.login(key='4117bb00bef94e0904c16afed79f1888e0839eb9')\n",
        "\n",
        "Y = torch.tensor(YY)\n",
        "S = torch.tensor(SS)\n",
        "\n",
        "kms = KMeans(NC).fit(Y)\n",
        "init_labels = kms.labels_\n",
        "init_label_class = np.unique(init_labels)\n",
        "\n",
        "mu_init = np.array([YY[init_labels == c,:].mean(0) for c in init_label_class])\n",
        "sigma_init = np.array([YY[init_labels == c,:].std(0) for c in init_label_class])\n",
        "\n",
        "psi_init = np.array([SS[init_labels == c].mean() for c in init_label_class])\n",
        "omega_init = np.array([SS[init_labels == c].std() for c in init_label_class])\n",
        "\n",
        "pi_init = np.array([np.mean(init_labels == c) for c in init_label_class])\n",
        "tau_init = np.ones((NC,NC))\n",
        "tau_init = tau_init / tau_init.sum()\n",
        "\n",
        "Theta = {\n",
        "    'log_mu': np.log(mu_init + 1e-6),\n",
        "    'log_sigma': np.log(sigma_init + 1e-6), #np.zeros_like(sigma_init),\n",
        "    'log_psi': np.log(psi_init + 1e-6),\n",
        "    'log_omega': np.log(omega_init + 1e-6),\n",
        "    'is_delta': np.array([0.5, 0.5]),\n",
        "    'is_pi': pi_init,\n",
        "    'is_tau': tau_init,\n",
        "}\n",
        "\n",
        "Theta = {k: torch.tensor(v, requires_grad=True) for (k,v) in Theta.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "styisGmuGXjF"
      },
      "source": [
        "mle1 = mlem_ycs(Y, S, Theta, dist='student', incCellSize=True) \n",
        "#mle1 = mlem_ncs(Y, Theta, dist='student', incCellSize=False)\n",
        "\n",
        "#mle1 = mlem_ycs(Y, S, Theta, dist='normal', incCellSize=True)\n",
        "#mle1 = mlem_ncs(Y, Theta, dist='normal', incCellSize=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}